[
  {
    "objectID": "Arboles_Des_series.html",
    "href": "Arboles_Des_series.html",
    "title": "Predicción 1 paso adelante",
    "section": "",
    "text": "Vamos a importar la bases de datos y a convertirlas en objetos de series de Tiempo. \\(\\{X_t\\}\\)\nCode\n# get working directory\nimport os\nos.getcwd()\n\n\n'C:\\\\Users\\\\dofca\\\\Desktop\\\\series'\nCode\n# librerias\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport sklearn\nimport openpyxl\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nimport warnings\nprint(f\"Matplotlib Version: {plt.__version__}\")\nprint(f\"Pandas Version: {pd.__version__}\")\nprint(f\"Numpy Version: {np.__version__}\")\nprint(f\"Sklearn: {sklearn.__version__}\")\n\n\nMatplotlib Version: 1.25.2\nPandas Version: 2.0.3\nNumpy Version: 1.25.2\nSklearn: 1.3.1\nCode\n# traer datos originales\ndata = pd.ExcelFile('Bitcoin.xlsx')\nprint(data.sheet_names)\n# dataframe con los datos\ndata = data.parse('Sheet1')\nprint(data)\nprint(type(data))\n\n\n['Sheet1']\n     FechaTiempo     Valor\n0     2021-12-31  46214.37\n1     2021-12-30  47150.71\n2     2021-12-29  46483.36\n3     2021-12-28  47543.30\n4     2021-12-27  50718.11\n...          ...       ...\n1821  2017-01-05    994.02\n1822  2017-01-04   1122.56\n1823  2017-01-03   1036.99\n1824  2017-01-02   1014.10\n1825  2017-01-01    998.80\n\n[1826 rows x 2 columns]\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nNotamos que estan organizador del más reciente al más antiguo asi entonces buscaremos organiarla cámo debe ser\nCode\ndata = data[::-1].reset_index(drop=False)\ndata.index = data['FechaTiempo']\ndata=data.asfreq('D')\ndata = data.drop(columns=['FechaTiempo','index'])\nprint(data)\n\n\n                Valor\nFechaTiempo          \n2017-01-01     998.80\n2017-01-02    1014.10\n2017-01-03    1036.99\n2017-01-04    1122.56\n2017-01-05     994.02\n...               ...\n2021-12-27   50718.11\n2021-12-28   47543.30\n2021-12-29   46483.36\n2021-12-30   47150.71\n2021-12-31   46214.37\n\n[1826 rows x 1 columns]\nCode\n# traer datos\ndata_2 = pd.ExcelFile('dlData.xlsx')\nprint(data_2.sheet_names)\n# dataframe con los datos\ndata_2 = data_2.parse('Sheet1')\nprint(data_2)\nprint(type(data_2))\n\n\n['Sheet1']\n      Unnamed: 0        V1\n0     2017-01-02  0.015202\n1     2017-01-03  0.022321\n2     2017-01-04  0.079290\n3     2017-01-05 -0.121610\n4     2017-01-06 -0.108785\n...          ...       ...\n1820  2021-12-27 -0.001440\n1821  2021-12-28 -0.064642\n1822  2021-12-29 -0.022546\n1823  2021-12-30  0.014255\n1824  2021-12-31 -0.020058\n\n[1825 rows x 2 columns]\n&lt;class 'pandas.core.frame.DataFrame'&gt;"
  },
  {
    "objectID": "Arboles_Des_series.html#serie-original",
    "href": "Arboles_Des_series.html#serie-original",
    "title": "Predicción 1 paso adelante",
    "section": "0.1 Serie original",
    "text": "0.1 Serie original\n\n\nCode\n# tipo de datos\nprint(data.info())\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 1826 entries, 2017-01-01 to 2021-12-31\nFreq: D\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   Valor   1826 non-null   float64\ndtypes: float64(1)\nmemory usage: 28.5 KB\nNone\n\n\n\n\nCode\n#mirando los datos\n#objeto ts\nBitcoints = data['Valor']\nprint(type(Bitcoints))\nplt.plot(Bitcoints)\n\n\n&lt;class 'pandas.core.series.Series'&gt;\n\n\n\n\n\nSe tiene concocimieno de que los árboles no son buenos manejando la tendencia, pero debido análisis descriptivo previo utilizaremos el método tanto para la serie original cómo para la serie diferenciada la cuál no presenta tendencía. Ademas miraremos si se tienen datos faltantes y si es regularmente espaciada pues esto es importante en la implementaciòn del modelo\n\n\nCode\nprint(f'Numero de filas con valores faltantes: {data.isnull().any(axis=1).mean()}')\n\n\nNumero de filas con valores faltantes: 0.0\n\n\n\n\nCode\n(data.index == pd.date_range(start=data.index.min(),\n                             end=data.index.max(),\n                             freq=data.index.freq)).all()\n\n\nTrue\n\n\n\n\nCode\ndata.shape\n\n\n(1826, 1)"
  },
  {
    "objectID": "Arboles_Des_series.html#pacf",
    "href": "Arboles_Des_series.html#pacf",
    "title": "Predicción 1 paso adelante",
    "section": "0.2 PACF",
    "text": "0.2 PACF\nusaremos la funcion de autocorrealcion parcial para darnos una idea de cuantos rezagos usaremos en el modelo\n\n\nCode\nfrom matplotlib import pyplot\nfrom statsmodels.graphics.tsaplots import plot_pacf\nplot_pacf(Bitcoints,lags=400,method='ywm',alpha=0.01)\npyplot.show()\n\n\n\n\n\n\n\nCode\nimport statsmodels.api as sm\npacf =  sm.tsa.stattools.pacf(Bitcoints, nlags=400,method='ywm')\nT = len(Bitcoints)\n\nsig_test = lambda tau_h: np.abs(tau_h) &gt; 2.58/np.sqrt(T)\n\n\n\n\nCode\npacf\n\n\narray([ 1.00000000e+00,  9.97420147e-01,  1.43160372e-02,  8.32568596e-03,\n       -2.16200676e-02, -6.64909379e-02, -2.05655458e-02, -1.11736885e-02,\n        2.64788871e-02,  1.88209990e-02, -1.95036725e-03, -3.88385078e-02,\n        1.46649638e-02,  4.02839902e-02, -3.20071212e-02,  7.74967752e-03,\n        8.32547029e-04, -3.67461933e-03, -2.23679602e-02,  3.46733025e-02,\n       -1.44698212e-02, -3.65428193e-02,  4.56437447e-02, -1.61095969e-02,\n       -1.14022513e-02, -2.79038247e-02, -6.17605219e-03, -5.56504201e-03,\n       -2.17656400e-02, -9.49903080e-02,  8.18256708e-03, -2.54952528e-02,\n       -4.40369560e-02,  5.88072667e-02, -4.49650306e-02,  1.84696791e-02,\n        1.29335086e-02, -3.01602433e-02, -2.24434633e-02,  3.04122054e-03,\n        2.43040409e-02, -6.97004162e-02,  1.49355410e-02,  3.10018773e-02,\n       -1.10061127e-02, -7.95068046e-02,  1.24266404e-02, -1.13964700e-02,\n       -4.03688623e-03,  1.55020918e-02,  1.70520860e-02,  1.74989043e-03,\n        1.21015350e-02, -3.07520794e-02, -2.40427813e-02,  3.27927080e-02,\n       -6.53037375e-03,  2.86474352e-02, -1.96808480e-02, -7.90078780e-02,\n       -2.18795898e-02,  7.65068090e-02, -8.24179943e-03, -1.35019893e-02,\n        2.53365289e-02,  3.52541195e-02, -8.01130656e-03, -3.91247489e-02,\n       -4.42760666e-02,  3.52794241e-02,  3.73066297e-02, -1.08543965e-02,\n       -1.17295344e-02, -6.29211466e-03,  4.27725260e-02,  1.80727993e-02,\n       -2.17990186e-02,  1.05513512e-02,  3.66616603e-03,  2.96900412e-02,\n       -1.55035039e-02, -5.30306130e-04, -4.40593353e-02,  3.84112629e-02,\n       -6.68843658e-03,  7.77826673e-04,  1.31354302e-02, -8.62851872e-03,\n        3.71172338e-02,  3.88706785e-03,  3.25068553e-02,  4.23596316e-02,\n       -3.65463786e-03, -6.20868209e-03,  4.90171389e-02, -2.61232839e-03,\n       -8.52400827e-03,  2.99264007e-03,  5.96575554e-03,  2.75967006e-02,\n        1.76070503e-03,  1.60482324e-02,  8.28280601e-03, -3.97355197e-03,\n       -4.20674978e-02, -3.28793301e-02,  3.38334033e-02,  1.58642323e-02,\n        2.62085867e-02,  2.97071376e-03,  1.71855475e-02,  1.72643895e-02,\n        3.05017035e-02, -1.35655469e-02, -5.31464285e-02, -1.75808674e-02,\n       -5.71492462e-02, -6.12127146e-02,  1.42092759e-02, -4.54482085e-02,\n        6.56194552e-02, -3.41984432e-03, -2.08306963e-02,  1.12317601e-03,\n       -2.08718936e-02,  2.95680402e-04,  2.94274144e-02, -1.83342533e-02,\n        1.23501400e-02, -5.31964809e-02,  1.88857866e-02,  6.64954781e-03,\n       -1.43379447e-02, -2.82466048e-02,  3.64362606e-02,  2.78739121e-02,\n        4.00735769e-02,  1.60950134e-02, -2.78719893e-02, -1.59682260e-02,\n       -6.23924118e-03, -8.63544090e-03,  5.67106623e-02,  8.27328234e-03,\n        4.69537337e-02,  7.33401221e-03,  9.90374812e-03, -2.04958984e-02,\n        5.00599531e-02,  2.47099009e-02,  1.23689854e-02,  2.88242312e-02,\n       -7.26544560e-03, -2.51611592e-03, -1.60389275e-02, -2.31570751e-04,\n        3.27550941e-02,  3.31473878e-02, -1.59668082e-02,  1.28527880e-02,\n       -1.45727877e-02,  4.27316691e-02,  1.82124277e-02,  1.63395166e-02,\n       -1.86837948e-02,  4.88482302e-02, -1.57743652e-02, -3.09831365e-04,\n       -1.89292664e-02, -1.65888742e-03,  1.63917403e-03, -1.30371168e-02,\n       -2.33155335e-02, -2.96915098e-02, -2.06273374e-02,  4.11185718e-02,\n       -1.54189343e-02,  1.44576365e-02, -3.76912726e-02,  6.90010391e-03,\n       -5.77743500e-03, -1.40188637e-02, -4.60928488e-03,  2.17524133e-02,\n       -7.82033891e-03, -1.92378986e-02, -3.05231074e-02,  8.28077748e-03,\n       -2.11831058e-02,  1.41848402e-02, -2.97118304e-02, -2.10906457e-02,\n       -1.51244719e-02, -4.85382690e-03, -1.91053390e-02, -3.03267295e-02,\n        1.77146734e-02, -3.04039577e-02, -1.14836058e-02, -6.85499644e-03,\n       -6.65167786e-03,  2.95699024e-03,  6.03414560e-03,  1.16911360e-02,\n        1.40041081e-03, -6.76935135e-03, -1.99822959e-02,  5.51752430e-03,\n       -2.10847546e-02, -7.18625743e-02,  8.63421171e-03, -1.10288853e-02,\n       -2.25733934e-02,  1.37294158e-02,  6.89325437e-03, -2.29161625e-02,\n        1.23999856e-02,  1.70546501e-02,  1.74117842e-03, -1.93238358e-02,\n       -1.83998668e-02,  4.29486879e-02,  4.14616038e-03,  5.70185572e-02,\n       -2.05325115e-02, -1.24786266e-03, -1.03254954e-02, -3.57573155e-02,\n       -5.01515489e-02,  2.03704280e-02, -4.35929732e-02, -7.38676493e-03,\n       -2.43014268e-02, -2.42638427e-02,  1.71087053e-02, -3.19742139e-02,\n        1.44070451e-03, -2.16294341e-02,  1.64339276e-02,  3.88857254e-02,\n       -1.35817837e-02, -3.72236303e-02,  1.70908329e-02, -4.42727962e-02,\n        8.03853006e-03, -1.16607008e-03, -3.42341762e-02,  4.01871212e-02,\n       -7.45536453e-05,  1.15154434e-02,  1.24299915e-02,  2.68021128e-02,\n       -9.64315448e-03, -5.46141388e-03, -2.44323785e-03, -2.54014031e-02,\n       -2.03952780e-03,  4.92366525e-03, -4.68436271e-03, -1.31835699e-02,\n       -4.76821607e-02, -1.88930236e-02,  1.30503528e-02,  2.08437365e-02,\n        1.33966358e-02,  1.13875457e-02,  2.48702460e-03, -3.67389694e-03,\n        5.22529370e-03,  1.41512307e-02, -1.04593709e-02,  1.19960657e-02,\n        2.30289603e-02, -1.29164094e-02, -6.84866454e-02,  4.79950896e-04,\n        2.78088050e-02, -8.54368533e-03,  5.97798147e-03,  1.48702416e-02,\n       -2.30013156e-02,  2.22415074e-02,  2.23569473e-02, -9.22784092e-03,\n       -2.50679846e-02, -8.66007569e-03, -6.57708149e-02,  9.05987059e-03,\n       -4.63119627e-03,  1.05235817e-02, -7.43702104e-03, -1.22062472e-02,\n        4.11623022e-02, -2.23490333e-02, -3.17178546e-02,  3.45309242e-02,\n        8.64316002e-03,  9.57089467e-03,  2.88525618e-02,  3.49899238e-02,\n        2.21508425e-02,  3.72112010e-02,  1.16919064e-02,  1.77118242e-02,\n       -3.24603405e-02,  1.27001670e-02, -4.74187985e-02,  4.00286316e-02,\n       -2.58404466e-02, -3.07331701e-03, -7.81151761e-03, -2.90424255e-02,\n        8.13580750e-03, -2.39895708e-02, -6.36117717e-03,  2.63070654e-03,\n       -1.05461221e-02,  2.92048113e-02, -1.86299124e-04, -4.12830566e-03,\n       -1.17998033e-02, -1.18160992e-02,  4.32107807e-04, -1.81627229e-02,\n        1.98359185e-02,  1.00646166e-02, -1.26461222e-02, -2.60622704e-03,\n        3.67424002e-02,  1.82104425e-03,  1.52220352e-02, -5.35907575e-03,\n       -2.18962499e-04,  6.61174007e-04,  2.27252797e-02, -2.92407883e-03,\n       -1.26736405e-02,  2.40680871e-02,  2.98768975e-03,  2.77788734e-02,\n        3.46233953e-03,  3.36272793e-02, -8.12321169e-03, -1.54525772e-02,\n       -3.43789613e-03,  3.29625701e-03, -1.72737662e-02, -3.50416112e-02,\n       -2.10695692e-02,  2.04856817e-02,  5.33495766e-03,  1.70959501e-02,\n       -4.01938437e-02, -1.96291282e-03, -1.60972712e-03, -2.82954716e-02,\n       -3.12503945e-02, -8.00801978e-03, -3.75004791e-03, -9.50506474e-03,\n        3.71845637e-02,  3.01580206e-02, -9.18347705e-03, -1.32259447e-02,\n       -2.16457975e-02,  2.33490331e-02,  3.56516909e-03, -6.48253565e-03,\n        3.82506917e-03,  2.36159112e-03,  5.84204891e-03,  2.04932024e-02,\n        9.97363648e-03, -1.95683501e-02, -2.44179291e-02, -9.65308557e-03,\n        9.34570056e-03,  2.66407297e-03, -3.36869156e-02,  1.24567631e-03,\n        1.09435878e-02,  1.52749433e-02,  9.14501791e-03,  8.26701313e-03,\n       -3.15384743e-03,  1.25007952e-02, -2.14876244e-02, -9.62489113e-03,\n        7.88850820e-03,  1.95622459e-02, -2.08497673e-02, -1.14023050e-02,\n        1.86448121e-02, -1.02349355e-02, -8.32842028e-03, -2.21430431e-02,\n       -5.45592033e-03, -1.43473437e-02,  1.78245698e-02,  1.71387693e-02,\n       -1.86369432e-03])\n\n\n\n\nCode\nfor i in range(len(pacf)):\n    if sig_test(pacf[i]) == False:\n        n_steps = i - 1\n        print('n_steps set to', n_steps)\n        break\n\n\nn_steps set to 1\n\n\nObservamos que con el pacf se nos recomienda usar solo un retraso el mas proximo, lo cuál nos recueda que cuando se quizo establecer una componente estacional se sugeria un periodo de 2.5, por lo tanto teniendo encuenta lo anterior usaremos 3 retrasos para la serie original"
  },
  {
    "objectID": "Arboles_Des_series.html#serie-trasnformada-y-diferenciada",
    "href": "Arboles_Des_series.html#serie-trasnformada-y-diferenciada",
    "title": "Predicción 1 paso adelante",
    "section": "0.3 Serie Trasnformada y diferenciada",
    "text": "0.3 Serie Trasnformada y diferenciada\n\n\nCode\n# tipo de datos\nprint(data_2.info())\n# nombre\ndata_2.columns = ['Fecha','Valor']\nprint(data_2.info())\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1825 entries, 0 to 1824\nData columns (total 2 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   Unnamed: 0  1825 non-null   object \n 1   V1          1825 non-null   float64\ndtypes: float64(1), object(1)\nmemory usage: 28.6+ KB\nNone\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1825 entries, 0 to 1824\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   Fecha   1825 non-null   object \n 1   Valor   1825 non-null   float64\ndtypes: float64(1), object(1)\nmemory usage: 28.6+ KB\nNone\n\n\n\n\nCode\n# fecha\ndata_2['Fecha']=pd.to_datetime(data_2['Fecha'])###Sólo es necesario si no tiene formato de fecha\ndlData = data_2.set_index('Fecha')\ndlData=dlData.asfreq('D')\nprint(type(dlData))\nprint(dlData)\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n               Valor\nFecha               \n2017-01-02  0.015202\n2017-01-03  0.022321\n2017-01-04  0.079290\n2017-01-05 -0.121610\n2017-01-06 -0.108785\n...              ...\n2021-12-27 -0.001440\n2021-12-28 -0.064642\n2021-12-29 -0.022546\n2021-12-30  0.014255\n2021-12-31 -0.020058\n\n[1825 rows x 1 columns]\n\n\n\n\nCode\n#objeto ts\ndlDatats = dlData['Valor']\nprint(type(dlDatats))\nplt.plot(dlDatats)\n\n\n&lt;class 'pandas.core.series.Series'&gt;\n\n\n\n\n\n\n\nCode\nprint(f'Numero de filas con valores faltantes: {data_2.isnull().any(axis=1).mean()}')\n\n\nNumero de filas con valores faltantes: 0.0\n\n\n\n\nCode\n(dlData.index == pd.date_range(start=dlData.index.min(),\n                             end=dlData.index.max(),\n                             freq=dlData.index.freq)).all()\n\n\nTrue\n\n\n\n\nCode\ndlData.shape\n\n\n(1825, 1)"
  },
  {
    "objectID": "Arboles_Des_series.html#pacf-1",
    "href": "Arboles_Des_series.html#pacf-1",
    "title": "Predicción 1 paso adelante",
    "section": "0.4 PACF",
    "text": "0.4 PACF\nusaremos la funcion de autocorrealcion parcial para darnos una idea de cuantos rezagos usaremos en el modelo\n\n\nCode\nfrom matplotlib import pyplot\nfrom statsmodels.graphics.tsaplots import plot_pacf\nplot_pacf(dlDatats,lags=400,method='ywm',alpha=0.01)\npyplot.show()\n\n\n\n\n\n\n\nCode\npacf\n\n\narray([ 1.00000000e+00,  9.97420147e-01,  1.43160372e-02,  8.32568596e-03,\n       -2.16200676e-02, -6.64909379e-02, -2.05655458e-02, -1.11736885e-02,\n        2.64788871e-02,  1.88209990e-02, -1.95036725e-03, -3.88385078e-02,\n        1.46649638e-02,  4.02839902e-02, -3.20071212e-02,  7.74967752e-03,\n        8.32547029e-04, -3.67461933e-03, -2.23679602e-02,  3.46733025e-02,\n       -1.44698212e-02, -3.65428193e-02,  4.56437447e-02, -1.61095969e-02,\n       -1.14022513e-02, -2.79038247e-02, -6.17605219e-03, -5.56504201e-03,\n       -2.17656400e-02, -9.49903080e-02,  8.18256708e-03, -2.54952528e-02,\n       -4.40369560e-02,  5.88072667e-02, -4.49650306e-02,  1.84696791e-02,\n        1.29335086e-02, -3.01602433e-02, -2.24434633e-02,  3.04122054e-03,\n        2.43040409e-02, -6.97004162e-02,  1.49355410e-02,  3.10018773e-02,\n       -1.10061127e-02, -7.95068046e-02,  1.24266404e-02, -1.13964700e-02,\n       -4.03688623e-03,  1.55020918e-02,  1.70520860e-02,  1.74989043e-03,\n        1.21015350e-02, -3.07520794e-02, -2.40427813e-02,  3.27927080e-02,\n       -6.53037375e-03,  2.86474352e-02, -1.96808480e-02, -7.90078780e-02,\n       -2.18795898e-02,  7.65068090e-02, -8.24179943e-03, -1.35019893e-02,\n        2.53365289e-02,  3.52541195e-02, -8.01130656e-03, -3.91247489e-02,\n       -4.42760666e-02,  3.52794241e-02,  3.73066297e-02, -1.08543965e-02,\n       -1.17295344e-02, -6.29211466e-03,  4.27725260e-02,  1.80727993e-02,\n       -2.17990186e-02,  1.05513512e-02,  3.66616603e-03,  2.96900412e-02,\n       -1.55035039e-02, -5.30306130e-04, -4.40593353e-02,  3.84112629e-02,\n       -6.68843658e-03,  7.77826673e-04,  1.31354302e-02, -8.62851872e-03,\n        3.71172338e-02,  3.88706785e-03,  3.25068553e-02,  4.23596316e-02,\n       -3.65463786e-03, -6.20868209e-03,  4.90171389e-02, -2.61232839e-03,\n       -8.52400827e-03,  2.99264007e-03,  5.96575554e-03,  2.75967006e-02,\n        1.76070503e-03,  1.60482324e-02,  8.28280601e-03, -3.97355197e-03,\n       -4.20674978e-02, -3.28793301e-02,  3.38334033e-02,  1.58642323e-02,\n        2.62085867e-02,  2.97071376e-03,  1.71855475e-02,  1.72643895e-02,\n        3.05017035e-02, -1.35655469e-02, -5.31464285e-02, -1.75808674e-02,\n       -5.71492462e-02, -6.12127146e-02,  1.42092759e-02, -4.54482085e-02,\n        6.56194552e-02, -3.41984432e-03, -2.08306963e-02,  1.12317601e-03,\n       -2.08718936e-02,  2.95680402e-04,  2.94274144e-02, -1.83342533e-02,\n        1.23501400e-02, -5.31964809e-02,  1.88857866e-02,  6.64954781e-03,\n       -1.43379447e-02, -2.82466048e-02,  3.64362606e-02,  2.78739121e-02,\n        4.00735769e-02,  1.60950134e-02, -2.78719893e-02, -1.59682260e-02,\n       -6.23924118e-03, -8.63544090e-03,  5.67106623e-02,  8.27328234e-03,\n        4.69537337e-02,  7.33401221e-03,  9.90374812e-03, -2.04958984e-02,\n        5.00599531e-02,  2.47099009e-02,  1.23689854e-02,  2.88242312e-02,\n       -7.26544560e-03, -2.51611592e-03, -1.60389275e-02, -2.31570751e-04,\n        3.27550941e-02,  3.31473878e-02, -1.59668082e-02,  1.28527880e-02,\n       -1.45727877e-02,  4.27316691e-02,  1.82124277e-02,  1.63395166e-02,\n       -1.86837948e-02,  4.88482302e-02, -1.57743652e-02, -3.09831365e-04,\n       -1.89292664e-02, -1.65888742e-03,  1.63917403e-03, -1.30371168e-02,\n       -2.33155335e-02, -2.96915098e-02, -2.06273374e-02,  4.11185718e-02,\n       -1.54189343e-02,  1.44576365e-02, -3.76912726e-02,  6.90010391e-03,\n       -5.77743500e-03, -1.40188637e-02, -4.60928488e-03,  2.17524133e-02,\n       -7.82033891e-03, -1.92378986e-02, -3.05231074e-02,  8.28077748e-03,\n       -2.11831058e-02,  1.41848402e-02, -2.97118304e-02, -2.10906457e-02,\n       -1.51244719e-02, -4.85382690e-03, -1.91053390e-02, -3.03267295e-02,\n        1.77146734e-02, -3.04039577e-02, -1.14836058e-02, -6.85499644e-03,\n       -6.65167786e-03,  2.95699024e-03,  6.03414560e-03,  1.16911360e-02,\n        1.40041081e-03, -6.76935135e-03, -1.99822959e-02,  5.51752430e-03,\n       -2.10847546e-02, -7.18625743e-02,  8.63421171e-03, -1.10288853e-02,\n       -2.25733934e-02,  1.37294158e-02,  6.89325437e-03, -2.29161625e-02,\n        1.23999856e-02,  1.70546501e-02,  1.74117842e-03, -1.93238358e-02,\n       -1.83998668e-02,  4.29486879e-02,  4.14616038e-03,  5.70185572e-02,\n       -2.05325115e-02, -1.24786266e-03, -1.03254954e-02, -3.57573155e-02,\n       -5.01515489e-02,  2.03704280e-02, -4.35929732e-02, -7.38676493e-03,\n       -2.43014268e-02, -2.42638427e-02,  1.71087053e-02, -3.19742139e-02,\n        1.44070451e-03, -2.16294341e-02,  1.64339276e-02,  3.88857254e-02,\n       -1.35817837e-02, -3.72236303e-02,  1.70908329e-02, -4.42727962e-02,\n        8.03853006e-03, -1.16607008e-03, -3.42341762e-02,  4.01871212e-02,\n       -7.45536453e-05,  1.15154434e-02,  1.24299915e-02,  2.68021128e-02,\n       -9.64315448e-03, -5.46141388e-03, -2.44323785e-03, -2.54014031e-02,\n       -2.03952780e-03,  4.92366525e-03, -4.68436271e-03, -1.31835699e-02,\n       -4.76821607e-02, -1.88930236e-02,  1.30503528e-02,  2.08437365e-02,\n        1.33966358e-02,  1.13875457e-02,  2.48702460e-03, -3.67389694e-03,\n        5.22529370e-03,  1.41512307e-02, -1.04593709e-02,  1.19960657e-02,\n        2.30289603e-02, -1.29164094e-02, -6.84866454e-02,  4.79950896e-04,\n        2.78088050e-02, -8.54368533e-03,  5.97798147e-03,  1.48702416e-02,\n       -2.30013156e-02,  2.22415074e-02,  2.23569473e-02, -9.22784092e-03,\n       -2.50679846e-02, -8.66007569e-03, -6.57708149e-02,  9.05987059e-03,\n       -4.63119627e-03,  1.05235817e-02, -7.43702104e-03, -1.22062472e-02,\n        4.11623022e-02, -2.23490333e-02, -3.17178546e-02,  3.45309242e-02,\n        8.64316002e-03,  9.57089467e-03,  2.88525618e-02,  3.49899238e-02,\n        2.21508425e-02,  3.72112010e-02,  1.16919064e-02,  1.77118242e-02,\n       -3.24603405e-02,  1.27001670e-02, -4.74187985e-02,  4.00286316e-02,\n       -2.58404466e-02, -3.07331701e-03, -7.81151761e-03, -2.90424255e-02,\n        8.13580750e-03, -2.39895708e-02, -6.36117717e-03,  2.63070654e-03,\n       -1.05461221e-02,  2.92048113e-02, -1.86299124e-04, -4.12830566e-03,\n       -1.17998033e-02, -1.18160992e-02,  4.32107807e-04, -1.81627229e-02,\n        1.98359185e-02,  1.00646166e-02, -1.26461222e-02, -2.60622704e-03,\n        3.67424002e-02,  1.82104425e-03,  1.52220352e-02, -5.35907575e-03,\n       -2.18962499e-04,  6.61174007e-04,  2.27252797e-02, -2.92407883e-03,\n       -1.26736405e-02,  2.40680871e-02,  2.98768975e-03,  2.77788734e-02,\n        3.46233953e-03,  3.36272793e-02, -8.12321169e-03, -1.54525772e-02,\n       -3.43789613e-03,  3.29625701e-03, -1.72737662e-02, -3.50416112e-02,\n       -2.10695692e-02,  2.04856817e-02,  5.33495766e-03,  1.70959501e-02,\n       -4.01938437e-02, -1.96291282e-03, -1.60972712e-03, -2.82954716e-02,\n       -3.12503945e-02, -8.00801978e-03, -3.75004791e-03, -9.50506474e-03,\n        3.71845637e-02,  3.01580206e-02, -9.18347705e-03, -1.32259447e-02,\n       -2.16457975e-02,  2.33490331e-02,  3.56516909e-03, -6.48253565e-03,\n        3.82506917e-03,  2.36159112e-03,  5.84204891e-03,  2.04932024e-02,\n        9.97363648e-03, -1.95683501e-02, -2.44179291e-02, -9.65308557e-03,\n        9.34570056e-03,  2.66407297e-03, -3.36869156e-02,  1.24567631e-03,\n        1.09435878e-02,  1.52749433e-02,  9.14501791e-03,  8.26701313e-03,\n       -3.15384743e-03,  1.25007952e-02, -2.14876244e-02, -9.62489113e-03,\n        7.88850820e-03,  1.95622459e-02, -2.08497673e-02, -1.14023050e-02,\n        1.86448121e-02, -1.02349355e-02, -8.32842028e-03, -2.21430431e-02,\n       -5.45592033e-03, -1.43473437e-02,  1.78245698e-02,  1.71387693e-02,\n       -1.86369432e-03])\n\n\n\n\nCode\nfor i in range(len(pacf)):\n    if sig_test(pacf[i]) == False:\n        n_steps = i - 1\n        print('n_steps set to', n_steps)\n        break\n\n\nn_steps set to 1\n\n\nVemos que para la serie diferencidad tenemos un resultado igual que para la serie original, asì tomaremos lo mismo 3 retrasos"
  },
  {
    "objectID": "DBitcoin.html",
    "href": "DBitcoin.html",
    "title": "Bitcoin",
    "section": "",
    "text": "#datos\nBTC_Daily &lt;- read.csv(\"datos/BTC-Daily.csv\")\nData &lt;- data.frame(BTC_Daily$date,BTC_Daily$close)\nData &lt;- data.frame(BTC_Daily$date,BTC_Daily$close)\ncolnames(Data) &lt;- c(\"FechaTiempo\", \"Valor\")\n# limpiando datos faltantes\nstr(Data)\n\n'data.frame':   2651 obs. of  2 variables:\n $ FechaTiempo: chr  \"2022-03-01 00:00:00\" \"2022-02-28 00:00:00\" \"2022-02-27 00:00:00\" \"2022-02-26 00:00:00\" ...\n $ Valor      : num  43185 43179 37713 39147 39232 ...\n\nData$FechaTiempo &lt;- strftime(Data$FechaTiempo, format=\"%Y-%m-%d\")\nstr(Data)\n\n'data.frame':   2651 obs. of  2 variables:\n $ FechaTiempo: chr  \"2022-03-01\" \"2022-02-28\" \"2022-02-27\" \"2022-02-26\" ...\n $ Valor      : num  43185 43179 37713 39147 39232 ...\n\nData$FechaTiempo &lt;- as.Date(Data$FechaTiempo)\n# procesamiento de los datos\n\nBitcoin &lt;- Data %&gt;%\n      filter(FechaTiempo &gt;= as.Date(\"2017-01-01\"),\n             FechaTiempo &lt;= as.Date(\"2021-12-31\"))\nstr(Bitcoin)\n\n'data.frame':   1826 obs. of  2 variables:\n $ FechaTiempo: Date, format: \"2021-12-31\" \"2021-12-30\" ...\n $ Valor      : num  46214 47151 46483 47543 50718 ...\n\n\n\n\n\n# objeto serie de tiempo\nData_xts &lt;- xts::xts(Bitcoin$Valor, order.by = Bitcoin$FechaTiempo)\nhead(Data_xts)\n\n              [,1]\n2017-01-01  998.80\n2017-01-02 1014.10\n2017-01-03 1036.99\n2017-01-04 1122.56\n2017-01-05  994.02\n2017-01-06  891.56\n\nTSstudio::ts_info(Data_xts)\n\n The Data_xts series is a xts object with 1 variable and 1826 observations\n Frequency: daily \n Start time: 2017-01-01 \n End time: 2021-12-31 \n\n#class(Data_xts)\n#frequency(Data_xts)\n#xts::periodicity(Data_xts)\n#xts::tclass(Data_xts)\n#plot(Data_xts)\n\nVista de los datos para inspección visual\n\nTSstudio::ts_plot(Data_xts,\n         title = \"Valor de cierre bitcoin en bolsa\",\n         Ytitle = \"Valor en dolares\",\n          Xtitle = \"Fecha\",\n          Xgrid = TRUE,\n       Ygrid = TRUE)\n\n\n\n\n\n\nVarianza marginal: Se notan periodos donde el rango de valores que puede tomar la variable se va fluctuando a medida que pasa el tiempo.\nComponente Estacional: No se evidencia un comportamiento cíclico en la serie.\nTendencia: Se muestra la serie no oscila sobre un valor fijo y tiene cambios abruptos de crecimiento y decrecimiento en algunos momentos.\n\n\n\n\nUsaremos la transformación de Box-Cox para estabilizar la varianza; primero miramos el lambda\n\n#Valor de lambda\nforecast::BoxCox.lambda(Data_xts, method =\"loglik\", lower = -1, upper = 3)\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\n[1] 0\n\nMASS::boxcox(lm(Data_xts ~ 1),seq(-1/2, 1/2, length = 50))\n\n\n\n\nVemos que se sugiere el valor \\(\\lambda = 0\\) lo cual dada de transformación de Box-Cox se usa la función logaritmo natural para la estabilización de la variabilidad así tenemos que:\n\n#trasnformación\nlData_xts &lt;- log(Data_xts)\n#plot(lData_xts)\n\n\nTSstudio::ts_plot(lData_xts,\n          title = \"Valor de Serie Trasnformada\",\n           Ytitle = \"Valor de la trasnformación\",\n          Xtitle = \"Fecha\",\n          Xgrid = TRUE,\n           Ygrid = TRUE)\n\n\n\n\n\nAhora miramos si es necesario aplicar otra transformación a la serie\n\n#Valor de lambda\n(forecast::BoxCox.lambda(lData_xts, method =\"loglik\", lower = -1, upper = 3))\n\n[1] 0.9\n\nMASS::boxcox(lm(lData_xts ~ 1),seq(-1, 2, length = 50))\n\n\n\n\nVemos que la sugerencia es \\(\\lambda = 0.9\\) lo cual es cercano a \\(1\\), además el IC de confianza captura al \\(1\\), por ende la transformación logarítmica parece haber estabilizado la varianza.\n\n#Gráfico de ellas juntas\npar(mfrow=c(2,1))\nplot(Data_xts, main = \"Series original\")\nplot(lData_xts, main = \"Series transformada\")\n\n\n\n\nSe puede ver cómo la transformación aplicada logra estabilizar la varianza en gran medida.\n\n\n\nTrabajaremos con la serie a la cuál se le realizo la transformación para estabilizar la varianza, realizaremos el gráfico de los valores de la función de auto-correlación\n\n#ACf\nacf(lData_xts, 180, main = \"Serie Bitcoin Trasnformada\")\n\n\n\n\nNotamos que los valores van teniendo un decaimiento leve lo cual nos da un indicio más claro de que existe tendencia en la serie, analizaremos el gráfico de retardos de la serie trasnformada para ver si podemos tener indicios de una relación no-lineal o lineal en la serie.\n\n#serie transformada\npar(mar = c(3,2,3,2))\nastsa::lag1.plot(lData_xts, 16,corr=T)\n\n\n\n\nVemos que se nota un fuerte relación linea hasta para el retraso número 16, por lo tanto con lo mostrado por el acf y el gráfico de retardos tenemos indicios fuertes de tendencia en la serie así usaremos los métodos: lineal determinista, Descomposición de promedios móviles y descomposición STL para estimar dicha componente.\n\n\najustaremos el modelo eliminaremos la tendencia y analizaremos los resultados\n\n#pasar a ts para Graficarlo\nldata_ts &lt;- TSstudio::xts_to_ts(lData_xts,frequency = 365,\n                                start = as.Date(\"2017-01-01\"))\n#modelo lineal\nsummary(fit &lt;- lm(ldata_ts~time(ldata_ts), na.action=NULL))\n\n\nCall:\nlm(formula = ldata_ts ~ time(ldata_ts), na.action = NULL)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.06366 -0.49265  0.01902  0.36341  1.68667 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.230e+03  1.780e+01  -69.10   &lt;2e-16 ***\ntime(ldata_ts)  6.135e-01  8.813e-03   69.61   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5439 on 1824 degrees of freedom\nMultiple R-squared:  0.7265,    Adjusted R-squared:  0.7264 \nF-statistic:  4846 on 1 and 1824 DF,  p-value: &lt; 2.2e-16\n\n# Gráfico\nplot(ldata_ts, ylab= \"Valor en escala logarítmica\")\nabline(fit,col = \"red\")# Se añade la recta ajusta\n\n\n\n\nahora eliminaremos la tendencia de la serie\n\n###Eliminamos la tendencia con la predicción la recta\nElimTenldata_ts &lt;- ldata_ts - predict(fit)\nplot(ElimTenldata_ts, main=\"Serie Sin tendencia\", \n     ylab= \"Valor en escala logarítmica\")\n\n\n\n\nObservamos que en la serie obtenida después de eliminar la tendencia lineal parece tener un comportamiento de alta variabilidad similar una caminata aleatoria.\n\nacf(ElimTenldata_ts,lag.max =length(ElimTenldata_ts), \n    main=\"Serie Sin tendencia\")\n\n\n\n# Series trasnfromada sin tendencia Retardos\npar(mar = c(3,2,3,2))\nastsa::lag1.plot(ElimTenldata_ts, 16,corr=F)\n\n\n\n\nNotamos que en la gráfica del acf se sigue teniendo un decaimiento lento de los valores de la función de auto-correlación para los primeros rezagos, además en el gráfico de retardos se sigue manteniendo una alta relación lineal entre el valor actual y sus regazos. Por ende todo esto nos da los argumentos necesarios para descartar la estimación linealcómo una buena estimación de la tendencia.\n\n\n\n\n# descomposición de promedios moviles\ndescom_ldata &lt;- decompose(ldata_ts)\nplot(descom_ldata)\n\n\n\n\nPodemos observar que usando un filtro de promedio móvil la tendencia estimada no se aproxima mucho a una lineal, cómo se puede apreciar en el siguiente gráfico; además la componente estacional no parece ser estimada de buena manera ya que no se ve un patrón de comportamiento claramente, además el residual presenta un comportamiento no estacionario aparentemente.\n\n# Gráfico\nplot(ldata_ts, ylab= \"Valor en escala logarítmica\")\nabline(fit,col = \"red\")# Se añade la recta ajusta\npoints(time(ldata_ts), descom_ldata$trend, col =\"green\", cex=0.3)\n\n\n\n\nEliminaremos la tendencia del promedio móvil centrado y de la frecuencia\n\n###Eliminamos la tendencia con la predicción promedio movil\nElimTenldata_ts &lt;- ldata_ts - descom_ldata$trend\nplot(ElimTenldata_ts, main=\"Serie Sin tendencia\", \n     ylab= \"Valor en escala logarítmica\")\n\n\n\n\nPodemos ver que la serie cómo en el caso lineal parece mostrar un comportamiento de caminata aleatoria.\n\nacf(ElimTenldata_ts[183:1644],lag.max = 730, \n    main=\"Serie Sin tendencia\")\n\n\n\n# Series trasnfromada sin tendencia Retardos\npar(mar = c(3,2,3,2))\nastsa::lag1.plot(ElimTenldata_ts[183:1644], 16,corr=F)\n\n\n\n\nNotamos que tanto en el acf cómo en la gráfica de retardos se ve un comportamiento similar al anterior lo cuál nos hace descartar la descompsición de promedios moviles para la estimación de la tendencia.\nFiltro promedio móvil con solo retrasos\nIntentaremos ajustar un promedio móvil que tenga en cuenta solo los retrasos y sea de los periodos de un año, seis meses, tres meses y mes.\n\n#gráfico\nfilter_1=stats::filter(ldata_ts, filter = rep(1/365, 365), sides = 1)\nfilter_2=stats::filter(ldata_ts, filter = rep(1/182, 182), sides = 1)\nfilter_3=stats::filter(ldata_ts, filter = rep(1/90, 90), sides = 1)\nfilter_4=stats::filter(ldata_ts, filter = rep(1/30, 30), sides = 1)\nplot(ldata_ts, ylab= \"Valor en escala logarítmica\")\npoints(time(ldata_ts), filter_1, col =\"green\", cex=0.33)\npoints(time(ldata_ts), filter_2, col =\"blue\", cex=0.33)\npoints(time(ldata_ts), filter_3, col =\"red\", cex=0.35)\npoints(time(ldata_ts), filter_4, col =\"cyan\", cex=0.31)\n\n\n\n#legend(locator(1), c(\"365 días\",\"182 días\",\"90 días\",\"30 días\"), col=c(\"green\",\"blue\",\"red\",\"cyan\"),lty=c(1,1,1,1),lwd=c(2,2,2,2))\n\nNotamos que para 3 meses y 6 meses los filtros de promedios móviles muestra una mejor estimación, por ende tomaremos para 3 meses cómo estimación de la tendencia de la serie\n\n###Eliminamos la tendencia con la predicción promedio movil\nElimTenldata_ts &lt;- ldata_ts - filter_3\nplot(ElimTenldata_ts, main=\"Serie Sin tendencia\", \n     ylab= \"Valor en escala logarítmica\")\n\n\n\n#\nacf(ElimTenldata_ts[90:1826],lag.max =1095, \n    main=\"Serie Sin tendencia\")\n\n\n\n# Series trasnfromada sin tendencia Retardos\npar(mar = c(3,2,3,2))\nastsa::lag1.plot(ElimTenldata_ts[90:1826], 16,corr=F)\n\n\n\n\nEl comportamiento de los gráficos es similar a los anteriores con eso tenemos indicios de que la estimación de la tendencia de manera determinista potencialmente no es buena idea.\n\n\n\nUsando la descomposición STL obtenemos la estimación de la tendencia\n\nindice_ldata &lt;- sort(Bitcoin$FechaTiempo)\n#  as.Date(as.yearmon(tk_index(ldata_ts)))\n## Otra forma de extraer el indice estimetk::tk_index(lAirPass)\nlogdata &lt;- as.matrix(ldata_ts)\ndf_ldata &lt;- data.frame(Fecha=indice_ldata,logdata=as.matrix(ldata_ts))\nstr(df_ldata)\n\n'data.frame':   1826 obs. of  2 variables:\n $ Fecha   : Date, format: \"2017-01-01\" \"2017-01-02\" ...\n $ Series.1: num  6.91 6.92 6.94 7.02 6.9 ...\n\ncolnames(df_ldata) &lt;- c(\"Fecha\", \"logdata\")\nstr(df_ldata)\n\n'data.frame':   1826 obs. of  2 variables:\n $ Fecha  : Date, format: \"2017-01-01\" \"2017-01-02\" ...\n $ logdata: num  6.91 6.92 6.94 7.02 6.9 ...\n\ntibble_ldata &lt;- tibble(df_ldata)\n####Primera aproximación del ajuste STL\ntibble_ldata%&gt;%timetk::plot_time_series(Fecha, logdata, \n                   .interactive = TRUE,\n                   .plotly_slider = TRUE)\n\n\n\n\n\n\n#####Ajuste STL\n#Note que obtenemos un objeto adicional en tibble_logpasajeros con Logpasa_ajus con parámetros que se pueden mover.\nlogdata_ajus &lt;- smooth_vec(logdata,span = 0.75, degree = 2)\ntibble_ldata%&gt;%dplyr::mutate(logdata_ajus)\n\n# A tibble: 1,826 × 3\n   Fecha      logdata logdata_ajus\n   &lt;date&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n 1 2017-01-01    6.91         6.90\n 2 2017-01-02    6.92         6.91\n 3 2017-01-03    6.94         6.91\n 4 2017-01-04    7.02         6.92\n 5 2017-01-05    6.90         6.92\n 6 2017-01-06    6.79         6.93\n 7 2017-01-07    6.80         6.94\n 8 2017-01-08    6.81         6.94\n 9 2017-01-09    6.80         6.95\n10 2017-01-10    6.81         6.96\n# ℹ 1,816 more rows\n\n###Ajuste STL moviendo los parámetros\ntibble_ldata%&gt;%mutate(logdata_ajus=smooth_vec(logdata,span = 0.75, degree = 2))%&gt;%\n  ggplot(aes(Fecha, logdata)) +\n    geom_line() +\n    geom_line(aes(y = logdata_ajus), color = \"red\")\n\n\n\n\nSe puede evidenciar que la Estimación de la tendencia via STL parece mejorar aspectos que la descomposición movil intentada con información de un año no se tenia.\n\n###Eliminamos la tendencia con la predicción la STL\nElimTenldata_xts &lt;- lData_xts - logdata_ajus\nplot(ElimTenldata_xts, main=\"Serie Sin tendencia\", \n     ylab= \"Valor en escala logarítmica\")\n\n\n\nacf(ElimTenldata_xts,lag.max =1094, \n    main=\"Serie Sin tendencia\")\n\n\n\n# Series trasnfromada sin tendencia Retardos\npar(mar = c(3,2,3,2))\nastsa::lag1.plot(ElimTenldata_xts, 16,corr=F)\n\n\n\n\nVemos que a diferencia de los promedio móviles si tenemos una estimación para todos los valores de la serie, además notamos que la acf y el gráfico de retardos tiene un comportamiento similar a los métodos anteriores.\n\n\n\n\n\n###Diferenciando con base en el objeto ts\ndldata&lt;-diff(ldata_ts)\n#plot(dldata)\n#abline(h=0, col = \"red\")\n#acf(dldata,lag.max =90, main=\"Serie Diferenciada\")\n\n\n###Diferenciando con base en el objeto xts\ndldata_xts&lt;-diff(lData_xts)\ndldata_xts &lt;- dldata_xts[-1]\nplot(dldata_xts, main = \"Serie diferenciada\")\n\n\n\n\nVemos que la serie al ser diferenciada muestra un comportamiento estacionario pues los valores oscilan sobre un valor fijo, además de un valor que muestra un comportamiento extremo pues varia demasiado con respecto a los demás.\n\n# función de autocorrelación\nacf(dldata_xts,lag.max =30, main=\"Serie Diferenciada\")\n\n\n\n# Series trasnfromada sin tendencia Retardos\npar(mar = c(3,2,3,2))\nastsa::lag1.plot(dldata_xts, 16,corr=T)\n\n\n\n\nEl gráfico de acf muestra que la tendencia parce a ver desparecido, además no parece destacar ningún valor para algún retraso de manera clara. Para el gráfico de retardos vemos claramente que ya no hay una relación lineal ni no lineal del valor actual con sus retardos.\nTomando en cuenta todo lo anterior trabajaremos con la serie aplicada la transformación de Box-Cox sugerida y diferenciada, además se tiene sospecha de que la serie presenta comportamiento de una camina aleatoria con tendencia no determinista.\n\n\n\n\n\n\n#Serie diferenciada\nTSstudio::ts_heatmap(dldata_xts, title = \"Mapa de calor - Cierre Bitcoin en bolsa dias año\")\n\n\n\n\n\nPara la serie diferenciada no se evidencia ninguno tipo de patrón lo cual nos da indicios de que no se tiene un componente estacional.\n\n\n\n\n# periodograma\nPeriodograma &lt;- spectrum(as.numeric(dldata_xts),log=\"no\",span=c(5,5))\n\n\n\n#\nubicacionDif &lt;- which.max(Periodograma$spec)\nsprintf(\"El valor de la frecuencia donde se máximiza el periodograma para la serie es: %s\",Periodograma$freq[ubicacionDif])\n\n[1] \"El valor de la frecuencia donde se máximiza el periodograma para la serie es: 0.4016\"\n\n#\nsprintf(\"El periodo correspondiente es aproximadamente: %s\",1/Periodograma$freq[ubicacionDif])\n\n[1] \"El periodo correspondiente es aproximadamente: 2.49003984063745\"\n\n\nPara la serie diferenciada el periodograma no es claro, a pesar del suviazamiento usado la curva sigue mostrando varios picos en su recorrido el máximo lo encontramos de tal manera que \\(\\omega = \\frac{251}{625}=0.416\\) lo cual se corresponde con un \\(s \\approx 2.5\\) .\n\n# intentando sacar el segundo más alto\nn_dld &lt;- length(Periodograma$spec)\nvalor_seg &lt;- sort(Periodograma$spec,partial=n_dld-1)[n_dld-1]\nubica_segundo &lt;- which(Periodograma$spec==valor_seg)\n\nsprintf(\"El valor de la frecuencia donde se alcanza el segundo máximo para el periodograma para REC es: %s\",Periodograma$freq[ubica_segundo])\n\n[1] \"El valor de la frecuencia donde se alcanza el segundo máximo para el periodograma para REC es: 0.401066666666667\"\n\n#\nsprintf(\"El periodo correspondiente es aproximadamente: %s\",1/Periodograma$freq[ubica_segundo])\n\n[1] \"El periodo correspondiente es aproximadamente: 2.49335106382979\"\n\n\nvemos que el segundo valor es bastante parecido al segundo.\n\n# valor de frecuencia \ntail(sort(Periodograma$spec))\n\n[1] 0.004895118 0.004914000 0.005095353 0.005470611 0.005752230 0.005966273\n\n\nCómo se puede observar los primeros seis valores son bastante cercanos entre ellos, por lo tanto sus valores de periodo serán similiares.\nCon esto descartamos la estimación de una componente estacional pues no tenemos evidencia clara de su existencia."
  },
  {
    "objectID": "DBitcoin.html#serie-diferenciada",
    "href": "DBitcoin.html#serie-diferenciada",
    "title": "Bitcoin",
    "section": "",
    "text": "###Diferenciando con base en el objeto ts\ndldata&lt;-diff(ldata_ts)\n#plot(dldata)\n#abline(h=0, col = \"red\")\n#acf(dldata,lag.max =90, main=\"Serie Diferenciada\")\n\n\n###Diferenciando con base en el objeto xts\ndldata_xts&lt;-diff(lData_xts)\ndldata_xts &lt;- dldata_xts[-1]\nplot(dldata_xts, main = \"Serie diferenciada\")\n\n\n\n\nVemos que la serie al ser diferenciada muestra un comportamiento estacionario pues los valores oscilan sobre un valor fijo, además de un valor que muestra un comportamiento extremo pues varia demasiado con respecto a los demás.\n\n# función de autocorrelación\nacf(dldata_xts,lag.max =30, main=\"Serie Diferenciada\")\n\n\n\n# Series trasnfromada sin tendencia Retardos\npar(mar = c(3,2,3,2))\nastsa::lag1.plot(dldata_xts, 16,corr=T)\n\n\n\n\nEl gráfico de acf muestra que la tendencia parce a ver desparecido, además no parece destacar ningún valor para algún retraso de manera clara. Para el gráfico de retardos vemos claramente que ya no hay una relación lineal ni no lineal del valor actual con sus retardos.\nTomando en cuenta todo lo anterior trabajaremos con la serie aplicada la transformación de Box-Cox sugerida y diferenciada, además se tiene sospecha de que la serie presenta comportamiento de una camina aleatoria con tendencia no determinista."
  },
  {
    "objectID": "DBitcoin.html#análisis-de-estacionalidad",
    "href": "DBitcoin.html#análisis-de-estacionalidad",
    "title": "Bitcoin",
    "section": "",
    "text": "#Serie diferenciada\nTSstudio::ts_heatmap(dldata_xts, title = \"Mapa de calor - Cierre Bitcoin en bolsa dias año\")\n\n\n\n\n\nPara la serie diferenciada no se evidencia ninguno tipo de patrón lo cual nos da indicios de que no se tiene un componente estacional.\n\n\n\n\n# periodograma\nPeriodograma &lt;- spectrum(as.numeric(dldata_xts),log=\"no\",span=c(5,5))\n\n\n\n#\nubicacionDif &lt;- which.max(Periodograma$spec)\nsprintf(\"El valor de la frecuencia donde se máximiza el periodograma para la serie es: %s\",Periodograma$freq[ubicacionDif])\n\n[1] \"El valor de la frecuencia donde se máximiza el periodograma para la serie es: 0.4016\"\n\n#\nsprintf(\"El periodo correspondiente es aproximadamente: %s\",1/Periodograma$freq[ubicacionDif])\n\n[1] \"El periodo correspondiente es aproximadamente: 2.49003984063745\"\n\n\nPara la serie diferenciada el periodograma no es claro, a pesar del suviazamiento usado la curva sigue mostrando varios picos en su recorrido el máximo lo encontramos de tal manera que \\(\\omega = \\frac{251}{625}=0.416\\) lo cual se corresponde con un \\(s \\approx 2.5\\) .\n\n# intentando sacar el segundo más alto\nn_dld &lt;- length(Periodograma$spec)\nvalor_seg &lt;- sort(Periodograma$spec,partial=n_dld-1)[n_dld-1]\nubica_segundo &lt;- which(Periodograma$spec==valor_seg)\n\nsprintf(\"El valor de la frecuencia donde se alcanza el segundo máximo para el periodograma para REC es: %s\",Periodograma$freq[ubica_segundo])\n\n[1] \"El valor de la frecuencia donde se alcanza el segundo máximo para el periodograma para REC es: 0.401066666666667\"\n\n#\nsprintf(\"El periodo correspondiente es aproximadamente: %s\",1/Periodograma$freq[ubica_segundo])\n\n[1] \"El periodo correspondiente es aproximadamente: 2.49335106382979\"\n\n\nvemos que el segundo valor es bastante parecido al segundo.\n\n# valor de frecuencia \ntail(sort(Periodograma$spec))\n\n[1] 0.004895118 0.004914000 0.005095353 0.005470611 0.005752230 0.005966273\n\n\nCómo se puede observar los primeros seis valores son bastante cercanos entre ellos, por lo tanto sus valores de periodo serán similiares.\nCon esto descartamos la estimación de una componente estacional pues no tenemos evidencia clara de su existencia."
  },
  {
    "objectID": "DExportaciones.html",
    "href": "DExportaciones.html",
    "title": "Exportaciones",
    "section": "",
    "text": "Code\nlibrary(TSstudio)\nlibrary(zoo)\nlibrary(xts)\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(feasts)\nlibrary(fable)\nlibrary(timetk)\nlibrary(tsibble)\nlibrary(lubridate)\nlibrary(dygraphs)\nlibrary(MASS)\n\n\n\n1 Exportaciones mensuales de Colombia\n\n\nCode\n#Gráfico dinámico con dygraphs\ndygraph(exportaciones_ts,main=\"Serie de Exportaciones Mensuales en Colombia\", ylab=\"Valor total de las exportaciones\")%&gt;% dyRangeSelector()\n\n\n\n\n\n\n\n\nCode\n#Obtener información del objeto de tipo ts\nTSstudio::ts_info(exportaciones_ts)\n\n\n The exportaciones_ts series is a ts object with 1 variable and 282 observations\n Frequency: 12 \n Start time: 2000 1 \n End time: 2023 6 \n\n\nLa serie de tiempo de exportaciones de Colombia presnetada en el gráfico es una serie de tiempo mensual con frecuencia 12 con valores desde enero del 2000 hasta junio del 2023, sin ninguna observación faltante, o sea que cuenta con 282 observaciones. Del gráfico podemos suponer las siguientes características de la serie:\n\nVarianza marginal: La serie pareciera tener varianza marginal no constante, pues su rango de valores resulta ser pequeño en ciertos periodos de tiempo y grande en otros. Por ejemplo, de noviembre de 2008 a febrero de 2011 hay un rango grande, de marzo de 2011 a octubre de 2014 hay un rango pequeño y de noviembre de 2014 a febrero de 2016 hay de nuevo un rango grande.\nTendencia: A simple vista la serie presenta una tendencia determinística lineal creciente en los primeros años de observación. Sin embargo, desde el año 2011 la serie no sigue el mismo patrón y su tendencia empieza a variar por periodos de tiempo, por lo que se puede pensar en una tendencia de tipo estocástico.\nComponente Estacional: Haciendo una observación detallada de la serie acortando los años con ayuda del gráfico dinámico, se puede ver que la serie presenta picos en los meses de mayo y noviembre. Por los que se cree que hay existencia de estacionalidad de periodo 6 meses.\n\n\n1.0.1 Estabilización de la Varianza\nA continuación se evaluará si es necesario y conveniente hacer una tranformación de Box-Cox para estabilizar la varianza marginal de la serie.\nVeremos primero qué valores del parámetro lambda de la transformación de Box-Cox nos maximiza la log-verosimilitud. Si estos valores están lejos de 1 se sugiere hacer una transformación de Box-Cox.\n\n\nCode\nMASS::boxcox(lm(exportaciones_ts ~ 1),seq(-1, 2, length = 500))\n\n\n\n\n\nA patir de la gráfica anterior se verifica que los valores de lambda que maximizan la log-verosimilitud estan alrededor de 0.6, con un intervalo que no incluye al 1. Se toma la decisión de transformar la serie con lambda igual a \\(0.6\\) y se observam los resultados en comparación con la serie original.\n\n\nCode\nlexportaciones_ts &lt;- (1/0.6)*((exportaciones_ts^(0.6))-1)\n\n\n\n\nCode\npar(mar = c(1,1,1,1))\npar(mfrow=c(1,2))\ndygraph(exportaciones_ts,main=\"Serie de Exportaciones Mensuales en Colombia\", ylab=\"Valor total de las exportaciones\")%&gt;% dyRangeSelector()\n\n\n\n\n\n\nCode\ndygraph(lexportaciones_ts,main=\"Serie transformada con lambda=0.6\", ylab=\"Valor\")%&gt;% dyRangeSelector()\n\n\n\n\n\n\nSe nota un ligero cambio en la varianza marginal de la serie, en particular una disminución general de la varianza marginal. Se verifican los valores del parámetro lambda de la transforación de Box-Cox que maximizan la log-verosimilitud de la serie transformada.\n\n\nCode\nMASS::boxcox(lm(lexportaciones_ts ~ 1),seq(-1, 2, length = 500))\n\n\n\n\n\nEl valor de lambda que maximiza la log-verosimilitud sobre la serie transformada es ahora muy cercano a 1, por lo que se considera una buena transformación. Sin embargo, dado que el cambio no es muy notorio se toma la recomendación de ser flexibles con el valor de lambda y se realiza una transformación que es más notoria y que cumple con incluir al uno entre los valores de lambda que maximizan la verosimilitud.\nEl valor usado será lambda = 0.45, obteniendo los resultados siguientes\n\n\nCode\nlexportaciones_ts &lt;- (1/0.45)*((exportaciones_ts^(0.45))-1)\n\n\n\n\nCode\npar(mar = c(1,1,1,1))\npar(mfrow=c(2,1))\ndygraph(exportaciones_ts,main=\"Serie de Exportaciones Mensuales en Colombia\", ylab=\"Valor total de las exportaciones\")%&gt;% dyRangeSelector()\n\n\n\n\n\n\nCode\ndygraph(lexportaciones_ts,main=\"Serie transformada con lambda=0.45\", ylab=\"Valor\")%&gt;% dyRangeSelector()\n\n\n\n\n\n\n\n\nCode\nMASS::boxcox(lm(lexportaciones_ts ~ 1),seq(-1, 5, length = 500))\n\n\n\n\n\nSe decide continuar con esta última serie obtenida.\n\n\nCode\n#Otras opciones de transformación \nlibrary(VGAM)\n\n\nLoading required package: stats4\n\n\nLoading required package: splines\n\n\nCode\nprueba&lt;-VGAM::yeo.johnson(exportaciones_ts, lambda = 0.4)\n\n\n\n\nCode\ndygraph(prueba,main=\"Serie transformada con yeo.jhonson\", ylab=\"Valor\")%&gt;% dyRangeSelector()\n\n\n\n\n\n\n\n\n1.0.2 Análisis de Tendencia\nInicialmente revisaremos la función de autocorrelación, pues su forma nos dará indicios de si exisste tendencia o no.\n\n\nCode\nacf(lexportaciones_ts, 36, main = \"ACF de la serie estabilizada\")\n\n\n\n\n\nLa serie presenta alta autocorrelación de los rezagos con un decaimiento leve a medida que el rezago es mayor, estoo es un indicio de que la serie presenta tendencia. A continuación se realizaran ajustes determinísticos de la misma.\n\n1.0.2.1 Ajuste determinístico lineal\n\n\nCode\nsummary(fit &lt;- lm(lexportaciones_ts~time(lexportaciones_ts), na.action=NULL))\n\n\n\nCall:\nlm(formula = lexportaciones_ts ~ time(lexportaciones_ts), na.action = NULL)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-633.53 -195.39  -61.69  170.42  666.72 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             -81496.270   4705.423  -17.32   &lt;2e-16 ***\ntime(lexportaciones_ts)     41.382      2.339   17.69   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 266.5 on 280 degrees of freedom\nMultiple R-squared:  0.5278,    Adjusted R-squared:  0.5261 \nF-statistic:   313 on 1 and 280 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n\nCode\nplot(lexportaciones_ts, main= \"Tendencia lineal ajustada\")\nabline(fit,col = \"red\")# Se añade la recta ajusta\n\n\n\n\n\n\n\n1.0.2.2 Serie sin tendencia\n\n\nCode\nnoTendlexportaciones_ts &lt;- lexportaciones_ts - predict(fit)\nplot(noTendlexportaciones_ts, main=\"Serie Sin tendencia\", \n     ylab= \"Valor\")\n\n\n\n\n\n\n\nCode\nacf(noTendlexportaciones_ts,lag.max =length(noTendlexportaciones_ts), \n    main=\"ACF Serie Sin tendencia\")\n\n\n\n\n\n\n\n1.0.2.3 Ajuste por medio de filtro de promedios móviles\n\n\nCode\ndescom_lexportaciones &lt;- decompose(lexportaciones_ts)\nplot(descom_lexportaciones)\n\n\n\n\n\n\n\nCode\nplot(lexportaciones_ts, ylab= \"Valor en escala logarítmica\")\nabline(fit,col = \"red\")# Se añade la recta ajusta\npoints(time(lexportaciones_ts), descom_lexportaciones$trend, col =\"green\", cex=0.4)\n\n\n\n\n\nLa línea de tendencia ajustada po medio del filtro de promedios móviles es muy distinta a la línea de tendencia lineal ajustada. Veamos como queda la serie al eliminar la tendencia usando este ajuste.\n\n\nCode\nnoTendlexportaciones_ts2 &lt;- lexportaciones_ts - descom_lexportaciones$trend\nplot(noTendlexportaciones_ts2, main=\"Serie Sin tendencia\", \n     ylab= \"Valor\")\n\n\n\n\n\nSe observa que esta estimación de la tendencia se ajusta mejor, pues al eliminar la tendencia la serie oscila constantemente alrededor de cero.\nSin embargo, a pesar que se estabilizó la varianza previamente, se sigue notando algo de varianza marginal no constante y de forma más notoria. Al inicio varinza baja y al final varianza alta.\n\n\nCode\nacf(noTendlexportaciones_ts2[7:276],lag.max =length(noTendlexportaciones_ts2), \n    main=\"ACF serie Sin tendencia\")\n\n\n\n\n\n\n\nCode\nacf(noTendlexportaciones_ts2[7:276],lag.max =48, \n    main=\"ACF serie Sin tendencia\")\n\n\n\n\n\n\n\n\n\n2 Descomposición STL\n\n\nCode\n####Primera aproximación del ajuste STL\nlexportaciones_tbl%&gt;%timetk::plot_time_series(Fecha, lexportaciones, \n                   .interactive = TRUE,\n                   .plotly_slider = TRUE)\n\n\n\n\n\n\n\n\nCode\n#####Ajuste STL\n\nlexportaciones_ajus &lt;- smooth_vec(lexportaciones,span = 0.75, degree = 2)\nlexportaciones_tbl%&gt;%dplyr::mutate(lexportaciones_ajus)\n\n\n# A tibble: 282 × 3\n   Fecha      lexportaciones lexportaciones_ajus\n   &lt;date&gt;              &lt;dbl&gt;               &lt;dbl&gt;\n 1 2000-01-01          1117.               1021.\n 2 2000-02-01          1138.               1026.\n 3 2000-03-01          1138.               1031.\n 4 2000-04-01          1053.               1036.\n 5 2000-05-01          1182.               1041.\n 6 2000-06-01          1186.               1046.\n 7 2000-07-01          1162.               1051.\n 8 2000-08-01          1226.               1056.\n 9 2000-09-01          1162.               1061.\n10 2000-10-01          1102.               1067.\n# ℹ 272 more rows\n\n\n\n\nCode\n###Ajuste STL moviendo los parámetros\nlexportaciones_tbl%&gt;%mutate(lexportaciones_ajus=smooth_vec(lexportaciones,span = 0.75, degree = 2))%&gt;%\n  ggplot(aes(Fecha, lexportaciones)) +\n    geom_line() +\n    geom_line(aes(y = lexportaciones_ajus), color = \"red\")\n\n\n\n\n\nSe observa que el ajuste es mucho más suavizado.\n\n\nCode\n###Eliminamos la tendencia con la predicción la recta xts\nnoTendlexportaciones_ts3 &lt;- lexportaciones_ts - lexportaciones_ajus\nplot(noTendlexportaciones_ts3, main=\"Serie Sin tendencia\", \n     ylab= \"Valor en escala logarítmica\") \n\n\n\n\n\n\n\nCode\nacf(noTendlexportaciones_ts3,lag.max =length(noTendlexportaciones_ts3), \n    main=\"ACF serie Sin tendencia\")\n\n\n\n\n\n\n2.0.0.1 Serie Diferenciada\n\n\nCode\n###Diferenciando con base en el objeto ts\ndlexportaciones&lt;-diff(lexportaciones_ts)\nplot(dlexportaciones)\nabline(h=0, col = \"red\")\n\n\n\n\n\nAl observar la serie diferenciada se hace más evidente la presencia de varianza marginal no constante en la serie, lo que quiere decir que las tansformación no la ha corregido lo suficiente.\n\n\nCode\nacf(dlexportaciones,lag.max =length(dlexportaciones), main=\"ACF de la Serie Diferenciada\")\n\n\n\n\n\n\n\nCode\nacf(dlexportaciones,lag.max =100, main=\"ACF de la Serie Diferenciada\")\n\n\n\n\n\nVarios rezagos superan el umbral de autocorrelación, en especial los que se encuentran cerca a las unidades. Por lo que se cree que hay presencia de una componente estacional de periodo 12.\nDadas las anteriores estimaciones de la tendencia y analizando cada una de las series con dichas tendencias estimadas eliminadas, se opta por continuar el estudio de la serie sin la tendencia dada por el filtro de promedios móviles y con la serie con la tendencia eliminada por medio de la diferenciación de la serie.\n\n\n2.0.1 Análisis de Estacionalidad\nA continuación se busca determinar si la serie presenta una componente estacional con la ayuda de diversos métodos. Se aplican los métodos sobre las series que fueron elegidas en la sección anterior.\n\n2.0.1.1 Correlación de las observaciones con sus retardos\nSe observa que tan correlacionadas estan las observaciones con sus retardos desde el 1 hasta el 12. Primero los resultados con la serie obtenida al estimar la tendencia con filtro de promedios móviles.\n\n\nCode\npar(mar = c(3,2,3,2))\nastsa::lag1.plot(noTendlexportaciones_ts2[7:276], 12,corr=T)\n\n\n\n\n\nLas correlaciones negativas que más destacan son las de los rezagos 4, 5 y 6. Y las positivas que mas destacan son las de los rezagos 1 y 12.\nAhora veamos de la serie diferenciada:\n\n\nCode\npar(mar = c(3,2,3,2))\nastsa::lag1.plot(dlexportaciones, 12,corr=T)\n\n\n\n\n\nLas correlaciones negativas que destacan son las de los rezagos 1 y 6, y la positiva es la del rezago 12. Coinciden en ser correlaciones negativas las del rezago 6 para ambos casos y en ser correlaciones positivas las del rezago 12 en ambos casos. Por lo que se sospecha de un ciclo estacional anual.\n\n\n2.0.1.2 Función de autocorrelación parcial\n\n\nCode\n#pacf\npacf(noTendlexportaciones_ts2[7:276], 100, main = \"PACF Serie sin tendencia\")\n\n\n\n\n\n\n\nCode\n#pacf\npacf(dlexportaciones, 100, main = \"PACF Serie Diferenciada\")\n\n\n\n\n\n\n\n2.0.1.3 AMI\nSobre la serie ajustada por medio del filtro de promedios móviles\n\n\nCode\ntseriesChaos::mutual(noTendlexportaciones_ts2[7:276], partitions = 16, lag.max = 50, plot=TRUE)\n\n\n\n\n\nAhora veamos sobre la serie diferenciada:\n\n\nCode\ntseriesChaos::mutual(dlexportaciones, partitions = 16, lag.max = 50, plot=TRUE)\n\n\n\n\n\n\n\n2.0.1.4 Mapas de calor\nCon la serie sin tendencia:\n\n\nCode\nprueba&lt;-xts(noTendlexportaciones_ts2[7:276],indice[7:276])\n\nTSstudio::ts_heatmap(prueba, title = \"Mapa de calor - Exportaciones sin tendencia\")\n\n\n\n\n\n\n\n\nCode\nTSstudio::ts_heatmap(dlexportaciones, title = \"Mapa de calor - Exportaciones sin tendencia\")\n\n\n\n\n\n\n\n\n2.0.1.5 Perdiodograma\n\n\nCode\n# periodograma\nPeriodograma &lt;- spectrum(as.numeric(noTendlexportaciones_ts2[7:276]),log=\"no\",span=c(5,5))\n\n\n\n\n\n\n\nCode\nubicacionElim&lt;- which.max(Periodograma$spec)\nsprintf(\"El valor de la frecuencia donde se máximiza el periodograma para la serie es: %s\",Periodograma$freq[ubicacionElim])\n\n\n[1] \"El valor de la frecuencia donde se máximiza el periodograma para la serie es: 0.0851851851851852\"\n\n\n\n\nCode\nsprintf(\"El periodo correspondiente es aproximadamente: %s\",1/Periodograma$freq[ubicacionElim])\n\n\n[1] \"El periodo correspondiente es aproximadamente: 11.7391304347826\"\n\n\n\n\nCode\n# periodograma\nPeriodograma &lt;- spectrum(as.numeric(dlexportaciones),log=\"no\",span=c(5,5))\n\n\n\n\n\n\n\nCode\nubicacionElim&lt;- which.max(Periodograma$spec)\nsprintf(\"El valor de la frecuencia donde se máximiza el periodograma para la serie es: %s\",Periodograma$freq[ubicacionElim])\n\n\n[1] \"El valor de la frecuencia donde se máximiza el periodograma para la serie es: 0.416666666666667\"\n\n\n\n\nCode\nsprintf(\"El periodo correspondiente es aproximadamente: %s\",1/Periodograma$freq[ubicacionElim])\n\n\n[1] \"El periodo correspondiente es aproximadamente: 2.4\"\n\n\n\n\n\n\n\n\n\n\n Volver arriba"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Series de tiempo univariadas",
    "section": "",
    "text": "Página web del curso 🤪.\n\n\n\n\n\n Volver arriba"
  },
  {
    "objectID": "ModelosDeArbolesSerieExportaciones.html",
    "href": "ModelosDeArbolesSerieExportaciones.html",
    "title": "Predicción 1 paso adelante usando 2 retardos",
    "section": "",
    "text": "Vamos a importar la bases de datos y a convertirlas en objetos de series de Tiempo. \\(\\{X_t\\}\\)\nCode\n# get working directory\nimport os\nos.getcwd()\n\n\n'C:\\\\Users\\\\dofca\\\\Desktop\\\\series'\nCode\n# librerias\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport sklearn\nimport openpyxl\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nimport warnings\nprint(f\"Matplotlib Version: {plt.__version__}\")\nprint(f\"Pandas Version: {pd.__version__}\")\nprint(f\"Numpy Version: {np.__version__}\")\nprint(f\"Sklearn: {sklearn.__version__}\")\n\n\nMatplotlib Version: 1.25.2\nPandas Version: 2.0.3\nNumpy Version: 1.25.2\nSklearn: 1.3.1\nCode\n# Lectura de la serie\ndata = pd.read_excel(\"datos/Exportaciones.xlsx\",\n                   header = 0, usecols = ['Mes','Total']).iloc[96:].reset_index(drop = True).round()\ndata['Total'] = data['Total'].astype(int) \ndata\n\n\n\n\n\n\n\n\n\nMes\nTotal\n\n\n\n\n0\n2000-01-01\n1011676\n\n\n1\n2000-02-01\n1054098\n\n\n2\n2000-03-01\n1053546\n\n\n3\n2000-04-01\n886359\n\n\n4\n2000-05-01\n1146258\n\n\n...\n...\n...\n\n\n277\n2023-02-01\n4202234\n\n\n278\n2023-03-01\n4431911\n\n\n279\n2023-04-01\n3739214\n\n\n280\n2023-05-01\n4497862\n\n\n281\n2023-06-01\n3985981\n\n\n\n\n282 rows × 2 columns\nCode\n# tipo de datos\nprint(data.info())\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 282 entries, 0 to 281\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype         \n---  ------  --------------  -----         \n 0   Mes     282 non-null    datetime64[ns]\n 1   Total   282 non-null    int32         \ndtypes: datetime64[ns](1), int32(1)\nmemory usage: 3.4 KB\nNone\nCode\n#mirando los datos\n#objeto ts\nexportaciones = data['Total']\nprint(type(exportaciones))\nplt.plot(exportaciones)\n\n\n&lt;class 'pandas.core.series.Series'&gt;\nCode\nprint(f'Numero de filas con valores faltantes: {data.isnull().any(axis=1).mean()}')\n\n\nNumero de filas con valores faltantes: 0.0\nCode\ndata.shape\n\n\n(282, 2)"
  },
  {
    "objectID": "ModelosDeArbolesSerieExportaciones.html#pacf",
    "href": "ModelosDeArbolesSerieExportaciones.html#pacf",
    "title": "Predicción 1 paso adelante usando 2 retardos",
    "section": "0.1 PACF",
    "text": "0.1 PACF\nusaremos la funcion de autocorrealcion parcial para darnos una idea de cuantos rezagos usaremos en el modelo\n\n\nCode\nfrom matplotlib import pyplot\nfrom statsmodels.graphics.tsaplots import plot_pacf\nplot_pacf(exportaciones,lags=140,method='ywm',alpha=0.01)\npyplot.show()\n\n\n\n\n\n\n\nCode\nimport statsmodels.api as sm\npacf =  sm.tsa.stattools.pacf(exportaciones, nlags=140,method='ywm')\nT = len(exportaciones)\n\nsig_test = lambda tau_h: np.abs(tau_h) &gt; 2.58/np.sqrt(T)\n\n\n\n\nCode\npacf\n\n\narray([ 1.00000000e+00,  9.59093603e-01,  2.97306056e-01,  1.17473996e-01,\n       -7.03116609e-02,  3.26288216e-02, -6.03088214e-02,  1.13079878e-01,\n       -1.13945940e-01, -2.33175139e-02, -1.11985646e-01,  2.02110227e-02,\n        1.97278486e-02, -1.61423863e-01, -1.73791666e-02, -2.59991629e-02,\n        2.41560086e-04,  7.51378723e-02, -2.72384453e-02,  7.60706013e-02,\n       -2.47900629e-02,  8.49355502e-02,  3.74581253e-03, -1.43352938e-02,\n        1.70410088e-02, -5.48238243e-02,  6.18273478e-02, -2.33524022e-02,\n        4.74134619e-02,  1.08676085e-01, -8.12916663e-02, -1.67238998e-02,\n       -3.96609731e-02, -1.05509554e-02,  5.15184774e-02, -4.09772449e-02,\n        8.88414578e-02, -1.42772584e-01,  4.04045194e-02, -7.10728718e-02,\n        6.10028381e-02, -9.75167769e-02, -4.70334839e-02,  8.36344066e-02,\n       -6.77084813e-02, -3.56866811e-02,  4.65635129e-02,  6.27645281e-02,\n        4.39629333e-02, -7.92100171e-02, -8.88109850e-02,  7.12681174e-02,\n       -3.21905999e-02,  4.85459580e-02,  6.50733823e-03,  2.13679789e-02,\n       -9.51162538e-02,  9.82890285e-03,  1.33457048e-02, -6.80490500e-03,\n       -3.46524136e-02, -7.92901439e-02,  2.19653527e-02,  4.03653681e-02,\n       -4.80582281e-03, -3.71454463e-02, -3.98280807e-02, -1.23770212e-01,\n        2.30426145e-02,  2.40775563e-02,  2.36460653e-02,  3.65899044e-02,\n        6.89818169e-02, -4.65605372e-02, -9.87423014e-02, -1.44166098e-02,\n       -3.74822317e-02,  1.14862473e-01, -1.62496570e-03,  4.79499922e-03,\n        1.21842509e-02,  4.07497124e-02, -4.06867111e-02, -4.16711832e-02,\n        3.49704696e-02, -9.46634772e-03, -5.98154753e-02, -9.68932715e-03,\n        4.16318031e-02,  3.05359681e-02,  5.76540164e-03,  3.41372435e-02,\n       -3.61253646e-02,  2.07301731e-02, -4.34001846e-03,  1.87940224e-02,\n       -5.97117781e-02,  3.06439973e-03, -4.93934874e-02,  1.06724162e-02,\n        1.20967188e-03,  5.28574452e-02,  2.65553192e-02,  8.33244420e-04,\n       -3.98366077e-02, -1.20608182e-02, -5.61664842e-02, -4.74597336e-02,\n       -2.63848485e-02,  3.65173396e-03, -4.37024075e-02,  1.02159583e-01,\n       -1.92846099e-02,  9.42032935e-03,  5.17064189e-02, -4.24857437e-02,\n        7.57797502e-02,  3.32127608e-02, -3.92237359e-02, -1.64670791e-03,\n       -1.41737394e-02,  1.55365471e-02, -7.03132739e-02, -1.44528291e-02,\n        5.83931996e-03, -7.49281785e-02, -1.00802917e-02, -2.71361284e-02,\n        1.70813154e-02, -7.28848521e-02,  3.34594698e-02,  5.60933395e-03,\n       -1.26753457e-02, -2.48107506e-02,  1.84242234e-02,  4.94408804e-02,\n       -1.08315454e-02, -4.22730382e-03,  1.63697862e-03, -5.47095096e-02,\n        1.97184305e-02])\n\n\n\n\nCode\nfor i in range(len(pacf)):\n    if sig_test(pacf[i]) == False:\n        n_steps = i - 1\n        print('n_steps set to', n_steps)\n        break\n\n\nn_steps set to 2\n\n\nObservamos que con el pacf se nos recomienda usar dos retardos, lo cuál nos recueda que cuando se quizo establecer una componente estacional se sugeria un periodo de 2.4, por lo tanto teniendo encuenta lo anterior usaremos 2 retrasos para la serie original"
  },
  {
    "objectID": "ModelosDeArbolesSerieExportaciones.html#pacf-1",
    "href": "ModelosDeArbolesSerieExportaciones.html#pacf-1",
    "title": "Predicción 1 paso adelante usando 2 retardos",
    "section": "3.1 PACF",
    "text": "3.1 PACF\nusaremos la funcion de autocorrealcion parcial para darnos una idea de cuantos rezagos usaremos en el modelo\n\n\nCode\nfrom matplotlib import pyplot\nfrom statsmodels.graphics.tsaplots import plot_pacf\nplot_pacf(exportaciones,lags=134,method='ywm',alpha=0.01)\npyplot.show()\n\n\n\n\n\n\n\nCode\nimport statsmodels.api as sm\npacf =  sm.tsa.stattools.pacf(exportaciones, nlags=134,method='ywm')\nT = len(exportaciones)\n\nsig_test = lambda tau_h: np.abs(tau_h) &gt; 2.58/np.sqrt(T)\n\n\n\n\nCode\npacf\n\n\narray([ 1.        ,  0.21355648, -0.01410819, -0.10451617, -0.2093758 ,\n       -0.14017377, -0.17906702,  0.09686678, -0.13392601,  0.01595307,\n       -0.13946366, -0.0563855 ,  0.17741868, -0.0609385 , -0.00478777,\n        0.00611697, -0.09012201,  0.04079611, -0.1288755 ,  0.01040772,\n       -0.06054336, -0.00809331,  0.05521414,  0.00145114, -0.06018931,\n       -0.05344687, -0.06106559, -0.09355214, -0.04415682,  0.08980262,\n       -0.12158518, -0.05246829, -0.08134046, -0.11923296,  0.01961602,\n       -0.06611188,  0.14387345, -0.08387056, -0.02154623, -0.03719252,\n        0.11636824, -0.03659881, -0.04184158,  0.0773012 , -0.09796869,\n       -0.09592436, -0.02679615,  0.03304619,  0.08238835, -0.10051005,\n       -0.1233177 ,  0.11816191, -0.10239664, -0.00298642,  0.05553658,\n       -0.02902818, -0.07533038, -0.030158  , -0.01596098, -0.0076432 ,\n       -0.02810041, -0.11429884,  0.06089394,  0.02933573,  0.09430358,\n        0.04947294,  0.0168977 , -0.05976736, -0.00941411, -0.04209226,\n       -0.00997868,  0.02671226,  0.05214744,  0.10015063, -0.08725013,\n       -0.02096121, -0.07610181,  0.02608155, -0.07163182, -0.01742543,\n       -0.01110346,  0.05773641, -0.0426505 , -0.04985717,  0.07024918,\n        0.0724035 , -0.02324498, -0.03840519, -0.00384265, -0.03725229,\n       -0.01497874,  0.02560091, -0.02994393,  0.01198395, -0.00979288,\n        0.0224643 , -0.04398166,  0.01457101,  0.01353388,  0.01837122,\n       -0.06478182,  0.05269882, -0.03988886, -0.01561551,  0.00534409,\n        0.02551739, -0.07313363,  0.01118899,  0.0051652 , -0.00649134,\n       -0.11102337,  0.05241729,  0.02498581, -0.05071817, -0.05116662,\n       -0.07926453,  0.00801331, -0.00355665, -0.0184545 ,  0.00602772,\n       -0.05354637, -0.00260395,  0.03088632,  0.00947186, -0.02269647,\n       -0.03670284,  0.01269883, -0.02970479, -0.00677465, -0.03938635,\n       -0.05408061, -0.01603313, -0.00598982, -0.03914117,  0.00226407])\n\n\n\n\nCode\nfor i in range(len(pacf)):\n    if sig_test(pacf[i]) == False:\n        n_steps = i - 1\n        print('n_steps set to', n_steps)\n        break\n\n\nn_steps set to 1\n\n\nObservamos que con el pacf se nos recomienda usar un solo retardo, usaremos 1 retraso para la serie sin Tendencia"
  },
  {
    "objectID": "ModelosDeArbolesSerieExportacionesdoceRetardos.html",
    "href": "ModelosDeArbolesSerieExportacionesdoceRetardos.html",
    "title": "Predicción 1 paso adelante usando 12 retardos",
    "section": "",
    "text": "Vamos a importar la bases de datos y a convertirlas en objetos de series de Tiempo. \\(\\{X_t\\}\\)\n\n\nCode\n# get working directory\nimport os\nos.getcwd()\n\n\n'C:\\\\Users\\\\dofca\\\\Desktop\\\\series'\n\n\n\n\nCode\n# librerias\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport sklearn\nimport openpyxl\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nimport warnings\nprint(f\"Matplotlib Version: {plt.__version__}\")\nprint(f\"Pandas Version: {pd.__version__}\")\nprint(f\"Numpy Version: {np.__version__}\")\nprint(f\"Sklearn: {sklearn.__version__}\")\n\n\nMatplotlib Version: 1.25.2\nPandas Version: 2.0.3\nNumpy Version: 1.25.2\nSklearn: 1.3.1\n\n\n\n\nCode\n# Lectura de la serie\ndata = pd.read_excel(\"datos/Exportaciones.xlsx\",\n                   header = 0, usecols = ['Mes','Total']).iloc[96:].reset_index(drop = True).round()\ndata['Total'] = data['Total'].astype(int) \ndata\n\n\n\n\n\n\n\n\n\nMes\nTotal\n\n\n\n\n0\n2000-01-01\n1011676\n\n\n1\n2000-02-01\n1054098\n\n\n2\n2000-03-01\n1053546\n\n\n3\n2000-04-01\n886359\n\n\n4\n2000-05-01\n1146258\n\n\n...\n...\n...\n\n\n277\n2023-02-01\n4202234\n\n\n278\n2023-03-01\n4431911\n\n\n279\n2023-04-01\n3739214\n\n\n280\n2023-05-01\n4497862\n\n\n281\n2023-06-01\n3985981\n\n\n\n\n282 rows × 2 columns\n\n\n\n\n\nCode\n# tipo de datos\nprint(data.info())\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 282 entries, 0 to 281\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype         \n---  ------  --------------  -----         \n 0   Mes     282 non-null    datetime64[ns]\n 1   Total   282 non-null    int32         \ndtypes: datetime64[ns](1), int32(1)\nmemory usage: 3.4 KB\nNone\n\n\n\n\nCode\n#mirando los datos\n#objeto ts\nexportaciones = data['Total']\nprint(type(exportaciones))\nplt.plot(exportaciones)\n\n\n&lt;class 'pandas.core.series.Series'&gt;\n\n\n\n\n\n\n\nCode\nprint(f'Numero de filas con valores faltantes: {data.isnull().any(axis=1).mean()}')\n\n\nNumero de filas con valores faltantes: 0.0\n\n\n\n\nCode\ndata.shape\n\n\n(282, 2)\n\n\n\n1 Árboles de decisión\n\n1.0.1 Creación de los rezagos\nDebido al análisis previo tomaremos los rezagos de 3 días atrás para poder predecir un paso adelante.\n\n\nCode\nfrom pandas import DataFrame\n# reframe as supervised learning\n# lag observation (t-1) is the input variable and t is the output variable.\ndf1 = DataFrame() # original\nprint(df1)\ndf2 = DataFrame() #diferenciada\nprint(df2)\n\n\nEmpty DataFrame\nColumns: []\nIndex: []\nEmpty DataFrame\nColumns: []\nIndex: []\n\n\n\n\nCode\n# arreglo de datos para el arreglo de rezagos Serie Original\nindice = pd.date_range(start='1/1/2000', periods=282, freq='M')\nprint(indice)\noriginalDatadf = pd.DataFrame(data['Total'].values,index=indice)\nprint(originalDatadf)\n\n\nDatetimeIndex(['2000-01-31', '2000-02-29', '2000-03-31', '2000-04-30',\n               '2000-05-31', '2000-06-30', '2000-07-31', '2000-08-31',\n               '2000-09-30', '2000-10-31',\n               ...\n               '2022-09-30', '2022-10-31', '2022-11-30', '2022-12-31',\n               '2023-01-31', '2023-02-28', '2023-03-31', '2023-04-30',\n               '2023-05-31', '2023-06-30'],\n              dtype='datetime64[ns]', length=282, freq='M')\n                  0\n2000-01-31  1011676\n2000-02-29  1054098\n2000-03-31  1053546\n2000-04-30   886359\n2000-05-31  1146258\n...             ...\n2023-02-28  4202234\n2023-03-31  4431911\n2023-04-30  3739214\n2023-05-31  4497862\n2023-06-30  3985981\n\n[282 rows x 1 columns]\n\n\n\n\nCode\n# Rezagos original\nfor i in range(12,0,-1):\n    df1[['t-'+str(i)]] = originalDatadf.shift(i)\nprint(df1)\n\n\n                 t-12       t-11       t-10        t-9        t-8        t-7  \\\n2000-01-31        NaN        NaN        NaN        NaN        NaN        NaN   \n2000-02-29        NaN        NaN        NaN        NaN        NaN        NaN   \n2000-03-31        NaN        NaN        NaN        NaN        NaN        NaN   \n2000-04-30        NaN        NaN        NaN        NaN        NaN        NaN   \n2000-05-31        NaN        NaN        NaN        NaN        NaN        NaN   \n...               ...        ...        ...        ...        ...        ...   \n2023-02-28  4209198.0  4780210.0  5460531.0  4662521.0  5497617.0  5913682.0   \n2023-03-31  4780210.0  5460531.0  4662521.0  5497617.0  5913682.0  4388737.0   \n2023-04-30  5460531.0  4662521.0  5497617.0  5913682.0  4388737.0  4778520.0   \n2023-05-31  4662521.0  5497617.0  5913682.0  4388737.0  4778520.0  4213182.0   \n2023-06-30  5497617.0  5913682.0  4388737.0  4778520.0  4213182.0  4562248.0   \n\n                  t-6        t-5        t-4        t-3        t-2        t-1  \n2000-01-31        NaN        NaN        NaN        NaN        NaN        NaN  \n2000-02-29        NaN        NaN        NaN        NaN        NaN  1011676.0  \n2000-03-31        NaN        NaN        NaN        NaN  1011676.0  1054098.0  \n2000-04-30        NaN        NaN        NaN  1011676.0  1054098.0  1053546.0  \n2000-05-31        NaN        NaN  1011676.0  1054098.0  1053546.0   886359.0  \n...               ...        ...        ...        ...        ...        ...  \n2023-02-28  4388737.0  4778520.0  4213182.0  4562248.0  4642084.0  3696188.0  \n2023-03-31  4778520.0  4213182.0  4562248.0  4642084.0  3696188.0  4202234.0  \n2023-04-30  4213182.0  4562248.0  4642084.0  3696188.0  4202234.0  4431911.0  \n2023-05-31  4562248.0  4642084.0  3696188.0  4202234.0  4431911.0  3739214.0  \n2023-06-30  4642084.0  3696188.0  4202234.0  4431911.0  3739214.0  4497862.0  \n\n[282 rows x 12 columns]\n\n\n\n\nCode\n# Create column t original\ndf1['t'] = originalDatadf.values\nprint(df1.head(14))\n\n\n                 t-12       t-11       t-10        t-9        t-8        t-7  \\\n2000-01-31        NaN        NaN        NaN        NaN        NaN        NaN   \n2000-02-29        NaN        NaN        NaN        NaN        NaN        NaN   \n2000-03-31        NaN        NaN        NaN        NaN        NaN        NaN   \n2000-04-30        NaN        NaN        NaN        NaN        NaN        NaN   \n2000-05-31        NaN        NaN        NaN        NaN        NaN        NaN   \n2000-06-30        NaN        NaN        NaN        NaN        NaN        NaN   \n2000-07-31        NaN        NaN        NaN        NaN        NaN        NaN   \n2000-08-31        NaN        NaN        NaN        NaN        NaN  1011676.0   \n2000-09-30        NaN        NaN        NaN        NaN  1011676.0  1054098.0   \n2000-10-31        NaN        NaN        NaN  1011676.0  1054098.0  1053546.0   \n2000-11-30        NaN        NaN  1011676.0  1054098.0  1053546.0   886359.0   \n2000-12-31        NaN  1011676.0  1054098.0  1053546.0   886359.0  1146258.0   \n2001-01-31  1011676.0  1054098.0  1053546.0   886359.0  1146258.0  1153956.0   \n2001-02-28  1054098.0  1053546.0   886359.0  1146258.0  1153956.0  1104408.0   \n\n                  t-6        t-5        t-4        t-3        t-2        t-1  \\\n2000-01-31        NaN        NaN        NaN        NaN        NaN        NaN   \n2000-02-29        NaN        NaN        NaN        NaN        NaN  1011676.0   \n2000-03-31        NaN        NaN        NaN        NaN  1011676.0  1054098.0   \n2000-04-30        NaN        NaN        NaN  1011676.0  1054098.0  1053546.0   \n2000-05-31        NaN        NaN  1011676.0  1054098.0  1053546.0   886359.0   \n2000-06-30        NaN  1011676.0  1054098.0  1053546.0   886359.0  1146258.0   \n2000-07-31  1011676.0  1054098.0  1053546.0   886359.0  1146258.0  1153956.0   \n2000-08-31  1054098.0  1053546.0   886359.0  1146258.0  1153956.0  1104408.0   \n2000-09-30  1053546.0   886359.0  1146258.0  1153956.0  1104408.0  1242391.0   \n2000-10-31   886359.0  1146258.0  1153956.0  1104408.0  1242391.0  1102913.0   \n2000-11-30  1146258.0  1153956.0  1104408.0  1242391.0  1102913.0   981716.0   \n2000-12-31  1153956.0  1104408.0  1242391.0  1102913.0   981716.0  1192681.0   \n2001-01-31  1104408.0  1242391.0  1102913.0   981716.0  1192681.0  1228398.0   \n2001-02-28  1242391.0  1102913.0   981716.0  1192681.0  1228398.0  1017195.0   \n\n                  t  \n2000-01-31  1011676  \n2000-02-29  1054098  \n2000-03-31  1053546  \n2000-04-30   886359  \n2000-05-31  1146258  \n2000-06-30  1153956  \n2000-07-31  1104408  \n2000-08-31  1242391  \n2000-09-30  1102913  \n2000-10-31   981716  \n2000-11-30  1192681  \n2000-12-31  1228398  \n2001-01-31  1017195  \n2001-02-28   964437  \n\n\n\n\nCode\n# Create a new subsetted dataframe, removing Nans from first 3 rows original\ndf1_Ori = df1[12:]\nprint(df1_Ori)\ndf1_Ori.size\n\n\n                 t-12       t-11       t-10        t-9        t-8        t-7  \\\n2001-01-31  1011676.0  1054098.0  1053546.0   886359.0  1146258.0  1153956.0   \n2001-02-28  1054098.0  1053546.0   886359.0  1146258.0  1153956.0  1104408.0   \n2001-03-31  1053546.0   886359.0  1146258.0  1153956.0  1104408.0  1242391.0   \n2001-04-30   886359.0  1146258.0  1153956.0  1104408.0  1242391.0  1102913.0   \n2001-05-31  1146258.0  1153956.0  1104408.0  1242391.0  1102913.0   981716.0   \n...               ...        ...        ...        ...        ...        ...   \n2023-02-28  4209198.0  4780210.0  5460531.0  4662521.0  5497617.0  5913682.0   \n2023-03-31  4780210.0  5460531.0  4662521.0  5497617.0  5913682.0  4388737.0   \n2023-04-30  5460531.0  4662521.0  5497617.0  5913682.0  4388737.0  4778520.0   \n2023-05-31  4662521.0  5497617.0  5913682.0  4388737.0  4778520.0  4213182.0   \n2023-06-30  5497617.0  5913682.0  4388737.0  4778520.0  4213182.0  4562248.0   \n\n                  t-6        t-5        t-4        t-3        t-2        t-1  \\\n2001-01-31  1104408.0  1242391.0  1102913.0   981716.0  1192681.0  1228398.0   \n2001-02-28  1242391.0  1102913.0   981716.0  1192681.0  1228398.0  1017195.0   \n2001-03-31  1102913.0   981716.0  1192681.0  1228398.0  1017195.0   964437.0   \n2001-04-30   981716.0  1192681.0  1228398.0  1017195.0   964437.0  1002450.0   \n2001-05-31  1192681.0  1228398.0  1017195.0   964437.0  1002450.0  1058457.0   \n...               ...        ...        ...        ...        ...        ...   \n2023-02-28  4388737.0  4778520.0  4213182.0  4562248.0  4642084.0  3696188.0   \n2023-03-31  4778520.0  4213182.0  4562248.0  4642084.0  3696188.0  4202234.0   \n2023-04-30  4213182.0  4562248.0  4642084.0  3696188.0  4202234.0  4431911.0   \n2023-05-31  4562248.0  4642084.0  3696188.0  4202234.0  4431911.0  3739214.0   \n2023-06-30  4642084.0  3696188.0  4202234.0  4431911.0  3739214.0  4497862.0   \n\n                  t  \n2001-01-31  1017195  \n2001-02-28   964437  \n2001-03-31  1002450  \n2001-04-30  1058457  \n2001-05-31  1068023  \n...             ...  \n2023-02-28  4202234  \n2023-03-31  4431911  \n2023-04-30  3739214  \n2023-05-31  4497862  \n2023-06-30  3985981  \n\n[270 rows x 13 columns]\n\n\n3510\n\n\n\n\nCode\n# Split data Serie Original\nOrig_Split = df1_Ori.values\n# split into lagged variables and original time series\nX1 = Orig_Split[:, 0:-1]  # slice all rows and start with column 0 and go up to but not including the last column\ny1 = Orig_Split[:,-1]  # slice all rows and last column, essentially separating out 't' column\nprint(X1)\nprint('Respuestas \\n',y1)\n\n\n[[1011676. 1054098. 1053546. ...  981716. 1192681. 1228398.]\n [1054098. 1053546.  886359. ... 1192681. 1228398. 1017195.]\n [1053546.  886359. 1146258. ... 1228398. 1017195.  964437.]\n ...\n [5460531. 4662521. 5497617. ... 3696188. 4202234. 4431911.]\n [4662521. 5497617. 5913682. ... 4202234. 4431911. 3739214.]\n [5497617. 5913682. 4388737. ... 4431911. 3739214. 4497862.]]\nRespuestas \n [1017195.  964437. 1002450. 1058457. 1068023.  996736. 1005867. 1189605.\n 1078781. 1013772.  965973.  968599.  943702.  945935.  859303. 1123902.\n 1076509.  921463. 1040876.  915457. 1055414. 1070342.  966897. 1055590.\n  923427. 1033043. 1034233. 1101056. 1181888.  995297. 1267598. 1092850.\n 1079472. 1169195. 1082836. 1167628. 1183399. 1031826. 1206657. 1271618.\n 1335727. 1433647. 1541103. 1516523. 1519458. 1529291. 1585709. 1633370.\n 1378980. 1529265. 1722081. 1682450. 1737198. 2097853. 1653833. 1882740.\n 1908016. 1788905. 1826199. 1938567. 1668171. 1862024. 1929864. 1872161.\n 2211681. 2039364. 2141958. 2129881. 2104243. 2271572. 2146547. 2134504.\n 1843668. 1914770. 2384657. 2497750. 2727980. 2114259. 2648147. 2621002.\n 2523170. 2623649. 3152653. 3227536. 2842306. 2822470. 3007288. 3365420.\n 3392615. 3675654. 3801685. 3294187. 3133994. 2981105. 2245379. 2224271.\n 2525698. 2340118. 2711332. 2427571. 2742519. 2738083. 2898600. 2673470.\n 2795983. 2948687. 2861294. 3182972. 2913433. 2869156. 3337903. 3490978.\n 3513331. 3060628. 3157626. 3291236. 3271661. 3535759. 3426095. 3845531.\n 3760176. 3958572. 4893312. 4823094. 5153710. 4708737. 4866229. 4941645.\n 4582401. 4772996. 5147330. 5306738. 4785773. 4999318. 5712355. 5010929.\n 5403375. 4563431. 4976905. 4570780. 4910403. 5432930. 4807338. 4951628.\n 4849196. 4667767. 4617842. 4949487. 5332470. 4870839. 4652297. 4977706.\n 4849996. 4837983. 4948665. 5272122. 4808832. 4271442. 4408181. 4316676.\n 5495867. 4704814. 5048930. 4813091. 5077247. 4322278. 3794686. 3794711.\n 2916976. 3160957. 3461944. 3219706. 3381084. 3217408. 3043778. 2868451.\n 2898168. 2815522. 2444535. 2588994. 1919053. 2328723. 2334998. 2463793.\n 2751470. 2780512. 2266997. 3044377. 2797686. 2770014. 2833622. 3477094.\n 2785044. 2716024. 3300421. 2697992. 3505436. 2895612. 3125483. 3191598.\n 3389679. 3277516. 3122237. 4014818. 3324889. 3027603. 3365116. 3786537.\n 3719410. 3331933. 3632055. 3684399. 3512842. 3768666. 3343509. 3407819.\n 3066110. 3183071. 3344850. 3862819. 3748342. 3096363. 3255830. 3264261.\n 3067349. 3326497. 2943625. 3330051. 3419466. 2943626. 2439036. 1864239.\n 2221172. 2289482. 2551988. 2584767. 2544874. 2644954. 2523372. 3028837.\n 2610936. 2938994. 3383554. 2976372. 3096913. 3182216. 3444158. 3465143.\n 3792236. 3799111. 4155805. 4544551. 3801609. 4209198. 4780210. 5460531.\n 4662521. 5497617. 5913682. 4388737. 4778520. 4213182. 4562248. 4642084.\n 3696188. 4202234. 4431911. 3739214. 4497862. 3985981.]\n\n\n\n\n\n2 Árbol para Serie Original\n\n2.0.0.1 Entrenamiento, Validación y prueba\n\n\nCode\nY1 = y1\nprint('Complete Observations for Target after Supervised configuration: %d' %len(Y1))\ntraintarget_size = int(len(Y1) * 0.70) \nvaltarget_size = int(len(Y1) * 0.10)+1# Set split\ntesttarget_size = int(len(Y1) * 0.20)# Set split\nprint(traintarget_size,valtarget_size,testtarget_size)\nprint('Train + Validation + Test: %d' %(traintarget_size+valtarget_size+testtarget_size))\n\n\nComplete Observations for Target after Supervised configuration: 270\n189 28 54\nTrain + Validation + Test: 271\n\n\n\n\nCode\n# Target Train-Validation-Test split(70-10-20)\ntrain_target, val_target,test_target = Y1[0:traintarget_size], Y1[(traintarget_size):(traintarget_size+valtarget_size)],Y1[(traintarget_size+valtarget_size):len(Y1)]\n\nprint('Observations for Target: %d' % (len(Y1)))\nprint('Training Observations for Target: %d' % (len(train_target)))\nprint('Validation Observations for Target: %d' % (len(val_target)))\nprint('Test Observations for Target: %d' % (len(test_target)))\n\n\nObservations for Target: 270\nTraining Observations for Target: 189\nValidation Observations for Target: 28\nTest Observations for Target: 53\n\n\n\n\nCode\n# Features Train--Val-Test split\n\ntrainfeature_size = int(len(X1) * 0.70)\nvalfeature_size = int(len(X1) * 0.10)+1# Set split\ntestfeature_size = int(len(X1) * 0.20)# Set split\ntrain_feature, val_feature,test_feature = X1[0:traintarget_size],X1[(traintarget_size):(traintarget_size+valtarget_size)] ,X1[(traintarget_size+valtarget_size):len(Y1)]\n\nprint('Observations for Feature: %d' % (len(X1)))\nprint('Training Observations for Feature: %d' % (len(train_feature)))\nprint('Validation Observations for Feature: %d' % (len(val_feature)))\nprint('Test Observations for Feature: %d' % (len(test_feature)))\n\n\nObservations for Feature: 270\nTraining Observations for Feature: 189\nValidation Observations for Feature: 28\nTest Observations for Feature: 53\n\n\n\n\n2.0.1 Árbol\n\n\nCode\n# Decision Tree Regresion Model\nfrom sklearn.tree import DecisionTreeRegressor\n# Create a decision tree regression model with default arguments\ndecision_tree_Orig = DecisionTreeRegressor()  # max-depth not set\n# The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n# Fit the model to the training features(covariables) and targets(respuestas)\ndecision_tree_Orig.fit(train_feature, train_target)\n# Check the score on train and test\nprint(\"Coeficiente R2 sobre el conjunto de entrenamiento:\",decision_tree_Orig.score(train_feature, train_target))\nprint(\"Coeficiente R2 sobre el conjunto de Validación:\",decision_tree_Orig.score(val_feature,val_target))  # predictions are horrible if negative value, no relationship if 0\nprint(\"el RECM sobre validación es:\",(((decision_tree_Orig.predict(val_feature)-val_target)**2).mean()) )\n\n\nCoeficiente R2 sobre el conjunto de entrenamiento: 1.0\nCoeficiente R2 sobre el conjunto de Validación: -1.7548809186918701\nel RECM sobre validación es: 332309503244.0\n\n\nVemos que el R2 para los datos de validación es bueno así sin ningún ajuste, Se relizara un ajuste de la profundidad como hiperparametro para ver si mejora dicho valor\n\n\nCode\n# Find the best Max Depth\n\n# Loop through a few different max depths and check the performance\n# Try different max depths. We want to optimize our ML models to make the best predictions possible.\n# For regular decision trees, max_depth, which is a hyperparameter, limits the number of splits in a tree.\n# You can find the best value of max_depth based on the R-squared score of the model on the test set.\n\nfor d in [2, 3, 4, 5,6,7,8,9,10,11,12,13,14,15]:\n    # Create the tree and fit it\n    decision_tree_Orig = DecisionTreeRegressor(max_depth=d)\n    decision_tree_Orig.fit(train_feature, train_target)\n    \n    # Print out the scores on train and test\n    print('max_depth=', str(d))\n    print(\"Coeficiente R2 sobre el conjunto de entrenamiento:\",decision_tree_Orig.score(train_feature, train_target))\n    print(\"Coeficiente R2 sobre el conjunto de validación:\",decision_tree_Orig.score(val_feature, val_target), '\\n')  # You want the test score to be positive and high\n    print(\"el RECM sobre el conjunto de validación es:\",sklearn.metrics.mean_squared_error(decision_tree_Orig.predict(val_feature),val_target, squared=False))\n\n\nmax_depth= 2\nCoeficiente R2 sobre el conjunto de entrenamiento: 0.9380991089808784\nCoeficiente R2 sobre el conjunto de validación: -2.051209928658386 \n\nel RECM sobre el conjunto de validación es: 606674.875354811\nmax_depth= 3\nCoeficiente R2 sobre el conjunto de entrenamiento: 0.9664178293704504\nCoeficiente R2 sobre el conjunto de validación: -1.7135186026890032 \n\nel RECM sobre el conjunto de validación es: 572118.9945819594\nmax_depth= 4\nCoeficiente R2 sobre el conjunto de entrenamiento: 0.9768825370828798\nCoeficiente R2 sobre el conjunto de validación: -1.7037887014868796 \n\nel RECM sobre el conjunto de validación es: 571092.3459406185\nmax_depth= 5\nCoeficiente R2 sobre el conjunto de entrenamiento: 0.9851230357060198\nCoeficiente R2 sobre el conjunto de validación: -1.832333851372148 \n\nel RECM sobre el conjunto de validación es: 584510.3243444414\nmax_depth= 6\nCoeficiente R2 sobre el conjunto de entrenamiento: 0.9925782041053538\nCoeficiente R2 sobre el conjunto de validación: -2.2041965658843563 \n\nel RECM sobre el conjunto de validación es: 621698.1004856585\nmax_depth= 7\nCoeficiente R2 sobre el conjunto de entrenamiento: 0.9978144610458868\nCoeficiente R2 sobre el conjunto de validación: -2.3497128586740375 \n\nel RECM sobre el conjunto de validación es: 635658.3486494402\nmax_depth= 8\nCoeficiente R2 sobre el conjunto de entrenamiento: 0.9994408733552114\nCoeficiente R2 sobre el conjunto de validación: -2.811262198389598 \n\nel RECM sobre el conjunto de validación es: 678038.5380785092\nmax_depth= 9\nCoeficiente R2 sobre el conjunto de entrenamiento: 0.9998981374422563\nCoeficiente R2 sobre el conjunto de validación: -2.0147327960575905 \n\nel RECM sobre el conjunto de validación es: 603037.5808233338\nmax_depth= 10\nCoeficiente R2 sobre el conjunto de entrenamiento: 0.9999911411632353\nCoeficiente R2 sobre el conjunto de validación: -2.1214885273158406 \n\nel RECM sobre el conjunto de validación es: 613621.8796439593\nmax_depth= 11\nCoeficiente R2 sobre el conjunto de entrenamiento: 0.9999995801959258\nCoeficiente R2 sobre el conjunto de validación: -2.1521096826836246 \n\nel RECM sobre el conjunto de validación es: 616624.2860853936\nmax_depth= 12\nCoeficiente R2 sobre el conjunto de entrenamiento: 0.9999999961093721\nCoeficiente R2 sobre el conjunto de validación: -2.5439260490331046 \n\nel RECM sobre el conjunto de validación es: 653826.1563723243\nmax_depth= 13\nCoeficiente R2 sobre el conjunto de entrenamiento: 1.0\nCoeficiente R2 sobre el conjunto de validación: -2.7126672435521564 \n\nel RECM sobre el conjunto de validación es: 669210.8571930343\nmax_depth= 14\nCoeficiente R2 sobre el conjunto de entrenamiento: 1.0\nCoeficiente R2 sobre el conjunto de validación: -1.9050269567306248 \n\nel RECM sobre el conjunto de validación es: 591963.6624602235\nmax_depth= 15\nCoeficiente R2 sobre el conjunto de entrenamiento: 1.0\nCoeficiente R2 sobre el conjunto de validación: -2.3128435879041693 \n\nel RECM sobre el conjunto de validación es: 632150.42019757\n\n\nNote que los scores para el conjunto de validación son negativos para todas las profundidades evaluadas. Ahora uniremos validacion y entrenamiento para re para reestimar los parametros\n\n\nCode\nprint(type(train_feature))\nprint(type(val_feature))\n#######\nprint(type(train_target))\nprint(type(val_target))\n####\nprint(train_feature.shape)\nprint(val_feature.shape)\n#####\n####\nprint(train_target.shape)\nprint(val_target.shape)\n###Concatenate Validation and test\ntrain_val_feature=np.concatenate((train_feature,val_feature),axis=0)\ntrain_val_target=np.concatenate((train_target,val_target),axis=0)\nprint(train_val_feature.shape)\nprint(train_val_target.shape)\n\n\n&lt;class 'numpy.ndarray'&gt;\n&lt;class 'numpy.ndarray'&gt;\n&lt;class 'numpy.ndarray'&gt;\n&lt;class 'numpy.ndarray'&gt;\n(189, 12)\n(28, 12)\n(189,)\n(28,)\n(217, 12)\n(217,)\n\n\n\n\nCode\n# Use the best max_depth \ndecision_tree_Orig = DecisionTreeRegressor(max_depth=4)  # fill in best max depth here\ndecision_tree_Orig.fit(train_val_feature, train_val_target)\n\n# Predict values for train and test\ntrain_val_prediction = decision_tree_Orig.predict(train_val_feature)\ntest_prediction = decision_tree_Orig.predict(test_feature)\n\n# Scatter the predictions vs actual values\nplt.scatter(train_val_prediction, train_val_target, label='train')  # blue\nplt.scatter(test_prediction, test_target, label='test')  # orange\n# Agrega títulos a los ejes\nplt.xlabel('Valores Predichos')  # Título para el eje x\nplt.ylabel('Valores Objetivo')  # Título para el eje y\n# Muestra una leyenda\nplt.legend()\nplt.show()\nprint(\"Raíz de la Pérdida cuadrática Entrenamiento:\",sklearn.metrics.mean_squared_error( train_val_prediction, train_val_target,squared=False))\n\nprint(\"Raíz de la Pérdida cuadrática Prueba:\",sklearn.metrics.mean_squared_error(test_prediction, test_target,squared=False))\n\n\n\n\n\nRaíz de la Pérdida cuadrática Entrenamiento: 237470.97155102508\nRaíz de la Pérdida cuadrática Prueba: 512170.57525904087\n\n\n\n\nCode\nfrom sklearn import tree\n\nlistacaract=list(df1_Ori.columns.values)\nrespuesta=listacaract.pop()\ntext_representation = tree.export_text(decision_tree_Orig)\nprint(text_representation)\n\n\n|--- feature_11 &lt;= 2818996.00\n|   |--- feature_0 &lt;= 1406313.50\n|   |   |--- feature_11 &lt;= 1269608.00\n|   |   |   |--- feature_7 &lt;= 944818.50\n|   |   |   |   |--- value: [944508.20]\n|   |   |   |--- feature_7 &gt;  944818.50\n|   |   |   |   |--- value: [1068707.63]\n|   |   |--- feature_11 &gt;  1269608.00\n|   |   |   |--- feature_7 &lt;= 1524374.50\n|   |   |   |   |--- value: [1485522.56]\n|   |   |   |--- feature_7 &gt;  1524374.50\n|   |   |   |   |--- value: [1688654.00]\n|   |--- feature_0 &gt;  1406313.50\n|   |   |--- feature_0 &lt;= 2101048.00\n|   |   |   |--- feature_0 &lt;= 1729639.50\n|   |   |   |   |--- value: [1895482.33]\n|   |   |   |--- feature_0 &gt;  1729639.50\n|   |   |   |   |--- value: [2177202.55]\n|   |   |--- feature_0 &gt;  2101048.00\n|   |   |   |--- feature_6 &lt;= 2851036.50\n|   |   |   |   |--- value: [2806855.75]\n|   |   |   |--- feature_6 &gt;  2851036.50\n|   |   |   |   |--- value: [2414665.18]\n|--- feature_11 &gt;  2818996.00\n|   |--- feature_11 &lt;= 3902051.50\n|   |   |--- feature_11 &lt;= 3328411.00\n|   |   |   |--- feature_5 &lt;= 3491265.00\n|   |   |   |   |--- value: [3049887.90]\n|   |   |   |--- feature_5 &gt;  3491265.00\n|   |   |   |   |--- value: [3412516.20]\n|   |   |--- feature_11 &gt;  3328411.00\n|   |   |   |--- feature_8 &lt;= 2810078.00\n|   |   |   |   |--- value: [2840328.00]\n|   |   |   |--- feature_8 &gt;  2810078.00\n|   |   |   |   |--- value: [3496117.27]\n|   |--- feature_11 &gt;  3902051.50\n|   |   |--- feature_7 &lt;= 3308846.50\n|   |   |   |--- value: [3324889.00]\n|   |   |--- feature_7 &gt;  3308846.50\n|   |   |   |--- feature_5 &lt;= 5150520.00\n|   |   |   |   |--- value: [4812843.37]\n|   |   |   |--- feature_5 &gt;  5150520.00\n|   |   |   |   |--- value: [5188817.57]\n\n\n\n\n\nCode\nfig = plt.figure(figsize=(25,20))\n_ = tree.plot_tree(decision_tree_Orig, \n                   feature_names=listacaract,  \n                   class_names=[respuesta],\n                   filled=True)\n\n\n\n\n\nAhora miraremos las predicciones comparadas con los valores verdaderos, para ver más claro lo anterior.\n\n\nCode\nprint(train_val_prediction.size)\nprint(train_val_target.size)\n\nprint(test_prediction.size)\nprint(test_target.size)\n\n\n217\n217\n53\n53\n\n\n\n\nCode\nindicetrian_val_test=df1_Ori.index\nprint(indicetrian_val_test.size)  ###Tamaño del índice\nindicetrain_val=indicetrian_val_test[0:225]\nindicetest=indicetrian_val_test[225:280]\n\n\n270\n\n\n\n\nCode\nprint(indicetrain_val.size)\nprint(indicetest.size)\n\n\n225\n45\n\n\n\n\nCode\ntargetjoint=np.concatenate((train_val_target,test_target))\npredictionjoint=np.concatenate((train_val_prediction,test_prediction))\nprint(targetjoint.size)\nprint(predictionjoint.size)\n\n\n270\n270\n\n\n\n\nCode\nd = {'observado': targetjoint, 'Predicción': predictionjoint}\nObsvsPred1=pd.DataFrame(data=d,index=indicetrian_val_test)\nObsvsPred1.head(10)\n\n\n\n\n\n\n\n\n\nobservado\nPredicción\n\n\n\n\n2001-01-31\n1017195.0\n1.068708e+06\n\n\n2001-02-28\n964437.0\n1.068708e+06\n\n\n2001-03-31\n1002450.0\n1.068708e+06\n\n\n2001-04-30\n1058457.0\n1.068708e+06\n\n\n2001-05-31\n1068023.0\n1.068708e+06\n\n\n2001-06-30\n996736.0\n1.068708e+06\n\n\n2001-07-31\n1005867.0\n1.068708e+06\n\n\n2001-08-31\n1189605.0\n1.068708e+06\n\n\n2001-09-30\n1078781.0\n1.068708e+06\n\n\n2001-10-31\n1013772.0\n1.068708e+06\n\n\n\n\n\n\n\n\n\nCode\n#gráfico\nax = ObsvsPred1['observado'].plot(marker=\"o\", figsize=(10, 6), linewidth=1, markersize=4)  # Ajusta el grosor de las líneas y puntos\nObsvsPred1['Predicción'].plot(marker=\"o\", linewidth=1, markersize=2, ax=ax)  # Ajusta el grosor de las líneas y puntos\n# Agrega una línea vertical roja\nax.axvline(x=indicetrian_val_test[223].date(), color='red', linewidth=0.5)  # Ajusta el grosor de la línea vertical\n# Muestra una leyenda\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n3 Serie de Exportaciones sin Tendencia\nImplementaremos ahora el modelo de árboles sobre la serie sin tendencia, eliminada usando la estimación dada por medio del filtro de promedios móviles. Vamos a importar la bases de datos y a convertirlas en objetos de series de Tiempo. \\(\\{X_t\\}\\)\n\n\nCode\n# Lectura de la serie\ndata2 = pd.read_excel(\"ExportacionesSinTendencia.xlsx\",\n                   header = 0, usecols = ['Fecha','ExportacionesSinTend']).reset_index(drop = True).round()\ndata2['ExportacionesSinTend'] = data2['ExportacionesSinTend'].astype(int) \ndata2\n\n\n\n\n\n\n\n\n\nFecha\nExportacionesSinTend\n\n\n\n\n0\n2000-07-01\n5\n\n\n1\n2000-08-01\n70\n\n\n2\n2000-09-01\n9\n\n\n3\n2000-10-01\n-53\n\n\n4\n2000-11-01\n46\n\n\n...\n...\n...\n\n\n265\n2022-08-01\n-71\n\n\n266\n2022-09-01\n17\n\n\n267\n2022-10-01\n-88\n\n\n268\n2022-11-01\n7\n\n\n269\n2022-12-01\n39\n\n\n\n\n270 rows × 2 columns\n\n\n\n\n\nCode\n# tipo de datos\nprint(data2.info())\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 270 entries, 0 to 269\nData columns (total 2 columns):\n #   Column                Non-Null Count  Dtype         \n---  ------                --------------  -----         \n 0   Fecha                 270 non-null    datetime64[ns]\n 1   ExportacionesSinTend  270 non-null    int32         \ndtypes: datetime64[ns](1), int32(1)\nmemory usage: 3.3 KB\nNone\n\n\n\n\nCode\n#mirando los datos\n#objeto ts\nexportaciones = data2['ExportacionesSinTend']\nprint(type(exportaciones))\nplt.plot(exportaciones)\n\n\n&lt;class 'pandas.core.series.Series'&gt;\n\n\n\n\n\n\n\nCode\nprint(f'Numero de filas con valores faltantes: {data2.isnull().any(axis=1).mean()}')\n\n\nNumero de filas con valores faltantes: 0.0\n\n\n\n\nCode\ndata2.shape\n\n\n(270, 2)\n\n\n\n\n4 Árboles de decisión\n\n4.0.1 Creación de los rezagos\nTomaremos los rezagos de 12 meses atrás para poder predecir un paso adelante.\n\n\nCode\nfrom pandas import DataFrame\n# reframe as supervised learning\n# lag observation (t-1) is the input variable and t is the output variable.\ndf1 = DataFrame() # original\nprint(df1)\ndf2 = DataFrame() #diferenciada\nprint(df2)\n\n\nEmpty DataFrame\nColumns: []\nIndex: []\nEmpty DataFrame\nColumns: []\nIndex: []\n\n\n\n\nCode\n# arreglo de datos para el arreglo de rezagos Serie Original\nindice = pd.date_range(start='1/7/2000', periods=270, freq='M')\nprint(indice)\noriginalDatadf = pd.DataFrame(data2['ExportacionesSinTend'].values,index=indice)\nprint(originalDatadf)\n\n\nDatetimeIndex(['2000-01-31', '2000-02-29', '2000-03-31', '2000-04-30',\n               '2000-05-31', '2000-06-30', '2000-07-31', '2000-08-31',\n               '2000-09-30', '2000-10-31',\n               ...\n               '2021-09-30', '2021-10-31', '2021-11-30', '2021-12-31',\n               '2022-01-31', '2022-02-28', '2022-03-31', '2022-04-30',\n               '2022-05-31', '2022-06-30'],\n              dtype='datetime64[ns]', length=270, freq='M')\n             0\n2000-01-31   5\n2000-02-29  70\n2000-03-31   9\n2000-04-30 -53\n2000-05-31  46\n...         ..\n2022-02-28 -71\n2022-03-31  17\n2022-04-30 -88\n2022-05-31   7\n2022-06-30  39\n\n[270 rows x 1 columns]\n\n\n\n\nCode\n# Rezagos original\nfor i in range(12,0,-1):\n    df1[['t-'+str(i)]] = originalDatadf.shift(i)\nprint(df1)\n\n\n            t-12   t-11   t-10    t-9    t-8    t-7    t-6    t-5    t-4  \\\n2000-01-31   NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n2000-02-29   NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n2000-03-31   NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n2000-04-30   NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n2000-05-31   NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    5.0   \n...          ...    ...    ...    ...    ...    ...    ...    ...    ...   \n2022-02-28 -30.0   24.0  -12.0   33.0   82.0 -132.0  -68.0   39.0  164.0   \n2022-03-31  24.0  -12.0   33.0   82.0 -132.0  -68.0   39.0  164.0   -7.0   \n2022-04-30 -12.0   33.0   82.0 -132.0  -68.0   39.0  164.0   -7.0  159.0   \n2022-05-31  33.0   82.0 -132.0  -68.0   39.0  164.0   -7.0  159.0  239.0   \n2022-06-30  82.0 -132.0  -68.0   39.0  164.0   -7.0  159.0  239.0  -71.0   \n\n              t-3    t-2    t-1  \n2000-01-31    NaN    NaN    NaN  \n2000-02-29    NaN    NaN    5.0  \n2000-03-31    NaN    5.0   70.0  \n2000-04-30    5.0   70.0    9.0  \n2000-05-31   70.0    9.0  -53.0  \n...           ...    ...    ...  \n2022-02-28   -7.0  159.0  239.0  \n2022-03-31  159.0  239.0  -71.0  \n2022-04-30  239.0  -71.0   17.0  \n2022-05-31  -71.0   17.0  -88.0  \n2022-06-30   17.0  -88.0    7.0  \n\n[270 rows x 12 columns]\n\n\n\n\nCode\n# Create column t original\ndf1['t'] = originalDatadf.values\nprint(df1.head(14))\n\n\n            t-12  t-11  t-10   t-9   t-8   t-7   t-6   t-5   t-4   t-3   t-2  \\\n2000-01-31   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n2000-02-29   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n2000-03-31   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   5.0   \n2000-04-30   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   5.0  70.0   \n2000-05-31   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   5.0  70.0   9.0   \n2000-06-30   NaN   NaN   NaN   NaN   NaN   NaN   NaN   5.0  70.0   9.0 -53.0   \n2000-07-31   NaN   NaN   NaN   NaN   NaN   NaN   5.0  70.0   9.0 -53.0  46.0   \n2000-08-31   NaN   NaN   NaN   NaN   NaN   5.0  70.0   9.0 -53.0  46.0  67.0   \n2000-09-30   NaN   NaN   NaN   NaN   5.0  70.0   9.0 -53.0  46.0  67.0 -28.0   \n2000-10-31   NaN   NaN   NaN   5.0  70.0   9.0 -53.0  46.0  67.0 -28.0 -51.0   \n2000-11-30   NaN   NaN   5.0  70.0   9.0 -53.0  46.0  67.0 -28.0 -51.0 -31.0   \n2000-12-31   NaN   5.0  70.0   9.0 -53.0  46.0  67.0 -28.0 -51.0 -31.0  -3.0   \n2001-01-31   5.0  70.0   9.0 -53.0  46.0  67.0 -28.0 -51.0 -31.0  -3.0   5.0   \n2001-02-28  70.0   9.0 -53.0  46.0  67.0 -28.0 -51.0 -31.0  -3.0   5.0 -20.0   \n\n             t-1   t  \n2000-01-31   NaN   5  \n2000-02-29   5.0  70  \n2000-03-31  70.0   9  \n2000-04-30   9.0 -53  \n2000-05-31 -53.0  46  \n2000-06-30  46.0  67  \n2000-07-31  67.0 -28  \n2000-08-31 -28.0 -51  \n2000-09-30 -51.0 -31  \n2000-10-31 -31.0  -3  \n2000-11-30  -3.0   5  \n2000-12-31   5.0 -20  \n2001-01-31 -20.0  -9  \n2001-02-28  -9.0  81  \n\n\n\n\nCode\n# Create a new subsetted dataframe, removing Nans from first 3 rows original\ndf1_Ori = df1[12:]\nprint(df1_Ori)\ndf1_Ori.size\n\n\n            t-12   t-11   t-10    t-9    t-8    t-7    t-6    t-5    t-4  \\\n2001-01-31   5.0   70.0    9.0  -53.0   46.0   67.0  -28.0  -51.0  -31.0   \n2001-02-28  70.0    9.0  -53.0   46.0   67.0  -28.0  -51.0  -31.0   -3.0   \n2001-03-31   9.0  -53.0   46.0   67.0  -28.0  -51.0  -31.0   -3.0    5.0   \n2001-04-30 -53.0   46.0   67.0  -28.0  -51.0  -31.0   -3.0    5.0  -20.0   \n2001-05-31  46.0   67.0  -28.0  -51.0  -31.0   -3.0    5.0  -20.0   -9.0   \n...          ...    ...    ...    ...    ...    ...    ...    ...    ...   \n2022-02-28 -30.0   24.0  -12.0   33.0   82.0 -132.0  -68.0   39.0  164.0   \n2022-03-31  24.0  -12.0   33.0   82.0 -132.0  -68.0   39.0  164.0   -7.0   \n2022-04-30 -12.0   33.0   82.0 -132.0  -68.0   39.0  164.0   -7.0  159.0   \n2022-05-31  33.0   82.0 -132.0  -68.0   39.0  164.0   -7.0  159.0  239.0   \n2022-06-30  82.0 -132.0  -68.0   39.0  164.0   -7.0  159.0  239.0  -71.0   \n\n              t-3    t-2    t-1   t  \n2001-01-31   -3.0    5.0  -20.0  -9  \n2001-02-28    5.0  -20.0   -9.0  81  \n2001-03-31  -20.0   -9.0   81.0  32  \n2001-04-30   -9.0   81.0   32.0   2  \n2001-05-31   81.0   32.0    2.0 -23  \n...           ...    ...    ...  ..  \n2022-02-28   -7.0  159.0  239.0 -71  \n2022-03-31  159.0  239.0  -71.0  17  \n2022-04-30  239.0  -71.0   17.0 -88  \n2022-05-31  -71.0   17.0  -88.0   7  \n2022-06-30   17.0  -88.0    7.0  39  \n\n[258 rows x 13 columns]\n\n\n3354\n\n\n\n\nCode\n# Split data Serie Original\nOrig_Split = df1_Ori.values\n# split into lagged variables and original time series\nX1 = Orig_Split[:, 0:-1]  # slice all rows and start with column 0 and go up to but not including the last column\ny1 = Orig_Split[:,-1]  # slice all rows and last column, essentially separating out 't' column\nprint(X1)\nprint('Respuestas \\n',y1)\n\n\n[[   5.   70.    9. ...   -3.    5.  -20.]\n [  70.    9.  -53. ...    5.  -20.   -9.]\n [   9.  -53.   46. ...  -20.   -9.   81.]\n ...\n [ -12.   33.   82. ...  239.  -71.   17.]\n [  33.   82. -132. ...  -71.   17.  -88.]\n [  82. -132.  -68. ...   17.  -88.    7.]]\nRespuestas \n [  -9.   81.   32.    2.  -23.  -20.  -32.  -26.  -66.   67.   43.  -37.\n   23.  -42.   23.   27.  -26.   14.  -58.  -11.  -14.   15.   49.  -45.\n   75.  -10.  -20.   16.  -31.   -2.   -8.  -92.  -26.  -11.    1.   25.\n   57.   34.   18.    6.   15.   16.  -98.  -44.   18.   -7.    6.  124.\n  -43.   31.   31.  -17.  -14.   20.  -84.  -24.   -6.  -36.   67.    3.\n   32.   24.    9.   49.   -7.  -19. -126. -114.   29.   54.  105. -110.\n   30.   -3.  -52.  -40.   91.   85.  -51.  -77.  -41.   43.   56.  150.\n  197.   80.   47.   20. -180. -169.  -55.  -95.   28.  -52.   34.   13.\n   43.  -33.  -11.   12.  -33.   42.  -38.  -60.   54.   81.   74.  -57.\n  -47.  -33.  -65.  -25.  -81.   -9.  -63.  -48.  130.   90.  131.    9.\n   19.   16.  -77.  -45.   29.   61.  -46.    1.  143.   -5.   72.  -97.\n   -6.  -90.   -6.  110.  -16.   12.   -9.  -49.  -63.   13.   95.   -3.\n  -52.   21.   -1.    5.   32.   98.    1. -118.  -88. -106.  156.   18.\n  123.  104.  180.   38.  -53.  -19. -211. -106.   13.  -12.   61.   47.\n   27.    1.   33.   31.  -62.   -6. -207.  -64.  -63.  -21.   60.   53.\n -123.   89.    4.  -18.  -11.  151.  -47.  -78.   73. -104.  104.  -66.\n  -15.   -7.   40.   -1.  -56.  159.  -20. -110.  -27.   71.   47.  -46.\n   39.   53.    9.   71.  -36.  -17. -100.  -61.   -9.  129.  110.  -51.\n  -12.  -11.  -50.   53.   -8.  123.  164.   54.  -79. -254. -121.  -90.\n    3.   22.   -1.    4.  -58.   66.  -74.    0.   94.  -41.  -38.  -48.\n   -9.  -30.   24.  -12.   33.   82. -132.  -68.   39.  164.   -7.  159.\n  239.  -71.   17.  -88.    7.   39.]\n\n\n\n\n\n5 Árbol para Serie Sin Tendencia\n\n5.0.0.1 Entrenamiento, Validación y prueba\n\n\nCode\nY1 = y1\nprint('Complete Observations for Target after Supervised configuration: %d' %len(Y1))\ntraintarget_size = int(len(Y1) * 0.70) \nvaltarget_size = int(len(Y1) * 0.10)+1# Set split\ntesttarget_size = int(len(Y1) * 0.20)# Set split\nprint(traintarget_size,valtarget_size,testtarget_size)\nprint('Train + Validation + Test: %d' %(traintarget_size+valtarget_size+testtarget_size))\n\n\nComplete Observations for Target after Supervised configuration: 258\n180 26 51\nTrain + Validation + Test: 257\n\n\n\n\nCode\n# Target Train-Validation-Test split(70-10-20)\ntrain_target, val_target,test_target = Y1[0:traintarget_size], Y1[(traintarget_size):(traintarget_size+valtarget_size)],Y1[(traintarget_size+valtarget_size):len(Y1)]\n\nprint('Observations for Target: %d' % (len(Y1)))\nprint('Training Observations for Target: %d' % (len(train_target)))\nprint('Validation Observations for Target: %d' % (len(val_target)))\nprint('Test Observations for Target: %d' % (len(test_target)))\n\n\nObservations for Target: 258\nTraining Observations for Target: 180\nValidation Observations for Target: 26\nTest Observations for Target: 52\n\n\n\n\nCode\n# Features Train--Val-Test split\n\ntrainfeature_size = int(len(X1) * 0.70)\nvalfeature_size = int(len(X1) * 0.10)+1# Set split\ntestfeature_size = int(len(X1) * 0.20)# Set split\ntrain_feature, val_feature,test_feature = X1[0:traintarget_size],X1[(traintarget_size):(traintarget_size+valtarget_size)] ,X1[(traintarget_size+valtarget_size):len(Y1)]\n\nprint('Observations for Feature: %d' % (len(X1)))\nprint('Training Observations for Feature: %d' % (len(train_feature)))\nprint('Validation Observations for Feature: %d' % (len(val_feature)))\nprint('Test Observations for Feature: %d' % (len(test_feature)))\n\n\nObservations for Feature: 258\nTraining Observations for Feature: 180\nValidation Observations for Feature: 26\nTest Observations for Feature: 52\n\n\n\n\n5.0.1 Árbol\n\n\nCode\n# Decision Tree Regresion Model\nfrom sklearn.tree import DecisionTreeRegressor\n# Create a decision tree regression model with default arguments\ndecision_tree_Orig = DecisionTreeRegressor()  # max-depth not set\n# The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n# Fit the model to the training features(covariables) and targets(respuestas)\ndecision_tree_Orig.fit(train_feature, train_target)\n# Check the score on train and test\nprint(\"Coeficiente R2 sobre el conjunto de entrenamiento:\",decision_tree_Orig.score(train_feature, train_target))\nprint(\"Coeficiente R2 sobre el conjunto de Validación:\",decision_tree_Orig.score(val_feature,val_target))  # predictions are horrible if negative value, no relationship if 0\nprint(\"el RECM sobre validación es:\",(((decision_tree_Orig.predict(val_feature)-val_target)**2).mean()) )\n\n\nCoeficiente R2 sobre el conjunto de entrenamiento: 1.0\nCoeficiente R2 sobre el conjunto de Validación: -0.36155906119852443\nel RECM sobre validación es: 7475.307692307692\n\n\nVemos que el R2 para los datos de validación es malo pues es negativo, Se relizará un ajuste de la profundidad como hiperparametro para ver si mejora dicho valor\n\n\nCode\n# Find the best Max Depth\n\n# Loop through a few different max depths and check the performance\n# Try different max depths. We want to optimize our ML models to make the best predictions possible.\n# For regular decision trees, max_depth, which is a hyperparameter, limits the number of splits in a tree.\n# You can find the best value of max_depth based on the R-squared score of the model on the test set.\n\nfor d in [2, 3, 4, 5,6,7,8,9,10,11,12,13,14,15]:\n    # Create the tree and fit it\n    decision_tree_Orig = DecisionTreeRegressor(max_depth=d)\n    decision_tree_Orig.fit(train_feature, train_target)\n    \n    # Print out the scores on train and test\n    print('max_depth=', str(d))\n    print(\"Coeficiente R2 sobre el conjunto de entrenamiento:\",decision_tree_Orig.score(train_feature, train_target))\n    print(\"Coeficiente R2 sobre el conjunto de validación:\",decision_tree_Orig.score(val_feature, val_target), '\\n')  # You want the test score to be positive and high\n    print(\"el RECM sobre el conjunto de validación es:\",sklearn.metrics.mean_squared_error(decision_tree_Orig.predict(val_feature),val_target, squared=False), '\\n')\n\n\nmax_depth= 2\nCoeficiente R2 sobre el conjunto de entrenamiento: 0.25686511970310677\nCoeficiente R2 sobre el conjunto de validación: -0.2651415670711108 \n\nel RECM sobre el conjunto de validación es: 83.3423720244207 \n\nmax_depth= 3\nCoeficiente R2 sobre el conjunto de entrenamiento: 0.4245443431150394\nCoeficiente R2 sobre el conjunto de validación: -0.25312344192618963 \n\nel RECM sobre el conjunto de validación es: 82.94557487875332 \n\nmax_depth= 4\nCoeficiente R2 sobre el conjunto de entrenamiento: 0.570233841687424\nCoeficiente R2 sobre el conjunto de validación: -0.3946378641992552 \n\nel RECM sobre el conjunto de validación es: 87.50382155206147 \n\nmax_depth= 5\nCoeficiente R2 sobre el conjunto de entrenamiento: 0.7247525080241783\nCoeficiente R2 sobre el conjunto de validación: -0.19662975618360146 \n\nel RECM sobre el conjunto de validación es: 81.05432498970345 \n\nmax_depth= 6\nCoeficiente R2 sobre el conjunto de entrenamiento: 0.8709713841871555\nCoeficiente R2 sobre el conjunto de validación: -0.35559462890663207 \n\nel RECM sobre el conjunto de validación es: 86.27028128286491 \n\nmax_depth= 7\nCoeficiente R2 sobre el conjunto de entrenamiento: 0.9433510456062553\nCoeficiente R2 sobre el conjunto de validación: -0.709072303001282 \n\nel RECM sobre el conjunto de validación es: 96.86714780774055 \n\nmax_depth= 8\nCoeficiente R2 sobre el conjunto de entrenamiento: 0.9726668839828992\nCoeficiente R2 sobre el conjunto de validación: -0.6473773745003699 \n\nel RECM sobre el conjunto de validación es: 95.10269911072858 \n\nmax_depth= 9\nCoeficiente R2 sobre el conjunto de entrenamiento: 0.9865860401853749\nCoeficiente R2 sobre el conjunto de validación: -0.39089588349704174 \n\nel RECM sobre el conjunto de validación es: 87.38635107682887 \n\nmax_depth= 10\nCoeficiente R2 sobre el conjunto de entrenamiento: 0.9941479557500952\nCoeficiente R2 sobre el conjunto de validación: -0.48267762709243045 \n\nel RECM sobre el conjunto de validación es: 90.2234981331616 \n\nmax_depth= 11\nCoeficiente R2 sobre el conjunto de entrenamiento: 0.9978703406029361\nCoeficiente R2 sobre el conjunto de validación: -0.8745405463882787 \n\nel RECM sobre el conjunto de validación es: 101.44805235569653 \n\nmax_depth= 12\nCoeficiente R2 sobre el conjunto de entrenamiento: 0.9990766376693829\nCoeficiente R2 sobre el conjunto de validación: -0.4727281499619058 \n\nel RECM sobre el conjunto de validación es: 89.92026712424794 \n\nmax_depth= 13\nCoeficiente R2 sobre el conjunto de entrenamiento: 0.9995620305866239\nCoeficiente R2 sobre el conjunto de validación: -0.24182245414347547 \n\nel RECM sobre el conjunto de validación es: 82.57071561348538 \n\nmax_depth= 14\nCoeficiente R2 sobre el conjunto de entrenamiento: 0.9999093006589739\nCoeficiente R2 sobre el conjunto de validación: -0.5578788994919184 \n\nel RECM sobre el conjunto de validación es: 92.48326251897608 \n\nmax_depth= 15\nCoeficiente R2 sobre el conjunto de entrenamiento: 0.9999917449841307\nCoeficiente R2 sobre el conjunto de validación: -0.7418972235102907 \n\nel RECM sobre el conjunto de validación es: 97.79295239669135 \n\n\n\nNote que los scores para el conjunto de validación son negativos para todas las profundidades evaluadas. Tomaremos el más cercano a cero que el el de la profundidad 6. Ahora uniremos validacion y entrenamiento para re para reestimar los parametros\n\n\nCode\nprint(type(train_feature))\nprint(type(val_feature))\n#######\nprint(type(train_target))\nprint(type(val_target))\n####\nprint(train_feature.shape)\nprint(val_feature.shape)\n#####\n####\nprint(train_target.shape)\nprint(val_target.shape)\n###Concatenate Validation and test\ntrain_val_feature=np.concatenate((train_feature,val_feature),axis=0)\ntrain_val_target=np.concatenate((train_target,val_target),axis=0)\nprint(train_val_feature.shape)\nprint(train_val_target.shape)\n\n\n&lt;class 'numpy.ndarray'&gt;\n&lt;class 'numpy.ndarray'&gt;\n&lt;class 'numpy.ndarray'&gt;\n&lt;class 'numpy.ndarray'&gt;\n(180, 12)\n(26, 12)\n(180,)\n(26,)\n(206, 12)\n(206,)\n\n\n\n\nCode\n# Use the best max_depth \ndecision_tree_Orig = DecisionTreeRegressor(max_depth=6)  # fill in best max depth here\ndecision_tree_Orig.fit(train_val_feature, train_val_target)\n\n# Predict values for train and test\ntrain_val_prediction = decision_tree_Orig.predict(train_val_feature)\ntest_prediction = decision_tree_Orig.predict(test_feature)\n\n# Scatter the predictions vs actual values\nplt.scatter(train_val_prediction, train_val_target, label='train')  # blue\nplt.scatter(test_prediction, test_target, label='test')  # orange\n# Agrega títulos a los ejes\nplt.xlabel('Valores Predichos')  # Título para el eje x\nplt.ylabel('Valores Objetivo')  # Título para el eje y\n# Muestra una leyenda\nplt.legend()\nplt.show()\nprint(\"Raíz de la Pérdida cuadrática Entrenamiento:\",sklearn.metrics.mean_squared_error( train_val_prediction, train_val_target,squared=False))\n\nprint(\"Raíz de la Pérdida cuadrática Prueba:\",sklearn.metrics.mean_squared_error(test_prediction, test_target,squared=False))\n\n\n\n\n\nRaíz de la Pérdida cuadrática Entrenamiento: 34.410731597123124\nRaíz de la Pérdida cuadrática Prueba: 102.6782863348308\n\n\n\n\nCode\nfrom sklearn import tree\n\nlistacaract=list(df1_Ori.columns.values)\nrespuesta=listacaract.pop()\ntext_representation = tree.export_text(decision_tree_Orig)\nprint(text_representation)\n\n\n|--- feature_6 &lt;= 18.50\n|   |--- feature_5 &lt;= 80.00\n|   |   |--- feature_0 &lt;= -10.50\n|   |   |   |--- feature_9 &lt;= -42.50\n|   |   |   |   |--- feature_9 &lt;= -62.50\n|   |   |   |   |   |--- feature_4 &lt;= 18.00\n|   |   |   |   |   |   |--- value: [-42.50]\n|   |   |   |   |   |--- feature_4 &gt;  18.00\n|   |   |   |   |   |   |--- value: [-5.00]\n|   |   |   |   |--- feature_9 &gt;  -62.50\n|   |   |   |   |   |--- feature_9 &lt;= -59.00\n|   |   |   |   |   |   |--- value: [-64.00]\n|   |   |   |   |   |--- feature_9 &gt;  -59.00\n|   |   |   |   |   |   |--- value: [-107.00]\n|   |   |   |--- feature_9 &gt;  -42.50\n|   |   |   |   |--- feature_1 &lt;= 30.00\n|   |   |   |   |   |--- feature_9 &lt;= -16.50\n|   |   |   |   |   |   |--- value: [-54.60]\n|   |   |   |   |   |--- feature_9 &gt;  -16.50\n|   |   |   |   |   |   |--- value: [-3.48]\n|   |   |   |   |--- feature_1 &gt;  30.00\n|   |   |   |   |   |--- feature_10 &lt;= -35.50\n|   |   |   |   |   |   |--- value: [76.00]\n|   |   |   |   |   |--- feature_10 &gt;  -35.50\n|   |   |   |   |   |   |--- value: [15.00]\n|   |   |--- feature_0 &gt;  -10.50\n|   |   |   |--- feature_10 &lt;= 116.50\n|   |   |   |   |--- feature_8 &lt;= 48.00\n|   |   |   |   |   |--- feature_1 &lt;= -71.50\n|   |   |   |   |   |   |--- value: [-37.25]\n|   |   |   |   |   |--- feature_1 &gt;  -71.50\n|   |   |   |   |   |   |--- value: [29.84]\n|   |   |   |   |--- feature_8 &gt;  48.00\n|   |   |   |   |   |--- feature_5 &lt;= -15.50\n|   |   |   |   |   |   |--- value: [7.30]\n|   |   |   |   |   |--- feature_5 &gt;  -15.50\n|   |   |   |   |   |   |--- value: [-38.25]\n|   |   |   |--- feature_10 &gt;  116.50\n|   |   |   |   |--- feature_4 &lt;= 8.50\n|   |   |   |   |   |--- feature_3 &lt;= 32.50\n|   |   |   |   |   |   |--- value: [131.00]\n|   |   |   |   |   |--- feature_3 &gt;  32.50\n|   |   |   |   |   |   |--- value: [180.00]\n|   |   |   |   |--- feature_4 &gt;  8.50\n|   |   |   |   |   |--- feature_4 &lt;= 50.50\n|   |   |   |   |   |   |--- value: [31.00]\n|   |   |   |   |   |--- feature_4 &gt;  50.50\n|   |   |   |   |   |   |--- value: [80.00]\n|   |--- feature_5 &gt;  80.00\n|   |   |--- feature_8 &lt;= 26.00\n|   |   |   |--- feature_11 &lt;= 120.50\n|   |   |   |   |--- feature_0 &lt;= 111.50\n|   |   |   |   |   |--- feature_1 &lt;= 7.00\n|   |   |   |   |   |   |--- value: [75.17]\n|   |   |   |   |   |--- feature_1 &gt;  7.00\n|   |   |   |   |   |   |--- value: [116.50]\n|   |   |   |   |--- feature_0 &gt;  111.50\n|   |   |   |   |   |--- value: [159.00]\n|   |   |   |--- feature_11 &gt;  120.50\n|   |   |   |   |--- value: [197.00]\n|   |   |--- feature_8 &gt;  26.00\n|   |   |   |--- feature_10 &lt;= 12.00\n|   |   |   |   |--- value: [-84.00]\n|   |   |   |--- feature_10 &gt;  12.00\n|   |   |   |   |--- feature_7 &lt;= 22.50\n|   |   |   |   |   |--- value: [-15.00]\n|   |   |   |   |--- feature_7 &gt;  22.50\n|   |   |   |   |   |--- value: [-19.00]\n|--- feature_6 &gt;  18.50\n|   |--- feature_9 &lt;= 25.50\n|   |   |--- feature_0 &lt;= -7.50\n|   |   |   |--- feature_8 &lt;= -18.50\n|   |   |   |   |--- feature_4 &lt;= -24.50\n|   |   |   |   |   |--- feature_10 &lt;= 21.00\n|   |   |   |   |   |   |--- value: [32.00]\n|   |   |   |   |   |--- feature_10 &gt;  21.00\n|   |   |   |   |   |   |--- value: [150.00]\n|   |   |   |   |--- feature_4 &gt;  -24.50\n|   |   |   |   |   |--- feature_9 &lt;= -85.50\n|   |   |   |   |   |   |--- value: [23.33]\n|   |   |   |   |   |--- feature_9 &gt;  -85.50\n|   |   |   |   |   |   |--- value: [-37.38]\n|   |   |   |--- feature_8 &gt;  -18.50\n|   |   |   |   |--- feature_7 &lt;= 33.00\n|   |   |   |   |   |--- feature_11 &lt;= -112.00\n|   |   |   |   |   |   |--- value: [-114.00]\n|   |   |   |   |   |--- feature_11 &gt;  -112.00\n|   |   |   |   |   |   |--- value: [-27.77]\n|   |   |   |   |--- feature_7 &gt;  33.00\n|   |   |   |   |   |--- feature_1 &lt;= -84.50\n|   |   |   |   |   |   |--- value: [-102.00]\n|   |   |   |   |   |--- feature_1 &gt;  -84.50\n|   |   |   |   |   |   |--- value: [-66.50]\n|   |   |--- feature_0 &gt;  -7.50\n|   |   |   |--- feature_0 &lt;= -5.50\n|   |   |   |   |--- feature_6 &lt;= 79.00\n|   |   |   |   |   |--- value: [151.00]\n|   |   |   |   |--- feature_6 &gt;  79.00\n|   |   |   |   |   |--- value: [91.00]\n|   |   |   |--- feature_0 &gt;  -5.50\n|   |   |   |   |--- feature_11 &lt;= -85.50\n|   |   |   |   |   |--- value: [156.00]\n|   |   |   |   |--- feature_11 &gt;  -85.50\n|   |   |   |   |   |--- feature_2 &lt;= 36.50\n|   |   |   |   |   |   |--- value: [13.46]\n|   |   |   |   |   |--- feature_2 &gt;  36.50\n|   |   |   |   |   |   |--- value: [-46.67]\n|   |--- feature_9 &gt;  25.50\n|   |   |--- feature_4 &lt;= 27.50\n|   |   |   |--- feature_8 &lt;= 173.50\n|   |   |   |   |--- feature_2 &lt;= -38.50\n|   |   |   |   |   |--- feature_0 &lt;= -1.50\n|   |   |   |   |   |   |--- value: [-8.50]\n|   |   |   |   |   |--- feature_0 &gt;  -1.50\n|   |   |   |   |   |   |--- value: [20.00]\n|   |   |   |   |--- feature_2 &gt;  -38.50\n|   |   |   |   |   |--- feature_9 &lt;= 151.00\n|   |   |   |   |   |   |--- value: [-71.83]\n|   |   |   |   |   |--- feature_9 &gt;  151.00\n|   |   |   |   |   |   |--- value: [-3.50]\n|   |   |   |--- feature_8 &gt;  173.50\n|   |   |   |   |--- value: [-180.00]\n|   |   |--- feature_4 &gt;  27.50\n|   |   |   |--- feature_1 &lt;= -38.50\n|   |   |   |   |--- feature_11 &lt;= -99.50\n|   |   |   |   |   |--- value: [-169.00]\n|   |   |   |   |--- feature_11 &gt;  -99.50\n|   |   |   |   |   |--- feature_11 &lt;= -12.50\n|   |   |   |   |   |   |--- value: [-211.00]\n|   |   |   |   |   |--- feature_11 &gt;  -12.50\n|   |   |   |   |   |   |--- value: [-207.00]\n|   |   |   |--- feature_1 &gt;  -38.50\n|   |   |   |   |--- feature_1 &lt;= -25.00\n|   |   |   |   |   |--- value: [-58.00]\n|   |   |   |   |--- feature_1 &gt;  -25.00\n|   |   |   |   |   |--- value: [-126.00]\n\n\n\n\n\nCode\nfig = plt.figure(figsize=(25,20))\n_ = tree.plot_tree(decision_tree_Orig, \n                   feature_names=listacaract,  \n                   class_names=[respuesta],\n                   filled=True)\n\n\n\n\n\nAhora miraremos las predicciones comparadas con los valores verdaderos, para ver más claro lo anterior.\n\n\nCode\nprint(train_val_prediction.size)\nprint(train_val_target.size)\n\nprint(test_prediction.size)\nprint(test_target.size)\n\n\n206\n206\n52\n52\n\n\n\n\nCode\nindicetrian_val_test=df1_Ori.index\nprint(indicetrian_val_test.size)  ###Tamaño del índice\nindicetrain_val=indicetrian_val_test[0:206]\nindicetest=indicetrian_val_test[206:258]\n\n\n258\n\n\n\n\nCode\nprint(indicetrain_val.size)\nprint(indicetest.size)\n\n\n206\n52\n\n\n\n\nCode\ntargetjoint=np.concatenate((train_val_target,test_target))\npredictionjoint=np.concatenate((train_val_prediction,test_prediction))\nprint(targetjoint.size)\nprint(predictionjoint.size)\n\n\n258\n258\n\n\n\n\nCode\nd = {'observado': targetjoint, 'Predicción': predictionjoint}\nObsvsPred1=pd.DataFrame(data=d,index=indicetrian_val_test)\nObsvsPred1.tail(52)\n\n\n\n\n\n\n\n\n\nobservado\nPredicción\n\n\n\n\n2018-03-31\n9.0\n29.842105\n\n\n2018-04-30\n71.0\n-71.833333\n\n\n2018-05-31\n-36.0\n-71.833333\n\n\n2018-06-30\n-17.0\n-38.250000\n\n\n2018-07-31\n-100.0\n-211.000000\n\n\n2018-08-31\n-61.0\n-27.769231\n\n\n2018-09-30\n-9.0\n76.000000\n\n\n2018-10-31\n129.0\n13.461538\n\n\n2018-11-30\n110.0\n29.842105\n\n\n2018-12-31\n-51.0\n15.000000\n\n\n2019-01-31\n-12.0\n29.842105\n\n\n2019-02-28\n-11.0\n7.300000\n\n\n2019-03-31\n-50.0\n7.300000\n\n\n2019-04-30\n53.0\n13.461538\n\n\n2019-05-31\n-8.0\n-27.769231\n\n\n2019-06-30\n123.0\n75.166667\n\n\n2019-07-31\n164.0\n-3.476190\n\n\n2019-08-31\n54.0\n-3.476190\n\n\n2019-09-30\n-79.0\n131.000000\n\n\n2019-10-31\n-254.0\n20.000000\n\n\n2019-11-30\n-121.0\n-38.250000\n\n\n2019-12-31\n-90.0\n-66.500000\n\n\n2020-01-31\n3.0\n23.333333\n\n\n2020-02-29\n22.0\n23.333333\n\n\n2020-03-31\n-1.0\n-5.000000\n\n\n2020-04-30\n4.0\n29.842105\n\n\n2020-05-31\n-58.0\n29.842105\n\n\n2020-06-30\n66.0\n29.842105\n\n\n2020-07-31\n-74.0\n29.842105\n\n\n2020-08-31\n0.0\n13.461538\n\n\n2020-09-30\n94.0\n-3.476190\n\n\n2020-10-31\n-41.0\n-5.000000\n\n\n2020-11-30\n-38.0\n-3.476190\n\n\n2020-12-31\n-48.0\n-71.833333\n\n\n2021-01-31\n-9.0\n-38.250000\n\n\n2021-02-28\n-30.0\n29.842105\n\n\n2021-03-31\n24.0\n13.461538\n\n\n2021-04-30\n-12.0\n75.166667\n\n\n2021-05-31\n33.0\n15.000000\n\n\n2021-06-30\n82.0\n-37.250000\n\n\n2021-07-31\n-132.0\n-3.476190\n\n\n2021-08-31\n-68.0\n29.842105\n\n\n2021-09-30\n39.0\n-71.833333\n\n\n2021-10-31\n164.0\n-42.500000\n\n\n2021-11-30\n-7.0\n-37.375000\n\n\n2021-12-31\n159.0\n-71.833333\n\n\n2022-01-31\n239.0\n-84.000000\n\n\n2022-02-28\n-71.0\n-3.476190\n\n\n2022-03-31\n17.0\n-3.500000\n\n\n2022-04-30\n-88.0\n-3.500000\n\n\n2022-05-31\n7.0\n-19.000000\n\n\n2022-06-30\n39.0\n13.461538\n\n\n\n\n\n\n\n\n\nCode\n#gráfico\nax = ObsvsPred1['observado'].plot(marker=\"o\", figsize=(10, 6), linewidth=1, markersize=4)  # Ajusta el grosor de las líneas y puntos\nObsvsPred1['Predicción'].plot(marker=\"o\", linewidth=1, markersize=2, ax=ax)  # Ajusta el grosor de las líneas y puntos\n# Agrega una línea vertical roja\nax.axvline(x=indicetrian_val_test[223].date(), color='red', linewidth=0.5)  # Ajusta el grosor de la línea vertical\n# Muestra una leyenda\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n Volver arriba"
  },
  {
    "objectID": "red_bitcoin_con_1_bayesiano.html",
    "href": "red_bitcoin_con_1_bayesiano.html",
    "title": "Predicción 1 paso adelante usando el último retardo (bayesiano)",
    "section": "",
    "text": "Code\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import CSVLogger, EarlyStopping\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport seaborn as sns\nimport time\nimport gc\nimport sys\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\nCode\nprint(f\"Tensorflow Version: {tf.__version__}\")\nprint(f\"Pandas Version: {pd.__version__}\")\nprint(f\"Numpy Version: {np.__version__}\")\nprint(f\"System Version: {sys.version}\")\n\nmpl.rcParams['figure.figsize'] = (17, 5)\nmpl.rcParams['axes.grid'] = False\nsns.set_style(\"whitegrid\")\n\nnotebookstart= time.time()\nCode\nimport IPython\nimport IPython.display"
  },
  {
    "objectID": "red_bitcoin_con_1_bayesiano.html#preparación-de-los-datos",
    "href": "red_bitcoin_con_1_bayesiano.html#preparación-de-los-datos",
    "title": "Predicción 1 paso adelante usando el último retardo (bayesiano)",
    "section": "0.1 Preparación de los datos",
    "text": "0.1 Preparación de los datos\n\n\nCode\n# Lectura de la serie\nBitcoin = pd.read_csv(\"C:/Users/dofca/Desktop/series/datos/BTC-Daily.csv\",\n                   header = 0, usecols = [1,6])\nBitcoin.rename(columns={\"date\": \"Fecha\", \"close\": \"Valor\"}, inplace=True)\nBitcoin['Fecha'] = pd.to_datetime(Bitcoin['Fecha'])\nBitcoin.sort_values(by=['Fecha'], inplace=True)\nventana = (Bitcoin['Fecha'] &gt;= '2017-01-01') & (Bitcoin['Fecha'] &lt;= '2021-12-31')\nBitcoin = Bitcoin.loc[ventana]\nBitcoin = Bitcoin.reset_index(drop = True)\n\nBitcoin\n\n\n\n\n\n\n\n\n\nFecha\nValor\n\n\n\n\n0\n2017-01-01\n998.80\n\n\n1\n2017-01-02\n1014.10\n\n\n2\n2017-01-03\n1036.99\n\n\n3\n2017-01-04\n1122.56\n\n\n4\n2017-01-05\n994.02\n\n\n...\n...\n...\n\n\n1821\n2021-12-27\n50718.11\n\n\n1822\n2021-12-28\n47543.30\n\n\n1823\n2021-12-29\n46483.36\n\n\n1824\n2021-12-30\n47150.71\n\n\n1825\n2021-12-31\n46214.37\n\n\n\n\n1826 rows × 2 columns\n\n\n\n\n\nCode\nfeatures_considered = ['Valor'] # la variable a usar en la predicción es ella misma\n\n\n\n\nCode\nfeatures = Bitcoin[features_considered] # solo se usará la variable Total en la predicción\nfeatures.index = Bitcoin['Fecha'] # variable que indica el tiempo (la serie es mensual)\nfeatures\n\n\n\n\nCode\nfeatures.plot(subplots = True) # gráfico de la serie de tiempo\n\n\narray([&lt;Axes: xlabel='Fecha'&gt;], dtype=object)\n\n\n\n\n\n\n\n\n\n\n\nCode\ncolumn_indices = {name: i for i, name in enumerate(features.columns)}\n\nn = len(features)\ntrain_df = features[0:int(n*0.7)]\nval_df = features[int(n*0.7):int(n*0.9)]\ntest_df = features[int(n*0.9):]\n\nnum_features = features.shape[1]\n\n\n\n\nCode\ntrain_mean = train_df.mean()\ntrain_std = train_df.std()\n\ntrain_df = (train_df - train_mean) / train_std\nval_df = (val_df - train_mean) / train_std\ntest_df = (test_df - train_mean) / train_std\n\n\n\n\nCode\nprint(\"longitud dataframe entrenamiento:\", train_df.shape)\nprint(\"longitud dataframe validación:\", val_df.shape)\nprint(\"longitud dataframe prueba:\", test_df.shape)\n\n\nlongitud dataframe entrenamiento: (1278, 1)\nlongitud dataframe validación: (365, 1)\nlongitud dataframe prueba: (183, 1)\n\n\n\n\nCode\ndf_std = (features - train_mean) / train_std\ndf_std = df_std.melt(var_name='Column', value_name='Normalized')\ndf_std\n\n\n\n\nCode\nplt.figure(figsize=(9, 5))\nax = sns.violinplot(x = 'Column', y = 'Normalized', data = df_std)\n_ = ax.set_xticklabels(features.keys(), rotation=90)"
  },
  {
    "objectID": "red_bitcoin_con_1_bayesiano.html#definición-de-clases-y-funciones-para-el-problema-de-aprendizaje-automático",
    "href": "red_bitcoin_con_1_bayesiano.html#definición-de-clases-y-funciones-para-el-problema-de-aprendizaje-automático",
    "title": "Predicción 1 paso adelante usando el último retardo (bayesiano)",
    "section": "0.2 Definición de clases y funciones para el problema de aprendizaje automático",
    "text": "0.2 Definición de clases y funciones para el problema de aprendizaje automático\n\n\nCode\nclass WindowGenerator():\n  def __init__(self, input_width, label_width, shift,\n               train_df=train_df, val_df=val_df, test_df=test_df,\n               label_columns=None):\n    # Store the raw data.\n    self.train_df = train_df\n    self.val_df = val_df\n    self.test_df = test_df\n\n    # Work out the label column indices.\n    self.label_columns = label_columns\n    if label_columns is not None:\n      self.label_columns_indices = {name: i for i, name in\n                                    enumerate(label_columns)}\n    self.column_indices = {name: i for i, name in\n                           enumerate(train_df.columns)}\n\n    # Work out the window parameters.\n    self.input_width = input_width\n    self.label_width = label_width\n    self.shift = shift\n\n    self.total_window_size = input_width + shift\n\n    self.input_slice = slice(0, input_width)\n    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n\n    self.label_start = self.total_window_size - self.label_width\n    self.labels_slice = slice(self.label_start, None)\n    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n\n  def __repr__(self):\n    return '\\n'.join([\n        f'Total window size: {self.total_window_size}',\n        f'Input indices: {self.input_indices}',\n        f'Label indices: {self.label_indices}',\n        f'Label column name(s): {self.label_columns}'])"
  },
  {
    "objectID": "red_bitcoin_con_1_bayesiano.html#split",
    "href": "red_bitcoin_con_1_bayesiano.html#split",
    "title": "Predicción 1 paso adelante usando el último retardo (bayesiano)",
    "section": "0.3 Split",
    "text": "0.3 Split\nDada una lista de entradas consecutivas, el método split_window las convertirá en una ventana de entradas y una ventana de etiquetas.\n\n\nCode\ndef split_window(self, features):\n  inputs = features[:, self.input_slice, :]\n  labels = features[:, self.labels_slice, :]\n  if self.label_columns is not None:\n    labels = tf.stack(\n        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n        axis=-1)\n\n  # Slicing doesn't preserve static shape information, so set the shapes\n  # manually. This way the `tf.data.Datasets` are easier to inspect.\n  inputs.set_shape([None, self.input_width, None])\n  labels.set_shape([None, self.label_width, None])\n\n  return inputs, labels\n\nWindowGenerator.split_window = split_window"
  },
  {
    "objectID": "red_bitcoin_con_1_bayesiano.html#transforma-nuestros-objetos-a-tipo-tensorflow",
    "href": "red_bitcoin_con_1_bayesiano.html#transforma-nuestros-objetos-a-tipo-tensorflow",
    "title": "Predicción 1 paso adelante usando el último retardo (bayesiano)",
    "section": "0.4 Transforma nuestros objetos a tipo tensorflow",
    "text": "0.4 Transforma nuestros objetos a tipo tensorflow\nTamaño del lote batch size = 32\n\n\nCode\ndef make_dataset(self, data):\n  data = np.array(data, dtype=np.float32)\n  ds = tf.keras.utils.timeseries_dataset_from_array(\n      data=data,\n      targets=None,\n      sequence_length=self.total_window_size,\n      sequence_stride=1,\n      shuffle=False,\n      batch_size=32,) \n\n  ds = ds.map(self.split_window)\n\n  return ds\n\nWindowGenerator.make_dataset = make_dataset\n\n\n\n\nCode\n@property\ndef train(self):\n  return self.make_dataset(self.train_df)\n\n@property\ndef val(self):\n  return self.make_dataset(self.val_df)\n\n@property\ndef test(self):\n  return self.make_dataset(self.test_df)\n\n@property\ndef example(self):\n  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n  result = getattr(self, '_example', None)\n  if result is None:\n    # No example batch was found, so get one from the `.train` dataset\n    result = next(iter(self.train))\n    # And cache it for next time\n    self._example = result\n  return result\n\nWindowGenerator.train = train\nWindowGenerator.val = val\nWindowGenerator.test = test\nWindowGenerator.example = example"
  },
  {
    "objectID": "red_bitcoin_con_1_bayesiano.html#configuración-del-modelo",
    "href": "red_bitcoin_con_1_bayesiano.html#configuración-del-modelo",
    "title": "Predicción 1 paso adelante usando el último retardo (bayesiano)",
    "section": "2.1 Configuración del modelo",
    "text": "2.1 Configuración del modelo\nCon la información del últimos día, predecir el valor de la serie 1 paso adelante.\n\nw1 = WindowGenerator(input_width=1, label_width=1, shift=1,\n                     label_columns=['Valor'])\nw1\n\n\n\nCode\nfor batch in w1.train.take(1):\n    inputs_train,targets_train = batch\n    \nprint(\"Input shape:\", inputs_train.numpy().shape)\nprint(\"Target shape:\", targets_train.numpy().shape)\n\n\nInput shape: (32, 1, 1)\nTarget shape: (32, 1, 1)\n\n\n\n\nCode\nfor batch in w1.val.take(1):\n    inputs_val,targets_val = batch\n\nprint(\"Input shape:\", inputs_val.numpy().shape)\nprint(\"Target shape:\", targets_val.numpy().shape)\n\n\n\n\nCode\nfor batch in w1.test.take(1):\n    inputs_test,targets_test = batch\n\nprint(\"Input shape:\", inputs_val.numpy().shape)\nprint(\"Target shape:\", targets_val.numpy().shape)\n\n\n\n\nCode\nw1.train.element_spec\n\n\n\n\nCode\nw1.plot()\n\n\n\n\n\n\n\n\n\n\n\nCode\n## Ejemplo de los lotes en los datos de entrenamiento\ni=1\nfor batch in w1.train.take(1):\n    inputs, targets = batch\n    print(\"Covariable o input\",i,inputs)\n    print(\"Respuesta o etiqueta\",i,targets)\n    i=i+1\n\n\n\n\nCode\n## Ejemplo de los lotes en los datos de validación\ni=1\nfor batch in w1.val.take(1):\n    inputs, targets = batch\n    print(\"Covariable o input\",i,inputs)\n    print(\"Respuesta o etiqueta\",i,targets)\n    i=i+1\n\n\n\n\nCode\n## Ejemplo de los lotes en los datos de prueba\ni=1\nfor batch in w1.test.take(10):\n    inputs, targets = batch\n    print(\"Covariable o input\",i,inputs)\n    print(\"Respuesta o etiqueta\",i,targets)\n    i=i+1\n\n\n\n\nCode\ninput_dataset_train = w1.train.map(lambda x,y: x)\ntarget_dataset_train = w1.train.map(lambda x,y: y)\n\n\n\n\nCode\ninput_dataset_val = w1.val.map(lambda x,y: x)\ntarget_dataset_val = w1.val.map(lambda x,y: y)\n\n\n\n\nCode\ninput_dataset_test = w1.test.map(lambda x,y: x)\ntarget_dataset_test = w1.test.map(lambda x,y: y)"
  },
  {
    "objectID": "red_bitcoin_con_1_bayesiano.html#búsqueda-de-los-hiperparámetros-e-implementación-del-modelo",
    "href": "red_bitcoin_con_1_bayesiano.html#búsqueda-de-los-hiperparámetros-e-implementación-del-modelo",
    "title": "Predicción 1 paso adelante usando el último retardo (bayesiano)",
    "section": "2.2 búsqueda de los hiperparámetros e implementación del modelo",
    "text": "2.2 búsqueda de los hiperparámetros e implementación del modelo\n\n\nCode\nfrom tensorflow import keras\nimport keras_tuner as kt\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout\nfrom tensorflow.keras import layers\n\n\n\n\nCode\ndef build_model(hp):\n    model = keras.Sequential()\n    model.add(layers.LSTM(units=hp.Int('input_unit',min_value=32,max_value=512,step=32),activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"]),return_sequences=True))\n    for i in range(hp.Int('n_layers', 1, 4)):\n        model.add(layers.LSTM(hp.Int(f'lstm_{i}_units',min_value=32,max_value=512,step=32),activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"]),return_sequences=True))\n    model.add(layers.LSTM(hp.Int('layer_2_neurons',min_value=32,max_value=512,step=32),activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"])))\n    model.add(layers.Dropout(hp.Float('Dropout_rate',min_value=0,max_value=0.5,step=0.1)))\n    model.add(layers.Dense(1, activation=\"linear\"))\n    model.compile(loss='mean_squared_error', optimizer='adam',metrics = ['mse'])\n    return model\n\n\n\n\nCode\ntuner_LSTM = kt.BayesianOptimization(\n    hypermodel=build_model,\n    objective=\"val_loss\",\n    max_trials=50,\n    seed=1234,\n    overwrite=True,\n    directory=\"dirsalida\",\n    project_name=\"helloworld\"\n)\n\n\n\n\nCode\nstop_early = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",patience=0)\n\n\n\n\nCode\n# tuner_LSTM.search_space_summary()\n\n\n\n\nCode\ntuner_LSTM.search((w1.train), epochs=20, validation_data=(w1.val),callbacks=[stop_early])\n\n\n\n\nCode\n# Get the top 2 models.\nmodels_LSTM = tuner_LSTM.get_best_models(num_models=2)\nbest_model_LSTM = models_LSTM[0]\n# Build the model.\n# Needed for `Sequential` without specified `input_shape`.\nbest_model_LSTM.build(input_shape=(32, 1, 1))\nbest_model_LSTM.summary()\n\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm (LSTM)                 (32, 1, 192)              148992    \n                                                                 \n lstm_1 (LSTM)               (32, 1, 384)              886272    \n                                                                 \n lstm_2 (LSTM)               (32, 320)                 902400    \n                                                                 \n dropout (Dropout)           (32, 320)                 0         \n                                                                 \n dense (Dense)               (32, 1)                   321       \n                                                                 \n=================================================================\nTotal params: 1937985 (7.39 MB)\nTrainable params: 1937985 (7.39 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\nCode\ntuner_LSTM.results_summary()\n\n\n\n\nCode\ntrain_plus_val=w1.train.concatenate(w1.val)###verificar que en efecto\n\n\n\n\nCode\n# Get the top 2 hyperparameters.\nbest_hps_LSTM = tuner_LSTM.get_best_hyperparameters(5)\n# Build the model with the best hp.\ncallback=tf.keras.callbacks.EarlyStopping(monitor=\"loss\",patience=0)\nmodel_LSTM = build_model(best_hps_LSTM[0])\n# Fit with the entire dataset.\nmodel_LSTM.fit(train_plus_val, epochs=20,callbacks=[callback])\n\n\nUna vez re-entrenado el modelo con el conjunto de hiperparámetros hallado anteriormente, se obtiene el siguiente MSE en en conjunto de entrenamiento + validación:\n\n\nCode\nmodel_LSTM.evaluate(train_plus_val, verbose=0)\n\n\n[0.2007797658443451, 0.2007797658443451]\n\n\n\ny el MSE sobre el conjunto de prueba:\n\n\nCode\nmodel_LSTM.evaluate(w1.test, verbose=0)\n\n\n[1.0403189659118652, 1.0403189659118652]"
  },
  {
    "objectID": "red_bitcoin_con_1_bayesiano.html#predicción-sobre-el-conjunto-de-prueba",
    "href": "red_bitcoin_con_1_bayesiano.html#predicción-sobre-el-conjunto-de-prueba",
    "title": "Predicción 1 paso adelante usando el último retardo (bayesiano)",
    "section": "2.3 Predicción sobre el conjunto de prueba",
    "text": "2.3 Predicción sobre el conjunto de prueba\n\n\nCode\nprediction_test=(model_LSTM.predict(w1.test, verbose=1)*train_std['Valor']+train_mean['Valor'])\nprint(prediction_test.shape)\n\n\n\n\nCode\ni=1\nfor batch in target_dataset_test.take(10):\n    if i==1:\n        targets_test = batch.numpy()\n    elif i&gt;1:\n        targets_test_aux = batch.numpy()\n        targets_test=np.append(targets_test,targets_test_aux)\n    i=i+1\n    \nprint(targets_test.shape)\n\n\n\n\nCode\ntrue_series=targets_test*train_std['Valor']+train_mean['Valor']\ntrue_series=true_series.reshape((182,1,1))\nprint(true_series.shape)\n\n\nUna vez que se hacen las predicciones sobre el conjunto de prueba, se hace la comparación con los valores reales y se obtiene el siguiente RECM (en la escala original):\n\n\nCode\nerrors_squared=tf.keras.metrics.mean_squared_error(true_series, prediction_test).numpy()\nprint(\"RECM:\",errors_squared.mean()**0.5)\n\n\n\n\nCode\ntest_index=test_df.index[:182]\ntrue_series_final=true_series.reshape(182)\nprediction_test_final=prediction_test.reshape(182)\n\n\n\n\nCode\nplt.plot(true_series_final)\nplt.plot(prediction_test_final)\nplt.legend(['Respesta real','Predicción de la Respuesta'],loc='lower right', fontsize=15)\nplt.ylabel('Y y $\\hat{Y}$ en conjunto de prueba', fontsize=15)\nplt.title('Red neuronal recurrente: Predicciones sobre el conjunto de prueba', fontsize=20)\n\n\nText(0.5, 1.0, 'Red neuronal recurrente: Predicciones sobre el conjunto de prueba')\n\n\n\n\n\n\n\n\n\n\n\nCode\npredicciones_prueba = model_LSTM.predict(w1.test)*train_std['Valor']+train_mean['Valor']\ntrain_val_predict = model_LSTM.predict(train_plus_val)*train_std['Valor']+train_mean['Valor']\n\n\n\n\nCode\nplt.figure(figsize=(20,10))\nplt.title('Red neuronal recurrente para la serie de tiempo del Bitcoin', fontsize=20)\nplt.xlabel('Fecha', fontsize=18)\nplt.ylabel('Precio de cierre del Bitcoin', fontsize=18)\nplt.plot(Bitcoin['Fecha'], Bitcoin['Valor'])\nplt.plot(Bitcoin['Fecha'][1:1642], train_val_predict)\nplt.plot(Bitcoin['Fecha'][1644:1826], predicciones_prueba)\nplt.legend(['Serie original', 'Predicciones en entrenamiento + validación', 'Predicciones en prueba'], loc='lower right',\n          fontsize=15)\nplt.show()"
  },
  {
    "objectID": "red_bitcoin_con_1_bayesiano.html#errores-de-predicción-del-modelo",
    "href": "red_bitcoin_con_1_bayesiano.html#errores-de-predicción-del-modelo",
    "title": "Predicción 1 paso adelante usando el último retardo (bayesiano)",
    "section": "2.4 Errores de Predicción del Modelo",
    "text": "2.4 Errores de Predicción del Modelo\n\n\n2.4.1 Sobre el conjunto de entrenamiento\n\n\nCode\nlabels_train = np.concatenate([y for x, y in w1.train], axis=0)\nlabels_train.shape\n\n\n\n\nCode\nlista=list(w1.train.unbatch().map(lambda x, y: (x, y)))\n\n\n\n\nCode\nprediccion_intra_muestra=model_LSTM.predict(w1.train, verbose=1)\nprediccion_intra_muestra=prediccion_intra_muestra.reshape(1277,1,1)\n\n\n\n\nCode\neror_prediction_train=labels_train-prediccion_intra_muestra\n\n\n\n\nCode\nx_vals = train_df.index[1:]\n\n\n\n\nCode\nprint(eror_prediction_train.shape)\nprint(x_vals.shape)\n\n\n\n\nCode\neror_prediction_train=eror_prediction_train.reshape(eror_prediction_train.shape[0])\neror_prediction_train.shape\n\n\n\n\nCode\nfig = plt.figure(figsize=(15,8))\nplt.plot(eror_prediction_train)\nplt.ylabel('$Y-\\hat{Y}$', fontsize=14)\nplt.title('Error de predicción sobre los datos de entrenamiento', fontsize=16);\n\n\n\n\n\n\n\n\n\n\n\nCode\ngraficapacf=plot_pacf(eror_prediction_train,lags=50,method='ldbiased') ###Se puede usar también em method='ywmle'\ngraficaacf=plot_acf(eror_prediction_train,lags=50,adjusted='ldbiased')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n #### Sobre el conjunto de prueba\n\n\nCode\nlabels_test = np.concatenate([y for x, y in w1.test], axis=0)\nprediccion_conjunto_test=model_LSTM.predict(w1.test, verbose=1)\nprediccion_conjunto_test=prediccion_conjunto_test.reshape(182,1,1)\neror_prediction_test=labels_test-prediccion_conjunto_test\neror_prediction_test=eror_prediction_test.reshape(eror_prediction_test.shape[0])\n\n\n\n\nCode\nfig1 = plt.figure(figsize=(15,8))\nplt.plot(eror_prediction_test)\nplt.ylabel('$Y-\\hat{Y}$', fontsize=14)\nplt.title('Error de predicción sobre los datos de prueba', fontsize=16)\n\n\nText(0.5, 1.0, 'Error de predicción sobre los datos de prueba')\n\n\n\n\n\n\n\n\n\n\n\nCode\ngraficapacf=plot_pacf(eror_prediction_test,lags=50,method='ldbiased') ###Se puede usar también em method='ywmle'\ngraficaacf=plot_acf(eror_prediction_test,lags=50,adjusted='ldbiased')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n ## Comparación con el modelo base\n\n\nCode\nclass Baseline(tf.keras.Model):\n  def __init__(self, label_index=None):\n    super().__init__()\n    self.label_index = label_index\n\n  def call(self, inputs):\n    if self.label_index is None:\n      return inputs\n    result = inputs[:, :, self.label_index]\n    return result[:, :, tf.newaxis]\n\n\n\n\nCode\nbaseline = Baseline(label_index=column_indices['Valor'])\n\nbaseline.compile(loss=tf.losses.MeanSquaredError(),\n                 metrics=[tf.metrics.MeanAbsoluteError()])\n\nval_performance = {}\nperformance = {}\nval_performance['Baseline'] = baseline.evaluate(w1.val, verbose=1)\nperformance['Baseline'] = baseline.evaluate(w1.test, verbose=1)\n\n\n 1/12 [=&gt;............................] - ETA: 1s - loss: 0.0068 - mean_absolute_error: 0.0495\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 1ms/step - loss: 0.2116 - mean_absolute_error: 0.2842\n1/6 [====&gt;.........................] - ETA: 0s - loss: 0.1063 - mean_absolute_error: 0.2683\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b6/6 [==============================] - 0s 0s/step - loss: 0.2565 - mean_absolute_error: 0.3836\n\n\n\n\nCode\n## loss en LSTM es mayor\nprint(model_LSTM.evaluate(w1.val, verbose=0)) \nprint(model_LSTM.evaluate(w1.test, verbose=0))\n\n\n[0.5345371961593628, 0.5345371961593628]\n[1.0403189659118652, 1.0403189659118652]\n\n\n\n\nCode\nprediction_test_base=(baseline.predict(w1.test, verbose=1)*train_std['Valor']+train_mean['Valor'])\nprint(prediction_test_base.shape)\n\n\nEl RECM sobre el conjunto de prueba con el modelo base es de:\n\n\nCode\nerrors_squared=tf.keras.metrics.mean_squared_error(true_series, prediction_test_base).numpy()\nprint(\"RECM:\",errors_squared.mean()**0.5)\n\n\nRECM: 1703.3208006714412\n\n\n\n\nCode\ntest_index=test_df.index[:182]\ntrue_series_final=true_series.reshape(182)\nprediction_test_final_base=prediction_test_base.reshape(182)\n\n\n\n\nCode\nplt.plot(true_series_final)\nplt.plot(prediction_test_final_base)\nplt.legend(['Respesta real','Predicción de la Respuesta'],loc='lower right', fontsize=15)\nplt.ylabel('Y y $\\hat{Y}$ en conjunto de prueba', fontsize=15)\nplt.title('Red neuronal recurrente: Predicciones sobre el conjunto de prueba', fontsize=20)\n\n\nText(0.5, 1.0, 'Red neuronal recurrente: Predicciones sobre el conjunto de prueba')\n\n\n\n\n\n\n\n\n\n\n\nCode\npredicciones_prueba_base = (baseline.predict(w1.test)*train_std['Valor']+train_mean['Valor']).reshape(182)\ntrain_val_predict_base = (baseline.predict(train_plus_val)*train_std['Valor']+train_mean['Valor']).reshape(1641)\n\n\n\n\nCode\nplt.figure(figsize=(20,10))\nplt.title('Modelo base para la serie de tiempo del Bitcoin', fontsize=20)\nplt.xlabel('Fecha', fontsize=18)\nplt.ylabel('Precio de cierre del Bitcoin', fontsize=18)\nplt.plot(Bitcoin['Fecha'], Bitcoin['Valor'])\nplt.plot(Bitcoin['Fecha'][1:1642], train_val_predict_base)\nplt.plot(Bitcoin['Fecha'][1644:1826], predicciones_prueba_base)\nplt.legend(['Serie original', 'Predicciones en entrenamiento + validación', 'Predicciones en prueba'], loc='lower right',\n          fontsize=15)\nplt.show()"
  },
  {
    "objectID": "red_bitcoin_con_1_grilla.html",
    "href": "red_bitcoin_con_1_grilla.html",
    "title": "Predicción 1 paso adelante usando el último retardo (grilla)",
    "section": "",
    "text": "Code\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import CSVLogger, EarlyStopping\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport seaborn as sns\nimport time\nimport gc\nimport sys\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\nCode\nprint(f\"Tensorflow Version: {tf.__version__}\")\nprint(f\"Pandas Version: {pd.__version__}\")\nprint(f\"Numpy Version: {np.__version__}\")\nprint(f\"System Version: {sys.version}\")\n\nmpl.rcParams['figure.figsize'] = (17, 5)\nmpl.rcParams['axes.grid'] = False\nsns.set_style(\"whitegrid\")\n\nnotebookstart= time.time()\nCode\nimport IPython\nimport IPython.display"
  },
  {
    "objectID": "red_bitcoin_con_1_grilla.html#preparación-de-los-datos",
    "href": "red_bitcoin_con_1_grilla.html#preparación-de-los-datos",
    "title": "Predicción 1 paso adelante usando el último retardo (grilla)",
    "section": "0.1 Preparación de los datos",
    "text": "0.1 Preparación de los datos\n\n\nCode\n# Lectura de la serie\nBitcoin = pd.read_csv(\"C:/Users/dofca/Desktop/series/datos/BTC-Daily.csv\",\n                   header = 0, usecols = [1,6])\nBitcoin.rename(columns={\"date\": \"Fecha\", \"close\": \"Valor\"}, inplace=True)\nBitcoin['Fecha'] = pd.to_datetime(Bitcoin['Fecha'])\nBitcoin.sort_values(by=['Fecha'], inplace=True)\nventana = (Bitcoin['Fecha'] &gt;= '2017-01-01') & (Bitcoin['Fecha'] &lt;= '2021-12-31')\nBitcoin = Bitcoin.loc[ventana]\nBitcoin = Bitcoin.reset_index(drop = True)\n\nBitcoin\n\n\n\n\n\n\n\n\n\nFecha\nValor\n\n\n\n\n0\n2017-01-01\n998.80\n\n\n1\n2017-01-02\n1014.10\n\n\n2\n2017-01-03\n1036.99\n\n\n3\n2017-01-04\n1122.56\n\n\n4\n2017-01-05\n994.02\n\n\n...\n...\n...\n\n\n1821\n2021-12-27\n50718.11\n\n\n1822\n2021-12-28\n47543.30\n\n\n1823\n2021-12-29\n46483.36\n\n\n1824\n2021-12-30\n47150.71\n\n\n1825\n2021-12-31\n46214.37\n\n\n\n\n1826 rows × 2 columns\n\n\n\n\n\nCode\nfeatures_considered = ['Valor'] # la variable a usar en la predicción es ella misma\n\n\n\n\nCode\nfeatures = Bitcoin[features_considered] # solo se usará la variable Total en la predicción\nfeatures.index = Bitcoin['Fecha'] # variable que indica el tiempo (la serie es mensual)\nfeatures\n\n\n\n\nCode\nfeatures.plot(subplots = True) # gráfico de la serie de tiempo\n\n\narray([&lt;Axes: xlabel='Fecha'&gt;], dtype=object)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# partición del conjuntos de datos en entrenamiento, validación y prueba\ncolumn_indices = {name: i for i, name in enumerate(features.columns)} # índice = 0\n\nn = len(features) \ntrain_df = features[0:int(n*0.7)] \nval_df = features[int(n*0.7):int(n*0.9)] \ntest_df = features[int(n*0.9):] \n\nnum_features = features.shape[1]\n\n\n\n\nCode\nprint(\"longitud dataframe entrenamiento:\", train_df.shape)\nprint(\"longitud dataframe validación:\", val_df.shape)\nprint(\"longitud dataframe prueba:\", test_df.shape)\n\n\nlongitud dataframe entrenamiento: (1278, 1)\nlongitud dataframe validación: (365, 1)\nlongitud dataframe prueba: (183, 1)\n\n\n\n\nCode\n# Normalización de las observaciones\ntrain_mean = train_df.mean()\ntrain_std = train_df.std()\n\ntrain_df = (train_df - train_mean) / train_std\nval_df = (val_df - train_mean) / train_std\ntest_df = (test_df - train_mean) / train_std\n\n\n\n\nCode\n# todo el dataframe normalizado por train_mean y train_std\ndf_std = (features - train_mean) / train_std\ndf_std = df_std.melt(var_name='Column', value_name='Normalized')\ndf_std\n\n\n\n\nCode\nplt.figure(figsize=(9, 5))\nax = sns.violinplot(x = 'Column', y = 'Normalized', data = df_std)\n_ = ax.set_xticklabels(features.keys(), rotation=90)"
  },
  {
    "objectID": "red_bitcoin_con_1_grilla.html#definición-de-clases-y-funciones-para-el-problema-de-aprendizaje-automático",
    "href": "red_bitcoin_con_1_grilla.html#definición-de-clases-y-funciones-para-el-problema-de-aprendizaje-automático",
    "title": "Predicción 1 paso adelante usando el último retardo (grilla)",
    "section": "0.2 Definición de clases y funciones para el problema de aprendizaje automático",
    "text": "0.2 Definición de clases y funciones para el problema de aprendizaje automático\n\n\nCode\nclass WindowGenerator():\n  def __init__(self, input_width, label_width, shift,\n               train_df=train_df, val_df=val_df, test_df=test_df,\n               label_columns=None):\n    # Store the raw data.\n    self.train_df = train_df\n    self.val_df = val_df\n    self.test_df = test_df\n\n    # Work out the label column indices.\n    self.label_columns = label_columns\n    if label_columns is not None:\n      self.label_columns_indices = {name: i for i, name in\n                                    enumerate(label_columns)}\n    self.column_indices = {name: i for i, name in\n                           enumerate(train_df.columns)}\n\n    # Work out the window parameters.\n    self.input_width = input_width\n    self.label_width = label_width\n    self.shift = shift\n\n    self.total_window_size = input_width + shift\n\n    self.input_slice = slice(0, input_width)\n    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n\n    self.label_start = self.total_window_size - self.label_width\n    self.labels_slice = slice(self.label_start, None)\n    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n\n  def __repr__(self):\n    return '\\n'.join([\n        f'Total window size: {self.total_window_size}',\n        f'Input indices: {self.input_indices}',\n        f'Label indices: {self.label_indices}',\n        f'Label column name(s): {self.label_columns}'])"
  },
  {
    "objectID": "red_bitcoin_con_1_grilla.html#split",
    "href": "red_bitcoin_con_1_grilla.html#split",
    "title": "Predicción 1 paso adelante usando el último retardo (grilla)",
    "section": "0.3 Split",
    "text": "0.3 Split\nDada una lista de entradas consecutivas, el método split_window las convertirá en una ventana de entradas y una ventana de etiquetas.\n\n\nCode\ndef split_window(self, features):\n  inputs = features[:, self.input_slice, :]\n  labels = features[:, self.labels_slice, :]\n  if self.label_columns is not None:\n    labels = tf.stack(\n        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n        axis=-1)\n\n  # Slicing doesn't preserve static shape information, so set the shapes\n  # manually. This way the `tf.data.Datasets` are easier to inspect.\n  inputs.set_shape([None, self.input_width, None])\n  labels.set_shape([None, self.label_width, None])\n\n  return inputs, labels\n\nWindowGenerator.split_window = split_window"
  },
  {
    "objectID": "red_bitcoin_con_1_grilla.html#transforma-nuestros-objetos-a-tipo-tensorflow",
    "href": "red_bitcoin_con_1_grilla.html#transforma-nuestros-objetos-a-tipo-tensorflow",
    "title": "Predicción 1 paso adelante usando el último retardo (grilla)",
    "section": "0.4 Transforma nuestros objetos a tipo tensorflow",
    "text": "0.4 Transforma nuestros objetos a tipo tensorflow\nTamaño del lote batch size = 32\n\n\nCode\ndef make_dataset(self, data):\n  data = np.array(data, dtype=np.float32)\n  ds = tf.keras.utils.timeseries_dataset_from_array(\n      data=data,\n      targets=None,\n      sequence_length=self.total_window_size,\n      sequence_stride=1,\n      shuffle=False,\n      batch_size=32,) \n\n  ds = ds.map(self.split_window)\n\n  return ds\n\nWindowGenerator.make_dataset = make_dataset\n\n\n\n\nCode\n@property\ndef train(self):\n  return self.make_dataset(self.train_df)\n\n@property\ndef val(self):\n  return self.make_dataset(self.val_df)\n\n@property\ndef test(self):\n  return self.make_dataset(self.test_df)\n\n@property\ndef example(self):\n  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n  result = getattr(self, '_example', None)\n  if result is None:\n    # No example batch was found, so get one from the `.train` dataset\n    result = next(iter(self.train))\n    # And cache it for next time\n    self._example = result\n  return result\n\nWindowGenerator.train = train\nWindowGenerator.val = val\nWindowGenerator.test = test\nWindowGenerator.example = example"
  },
  {
    "objectID": "red_bitcoin_con_1_grilla.html#configuración-del-modelo",
    "href": "red_bitcoin_con_1_grilla.html#configuración-del-modelo",
    "title": "Predicción 1 paso adelante usando el último retardo (grilla)",
    "section": "2.1 Configuración del modelo",
    "text": "2.1 Configuración del modelo\n\nw1 = WindowGenerator(input_width=1, label_width=1, shift=1,\n                     label_columns=['Valor'])\nw1\n\n\n\nCode\nfor batch in w1.train.take(1):\n    inputs_train,targets_train = batch\n    \nprint(\"Input shape:\", inputs_train.numpy().shape)\nprint(\"Target shape:\", targets_train.numpy().shape)\n\n\nInput shape: (32, 1, 1)\nTarget shape: (32, 1, 1)\n\n\n\n\nCode\nfor batch in w1.val.take(1):\n    inputs_val,targets_val = batch\n\nprint(\"Input shape:\", inputs_val.numpy().shape)\nprint(\"Target shape:\", targets_val.numpy().shape)\n\n\n\n\nCode\nfor batch in w1.test.take(1):\n    inputs_test,targets_test = batch\n\nprint(\"Input shape:\", inputs_val.numpy().shape)\nprint(\"Target shape:\", targets_val.numpy().shape)\n\n\n\n\nCode\nw1.train.element_spec\n\n\n\n\nCode\nw1.plot()\n\n\n\n\n\n\n\n\n\n\n\nCode\n## Ejemplo de los lotes en los datos de entrenamiento\ni=1\nfor batch in w1.train.take(1):\n    inputs, targets = batch\n    print(\"Covariable o input\",i,inputs)\n    print(\"Respuesta o etiqueta\",i,targets)\n    i=i+1\n\n\n\n\nCode\n## Ejemplo de los lotes en los datos de validación\ni=1\nfor batch in w1.val.take(1):\n    inputs, targets = batch\n    print(\"Covariable o input\",i,inputs)\n    print(\"Respuesta o etiqueta\",i,targets)\n    i=i+1\n\n\n\n\nCode\n## Ejemplo de los lotes en los datos de prueba\ni=1\nfor batch in w1.test.take(10):\n    inputs, targets = batch\n    print(\"Covariable o input\",i,inputs)\n    print(\"Respuesta o etiqueta\",i,targets)\n    i=i+1\n\n\n\n\nCode\ninput_dataset_train = w1.train.map(lambda x,y: x)\ntarget_dataset_train = w1.train.map(lambda x,y: y)\n\n\n\n\nCode\ninput_dataset_val = w1.val.map(lambda x,y: x)\ntarget_dataset_val = w1.val.map(lambda x,y: y)\n\n\n\n\nCode\ninput_dataset_test = w1.test.map(lambda x,y: x)\ntarget_dataset_test = w1.test.map(lambda x,y: y)"
  },
  {
    "objectID": "red_bitcoin_con_1_grilla.html#búsqueda-de-los-hiperparámetros-e-implementación-del-modelo",
    "href": "red_bitcoin_con_1_grilla.html#búsqueda-de-los-hiperparámetros-e-implementación-del-modelo",
    "title": "Predicción 1 paso adelante usando el último retardo (grilla)",
    "section": "2.2 búsqueda de los hiperparámetros e implementación del modelo",
    "text": "2.2 búsqueda de los hiperparámetros e implementación del modelo\n\n\nCode\nfrom tensorflow import keras\nimport keras_tuner as kt\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout\nfrom tensorflow.keras import layers\n\n\n\n\nCode\ndef build_model(hp):\n    model = keras.Sequential()\n    model.add(layers.LSTM(units=hp.Int('input_unit',min_value=32,max_value=512,step=32),activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"]),return_sequences=True))\n    for i in range(hp.Int('n_layers', 1, 4)):\n        model.add(layers.LSTM(hp.Int(f'lstm_{i}_units',min_value=32,max_value=512,step=32),activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"]),return_sequences=True))\n    model.add(layers.LSTM(hp.Int('layer_2_neurons',min_value=32,max_value=512,step=32),activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"])))\n    model.add(layers.Dropout(hp.Float('Dropout_rate',min_value=0,max_value=0.5,step=0.1)))\n    model.add(layers.Dense(1, activation=\"linear\"))\n    model.compile(loss='mean_squared_error', optimizer='adam',metrics = ['mse'])\n    return model\n\n\n\n\nCode\ntuner_LSTM = kt.GridSearch(\n    hypermodel=build_model,\n    objective=\"val_loss\",\n    max_trials=50,\n    seed=1234,\n    overwrite=True,\n    directory=\"dirsalida\",\n    project_name=\"helloworld\"\n)\n\n\n\n\nCode\nstop_early = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",patience=0)\n\n\n\n\nCode\n# tuner_LSTM.search_space_summary()\n\n\n\n\nCode\ntuner_LSTM.search((w1.train), epochs=20, validation_data=(w1.val),callbacks=[stop_early])\n\n\n\n\nCode\n# Get the top 2 models.\nmodels_LSTM = tuner_LSTM.get_best_models(num_models=2)\nbest_model_LSTM = models_LSTM[0]\n# Build the model.\n# Needed for `Sequential` without specified `input_shape`.\nbest_model_LSTM.build(input_shape=(32, 1, 1))\nbest_model_LSTM.summary()\n\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm (LSTM)                 (32, 1, 32)               4352      \n                                                                 \n lstm_1 (LSTM)               (32, 1, 32)               8320      \n                                                                 \n lstm_2 (LSTM)               (32, 32)                  8320      \n                                                                 \n dropout (Dropout)           (32, 32)                  0         \n                                                                 \n dense (Dense)               (32, 1)                   33        \n                                                                 \n=================================================================\nTotal params: 21025 (82.13 KB)\nTrainable params: 21025 (82.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\nCode\ntuner_LSTM.results_summary()\n\n\n\n\nCode\ntrain_plus_val=w1.train.concatenate(w1.val)###verificar que en efecto\n\n\n\n\nCode\n# Get the top 2 hyperparameters.\nbest_hps_LSTM = tuner_LSTM.get_best_hyperparameters(5)\n# Build the model with the best hp.\ncallback=tf.keras.callbacks.EarlyStopping(monitor=\"loss\",patience=0)\nmodel_LSTM = build_model(best_hps_LSTM[0])\n# Fit with the entire dataset.\nmodel_LSTM.fit(train_plus_val, epochs=20,callbacks=[callback])\n\n\nUna vez re-entrenado el modelo con el conjunto de hiperparámetros hallado anteriormente, se obtiene el siguiente MSE en en conjunto de entrenamiento + validación:\n\n\nCode\nmodel_LSTM.evaluate(train_plus_val, verbose=0)\n\n\n[1.1643624305725098, 1.1643624305725098]\n\n\n\ny el MSE sobre el conjunto de prueba:\n\n\nCode\nmodel_LSTM.evaluate(w1.test, verbose=0)\n\n\n[6.678896903991699, 6.678896903991699]"
  },
  {
    "objectID": "red_bitcoin_con_1_grilla.html#predicción-sobre-el-conjunto-de-prueba",
    "href": "red_bitcoin_con_1_grilla.html#predicción-sobre-el-conjunto-de-prueba",
    "title": "Predicción 1 paso adelante usando el último retardo (grilla)",
    "section": "2.3 Predicción sobre el conjunto de prueba",
    "text": "2.3 Predicción sobre el conjunto de prueba\n\n\nCode\nprediction_test=(model_LSTM.predict(w1.test, verbose=1)*train_std['Valor']+train_mean['Valor'])\nprint(prediction_test.shape)\n\n\n\n\nCode\ni=1\nfor batch in target_dataset_test.take(10):\n    if i==1:\n        targets_test = batch.numpy()\n    elif i&gt;1:\n        targets_test_aux = batch.numpy()\n        targets_test=np.append(targets_test,targets_test_aux)\n    i=i+1\n    \nprint(targets_test.shape)\n\n\n\n\nCode\ntrue_series=targets_test*train_std['Valor']+train_mean['Valor']\ntrue_series=true_series.reshape((182,1,1))\nprint(true_series.shape)\n\n\nUna vez que se hacen las predicciones sobre el conjunto de prueba, se hace la comparación con los valores reales y se obtiene el siguiente RECM (en la escala original):\n\n\nCode\nerrors_squared=tf.keras.metrics.mean_squared_error(true_series, prediction_test).numpy()\nprint(\"RECM:\",errors_squared.mean()**0.5)\n\n\nRECM: 16257.529455609176\n\n\n\n\nCode\ntest_index=test_df.index[:182]\ntrue_series_final=true_series.reshape(182)\nprediction_test_final=prediction_test.reshape(182)\n\n\n\n\nCode\nplt.plot(true_series_final)\nplt.plot(prediction_test_final)\nplt.legend(['Respesta real','Predicción de la Respuesta'],loc='lower right', fontsize=15)\nplt.ylabel('Y y $\\hat{Y}$ en conjunto de prueba', fontsize=15)\nplt.title('Red neuronal recurrente: Predicciones sobre el conjunto de prueba', fontsize=20)\n\n\nText(0.5, 1.0, 'Red neuronal recurrente: Predicciones sobre el conjunto de prueba')\n\n\n\n\n\n\n\n\n\n\n\nCode\npredicciones_prueba = model_LSTM.predict(w1.test)*train_std['Valor']+train_mean['Valor']\ntrain_val_predict = model_LSTM.predict(train_plus_val)*train_std['Valor']+train_mean['Valor']\n\n\n\n\nCode\nplt.figure(figsize=(20,10))\nplt.title('Red neuronal recurrente para la serie de tiempo del Bitcoin', fontsize=20)\nplt.xlabel('Fecha', fontsize=18)\nplt.ylabel('Precio de cierre del Bitcoin', fontsize=18)\nplt.plot(Bitcoin['Fecha'], Bitcoin['Valor'])\nplt.plot(Bitcoin['Fecha'][1:1642], train_val_predict)\nplt.plot(Bitcoin['Fecha'][1644:1826], predicciones_prueba)\nplt.legend(['Serie original', 'Predicciones en entrenamiento + validación', 'Predicciones en prueba'], loc='lower right',\n          fontsize=15)\nplt.show()"
  },
  {
    "objectID": "red_bitcoin_con_1_grilla.html#errores-de-predicción-del-modelo",
    "href": "red_bitcoin_con_1_grilla.html#errores-de-predicción-del-modelo",
    "title": "Predicción 1 paso adelante usando el último retardo (grilla)",
    "section": "2.4 Errores de Predicción del Modelo",
    "text": "2.4 Errores de Predicción del Modelo\n\n\n2.4.1 Sobre el conjunto de entrenamiento\n\n\nCode\nlabels_train = np.concatenate([y for x, y in w1.train], axis=0)\nlabels_train.shape\n\n\n\n\nCode\nlista=list(w1.train.unbatch().map(lambda x, y: (x, y)))\n\n\n\n\nCode\nprediccion_intra_muestra=model_LSTM.predict(w1.train, verbose=1)\nprediccion_intra_muestra=prediccion_intra_muestra.reshape(1277,1,1)\n\n\n\n\nCode\neror_prediction_train=labels_train-prediccion_intra_muestra\n\n\n\n\nCode\nx_vals = train_df.index[1:]\n\n\n\n\nCode\nprint(eror_prediction_train.shape)\nprint(x_vals.shape)\n\n\n\n\nCode\neror_prediction_train=eror_prediction_train.reshape(eror_prediction_train.shape[0])\neror_prediction_train.shape\n\n\n\n\nCode\nfig = plt.figure(figsize=(15,8))\nplt.plot(eror_prediction_train)\nplt.ylabel('$Y-\\hat{Y}$', fontsize=14)\nplt.title('Error de predicción sobre los datos de entrenamiento', fontsize=16);\n\n\n\n\n\n\n\n\n\n\n\nCode\ngraficapacf=plot_pacf(eror_prediction_train,lags=50,method='ldbiased') ###Se puede usar también em method='ywmle'\ngraficaacf=plot_acf(eror_prediction_train,lags=50,adjusted='ldbiased')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.4.2 Sobre el conjunto de prueba\n\n\nCode\nlabels_test = np.concatenate([y for x, y in w1.test], axis=0)\nprediccion_conjunto_test=model_LSTM.predict(w1.test, verbose=1)\nprediccion_conjunto_test=prediccion_conjunto_test.reshape(182,1,1)\neror_prediction_test=labels_test-prediccion_conjunto_test\neror_prediction_test=eror_prediction_test.reshape(eror_prediction_test.shape[0])\n\n\n\n\nCode\nfig1 = plt.figure(figsize=(15,8))\nplt.plot(eror_prediction_test)\nplt.ylabel('$Y-\\hat{Y}$', fontsize=14)\nplt.title('Error de predicción sobre los datos de prueba', fontsize=16)\n\n\nText(0.5, 1.0, 'Error de predicción sobre los datos de prueba')\n\n\n\n\n\n\n\n\n\n\n\nCode\ngraficapacf=plot_pacf(eror_prediction_test,lags=50,method='ldbiased') ###Se puede usar también em method='ywmle'\ngraficaacf=plot_acf(eror_prediction_test,lags=50,adjusted='ldbiased')"
  },
  {
    "objectID": "red_exportaciones_con_12.html",
    "href": "red_exportaciones_con_12.html",
    "title": "Predicción 1 paso adelante usando 12 retardos",
    "section": "",
    "text": "Code\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import CSVLogger, EarlyStopping\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport seaborn as sns\nimport time\nimport gc\nimport sys\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\nprint(f\"Tensorflow Version: {tf.__version__}\")\nprint(f\"Pandas Version: {pd.__version__}\")\nprint(f\"Numpy Version: {np.__version__}\")\nprint(f\"System Version: {sys.version}\")\n\nmpl.rcParams['figure.figsize'] = (17, 5)\nmpl.rcParams['axes.grid'] = False\nsns.set_style(\"whitegrid\")\n\nnotebookstart= time.time()\nCode\nimport IPython\nimport IPython.display"
  },
  {
    "objectID": "red_exportaciones_con_12.html#preparación-de-los-datos",
    "href": "red_exportaciones_con_12.html#preparación-de-los-datos",
    "title": "Predicción 1 paso adelante usando 12 retardos",
    "section": "1 Preparación de los datos",
    "text": "1 Preparación de los datos\n\n\nCode\n# Lectura de la serie\nExportaciones = pd.read_excel(\"C:/Users/dofca/Desktop/series/datos/Exportaciones.xlsx\",\n                   header = 0, usecols = ['Mes','Total']).iloc[96:].reset_index(drop = True).round()\nExportaciones['Total'] = Exportaciones['Total'].astype(int) \nExportaciones\n\n\n\n\n\n\n\n\n\nMes\nTotal\n\n\n\n\n0\n2000-01-01\n1011676\n\n\n1\n2000-02-01\n1054098\n\n\n2\n2000-03-01\n1053546\n\n\n3\n2000-04-01\n886359\n\n\n4\n2000-05-01\n1146258\n\n\n...\n...\n...\n\n\n277\n2023-02-01\n4202234\n\n\n278\n2023-03-01\n4431911\n\n\n279\n2023-04-01\n3739214\n\n\n280\n2023-05-01\n4497862\n\n\n281\n2023-06-01\n3985981\n\n\n\n\n282 rows × 2 columns\n\n\n\n\n\nCode\nfeatures_considered = ['Total'] # la variable a usar en la predicción es ella misma\nfeatures = Exportaciones[features_considered] # solo se usará la variable Total en la predicción\nfeatures.index = Exportaciones['Mes'] # variable que indica el tiempo (la serie es mensual)\nfeatures.head()\n\n\n\n\nCode\nfeatures.plot(subplots = True, ) # gráfico de la serie de tiempo\n\n\narray([&lt;Axes: xlabel='Mes'&gt;], dtype=object)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# partición del conjuntos de datos en entrenamiento, validación y prueba\ncolumn_indices = {name: i for i, name in enumerate(features.columns)} # índice = 0\n\nn = len(features) # longitud de la serie (282)\ntrain_df = features[0:int(n*0.7)] # 2000-01 hasta 2016-05\nval_df = features[int(n*0.7):int(n*0.9)] # 2016-06 hasta 2021-01\ntest_df = features[int(n*0.9):] # 2021-02 hasta 2023-06\n\nnum_features = features.shape[1]\n\n\n\n\nCode\nprint(\"longitud dataframe entrenamiento:\", train_df.shape)\nprint(\"longitud dataframe validación:\", val_df.shape)\nprint(\"longitud dataframe prueba:\", test_df.shape)\n\n\nlongitud dataframe entrenamiento: (197, 1)\nlongitud dataframe validación: (56, 1)\nlongitud dataframe prueba: (29, 1)\n\n\n\n\nCode\n# Normalización de las observaciones\ntrain_mean = train_df.mean()\ntrain_std = train_df.std()\n\ntrain_df = (train_df - train_mean) / train_std\nval_df = (val_df - train_mean) / train_std\ntest_df = (test_df - train_mean) / train_std\n\n\n\n\nCode\n# todo el dataframe normalizado por train_mean y train_std\ndf_std = (features - train_mean) / train_std\ndf_std = df_std.melt(var_name='', value_name='Datos normalizados')\ndf_std\n\n\n\n\nCode\nplt.figure(figsize=(12, 8))\nax = sns.violinplot(x = '', y = 'Datos normalizados', data = df_std)\n_ = ax.set_xticklabels(features.keys(), rotation=0)"
  },
  {
    "objectID": "red_exportaciones_con_12.html#definición-de-clases-y-funciones-para-el-problema-de-aprendizaje-automático",
    "href": "red_exportaciones_con_12.html#definición-de-clases-y-funciones-para-el-problema-de-aprendizaje-automático",
    "title": "Predicción 1 paso adelante usando 12 retardos",
    "section": "2 Definición de clases y funciones para el problema de aprendizaje automático",
    "text": "2 Definición de clases y funciones para el problema de aprendizaje automático\n\n\nCode\nclass WindowGenerator():\n  def __init__(self, input_width, label_width, shift,\n               train_df=train_df, val_df=val_df, test_df=test_df,\n               label_columns=None):\n    # Store the raw data.\n    self.train_df = train_df\n    self.val_df = val_df\n    self.test_df = test_df\n\n    # Work out the label column indices.\n    self.label_columns = label_columns\n    if label_columns is not None:\n      self.label_columns_indices = {name: i for i, name in\n                                    enumerate(label_columns)}\n    self.column_indices = {name: i for i, name in\n                           enumerate(train_df.columns)}\n\n    # Work out the window parameters.\n    self.input_width = input_width\n    self.label_width = label_width\n    self.shift = shift\n\n    self.total_window_size = input_width + shift\n\n    self.input_slice = slice(0, input_width)\n    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n\n    self.label_start = self.total_window_size - self.label_width\n    self.labels_slice = slice(self.label_start, None)\n    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n\n  def __repr__(self):\n    return '\\n'.join([\n        f'Total window size: {self.total_window_size}',\n        f'Input indices: {self.input_indices}',\n        f'Label indices: {self.label_indices}',\n        f'Label column name(s): {self.label_columns}'])\n\n\n\n\nCode\ndef split_window(self, features):\n  inputs = features[:, self.input_slice, :]\n  labels = features[:, self.labels_slice, :]\n  if self.label_columns is not None:\n    labels = tf.stack(\n        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n        axis=-1)\n\n  # Slicing doesn't preserve static shape information, so set the shapes\n  # manually. This way the `tf.data.Datasets` are easier to inspect.\n  inputs.set_shape([None, self.input_width, None])\n  labels.set_shape([None, self.label_width, None])\n\n  return inputs, labels\n\nWindowGenerator.split_window = split_window\n\n\n\n\nCode\ndef make_dataset(self, data):\n  data = np.array(data, dtype=np.float32)\n  ds = tf.keras.utils.timeseries_dataset_from_array(\n      data=data,\n      targets=None,\n      sequence_length=self.total_window_size,\n      sequence_stride=1,\n      shuffle=False,\n      batch_size=32,) \n\n  ds = ds.map(self.split_window)\n\n  return ds\n\nWindowGenerator.make_dataset = make_dataset\n\n\n\n\nCode\n@property\ndef train(self):\n  return self.make_dataset(self.train_df)\n\n@property\ndef val(self):\n  return self.make_dataset(self.val_df)\n\n@property\ndef test(self):\n  return self.make_dataset(self.test_df)\n\n@property\ndef example(self):\n  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n  result = getattr(self, '_example', None)\n  if result is None:\n    # No example batch was found, so get one from the `.train` dataset\n    result = next(iter(self.train))\n    # And cache it for next time\n    self._example = result\n  return result\n\nWindowGenerator.train = train\nWindowGenerator.val = val\nWindowGenerator.test = test\nWindowGenerator.example = example\n\n\n\n\nCode\ndef plot(self, model=None, plot_col='Total', max_subplots=3):\n  inputs, labels = self.example\n  plt.figure(figsize=(12, 8))\n  plot_col_index = self.column_indices[plot_col]\n  max_n = min(max_subplots, len(inputs))\n  for n in range(max_n):\n    plt.subplot(max_n, 1, n+1)\n    plt.ylabel(f'{plot_col} [normed]')\n    plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n             label='Inputs', marker='.', zorder=-10)\n\n    if self.label_columns:\n      label_col_index = self.label_columns_indices.get(plot_col, None)\n    else:\n      label_col_index = plot_col_index\n\n    if label_col_index is None:\n      continue\n\n    plt.scatter(self.label_indices, labels[n, :, label_col_index],\n                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n    if model is not None:\n      predictions = model(inputs)\n      plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n                  marker='X', edgecolors='k', label='Predictions',\n                  c='#ff7f0e', s=64)\n\n    if n == 0:\n      plt.legend()\n\n  plt.xlabel('Time [h]')\n\nWindowGenerator.plot = plot\n\n\n\n\nCode\n# Definimos número de épocas necesarias y funciones de pérdida\nMAX_EPOCHS = 20\n\ndef compile_and_fit(model, window, patience=2): #patiences como el número de épocas que espera antes de parar\n  # Para evitar sobreajuste\n  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                                    patience=patience,\n                                                    mode='min')\n\n  model.compile(loss=tf.losses.MeanSquaredError(),\n                optimizer=tf.optimizers.Adam(),\n                metrics=[tf.metrics.MeanAbsoluteError()])\n\n  history = model.fit(window.train, epochs=MAX_EPOCHS,\n                      validation_data=window.val,\n                      callbacks=[early_stopping])\n  return history"
  },
  {
    "objectID": "red_exportaciones_con_12.html#configuración-del-modelo",
    "href": "red_exportaciones_con_12.html#configuración-del-modelo",
    "title": "Predicción 1 paso adelante usando 12 retardos",
    "section": "3 Configuración del modelo",
    "text": "3 Configuración del modelo\n\nw1 = WindowGenerator(input_width=12, label_width=1, shift=1,\n                     label_columns=['Total'])\nw1\n\n\n\nCode\nfor batch in w1.train.take(1):\n    inputs_train,targets_train = batch\n    \nprint(\"Input shape:\", inputs_train.numpy().shape)\nprint(\"Target shape:\", targets_train.numpy().shape)\n\n\nInput shape: (32, 12, 1)\nTarget shape: (32, 1, 1)\n\n\n\n\nCode\nfor batch in w1.val.take(1):\n    inputs_val,targets_val = batch\n\nprint(\"Input shape:\", inputs_val.numpy().shape)\nprint(\"Target shape:\", targets_val.numpy().shape)\n\n\n\n\nCode\nfor batch in w1.test.take(1):\n    inputs_test,targets_test = batch\n\nprint(\"Input shape:\", inputs_val.numpy().shape)\nprint(\"Target shape:\", targets_val.numpy().shape)\n\n\n\n\nCode\nw1.train.element_spec\n\n\n\n\nCode\nw1.plot()\n\n\n\n\n\n\n\n\n\n\n\nCode\n## Ejemplo de los lotes en los datos de entrenamiento\ni=1\nfor batch in w1.train.take(1):\n    inputs, targets = batch\n    print(\"Covariable o input\",i,inputs)\n    print(\"Respuesta o etiqueta\",i,targets)\n    i=i+1\n\n\n\n\nCode\n## Ejemplo de los lotes en los datos de validación\ni=1\nfor batch in w1.val.take(1):\n    inputs, targets = batch\n    print(\"Covariable o input\",i,inputs)\n    print(\"Respuesta o etiqueta\",i,targets)\n    i=i+1\n\n\n\n\nCode\n## Ejemplo de los lotes en los datos de prueba\ni=1\nfor batch in w1.test.take(1):\n    inputs, targets = batch\n    print(\"Covariable o input\",i,inputs)\n    print(\"Respuesta o etiqueta\",i,targets)\n    i=i+1\n\n\n\n\nCode\ninput_dataset_train = w1.train.map(lambda x,y: x)\ntarget_dataset_train = w1.train.map(lambda x,y: y)\n\n\n\n\nCode\ninput_dataset_val = w1.val.map(lambda x,y: x)\ntarget_dataset_val = w1.val.map(lambda x,y: y)\n\n\n\n\nCode\ninput_dataset_test = w1.test.map(lambda x,y: x)\ntarget_dataset_test = w1.test.map(lambda x,y: y)"
  },
  {
    "objectID": "red_exportaciones_con_12.html#búsqueda-de-los-hiperparámetros-e-implementación-del-modelo",
    "href": "red_exportaciones_con_12.html#búsqueda-de-los-hiperparámetros-e-implementación-del-modelo",
    "title": "Predicción 1 paso adelante usando 12 retardos",
    "section": "4 búsqueda de los hiperparámetros e implementación del modelo",
    "text": "4 búsqueda de los hiperparámetros e implementación del modelo\n\n\nCode\nfrom tensorflow import keras\nimport keras_tuner as kt\nfrom tensorflow.keras import layers\nfrom keras.layers import Dense, Activation, Flatten\n\n\n\n\nCode\ndef build_model(hp):\n    model = keras.Sequential()\n    model.add(layers.Dense(units=hp.Int(\"num_units\", min_value=32, max_value=564, step=32),\n                activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"])))\n    # Tune the number of layers.\n    for i in range(hp.Int(\"num_layers\", 1, 5)):\n        model.add(\n            layers.Dense(\n                # Tune number of units separately.\n                units=hp.Int(f\"units_{i}\", min_value=32, max_value=564, step=32),\n                activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"]),\n            )\n        )\n    if hp.Boolean(\"dropout\"):\n        model.add(layers.Dropout(rate=0.25))\n    model.add(Flatten())\n    model.add(layers.Dense(1, activation=\"linear\"))\n    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n        loss=\"mean_squared_error\",\n        metrics=[\"mean_squared_error\"]\n    )\n    return model\n\n\nbuild_model(kt.HyperParameters())\n\n\n\n\nCode\ntuner_BayesianOptimization_mlp = kt.BayesianOptimization(\n    hypermodel=build_model,\n    objective=\"val_loss\",\n    max_trials=50,\n    seed=12325,\n    overwrite=True,\n    directory=\"dirsalida\",\n    project_name=\"helloworld\"\n)\n\n\n\n\nCode\n# tuner_BayesianOptimization_mlp.search_space_summary()\n\n\n\n\nCode\nstop_early=tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",patience=0)\n\n\n\n\nCode\ntuner_BayesianOptimization_mlp.search((w1.train), epochs=20, validation_data=(w1.val),callbacks=[stop_early])\n\n\n\n\nCode\n# Get the top 2 models.\nmodels_mlp = tuner_BayesianOptimization_mlp.get_best_models(num_models=2)\nbest_model_mlp = models_mlp[0]\n# Build the model.\n# Needed for `Sequential` without specified `input_shape`.\nbest_model_mlp.build(input_shape=(32,12,1))\nbest_model_mlp.summary()\n\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               (32, 12, 384)             768       \n                                                                 \n dense_1 (Dense)             (32, 12, 160)             61600     \n                                                                 \n flatten (Flatten)           (32, 1920)                0         \n                                                                 \n dense_2 (Dense)             (32, 1)                   1921      \n                                                                 \n=================================================================\nTotal params: 64289 (251.13 KB)\nTrainable params: 64289 (251.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\nCode\ntuner_BayesianOptimization_mlp.results_summary()\n\n\n\n\nCode\ntrain_plus_val=w1.train.concatenate(w1.val)###verificar que en efecto\ntrain_plus_val\n\n\n\n\nCode\n# Get the top 2 hyperparameters.\nbest_hps_mlp = tuner_BayesianOptimization_mlp.get_best_hyperparameters(5)\n# Build the model with the best hp.\ncallback=tf.keras.callbacks.EarlyStopping(monitor=\"loss\",patience=5)\nmodel_mlp = build_model(best_hps_mlp[0])\n# Fit with the entire dataset.\nmodel_mlp.fit(train_plus_val, epochs=20,callbacks=[callback])\n\n\nUna vez re-entrenado el modelo con el conjunto de hiperparámetros hallado anteriormente, se obtiene el siguiente MSE en en conjunto de entrenamiento + validación:\n\n\nCode\nmodel_mlp.evaluate(train_plus_val, verbose=0)\n\n\n[0.04041048884391785, 0.04041048884391785]\n\n\n\ny el MSE sobre el conjunto de prueba:\n\n\nCode\nmodel_mlp.evaluate(w1.test, verbose=0)\n\n\n[0.17413842678070068, 0.17413842678070068]"
  },
  {
    "objectID": "red_exportaciones_con_12.html#predicción-sobre-el-conjunto-de-prueba",
    "href": "red_exportaciones_con_12.html#predicción-sobre-el-conjunto-de-prueba",
    "title": "Predicción 1 paso adelante usando 12 retardos",
    "section": "5 Predicción sobre el conjunto de prueba",
    "text": "5 Predicción sobre el conjunto de prueba\n\n\nCode\nprediction_test=(model_mlp.predict(w1.test, verbose=1)*train_std['Total']+train_mean['Total'])\nprint(prediction_test.shape)\n\n\n\n\nCode\ni=1\nfor batch in target_dataset_test.take(10):\n    if i==1:\n        targets_test = batch.numpy()\n    elif i&gt;1:\n        targets_test_aux = batch.numpy()\n        targets_test=np.append(targets_test,targets_test_aux)\n    i=i+1\n    \nprint(targets_test.shape)\n\n\n\n\nCode\ntrue_series=targets_test*train_std['Total']+train_mean['Total']\ntrue_series=true_series.reshape((17,1,1))\nprint(true_series.shape)\n\n\nUna vez que se hacen las predicciones sobre el conjunto de prueba, se hace la comparación con los valores reales y se obtiene el siguiente RECM (en la escala original):\n\n\nCode\nerrors_squared=tf.keras.metrics.mean_squared_error(true_series, prediction_test).numpy()\nprint(\"RECM:\",errors_squared.mean()**0.5)\n\n\nRECM: 759647.8106069944\n\n\n\n\nCode\ntest_index=test_df.index[:17]\ntrue_series_final=true_series.reshape(17)\nprediction_test_final=prediction_test.reshape(17)\n\n\n\n\nCode\nplt.plot(true_series_final)\nplt.plot(prediction_test_final)\nplt.legend(['Respesta real','Predicción de la Respuesta'],loc='lower right', fontsize=15)\nplt.ylabel('Y y $\\hat{Y}$ en conjunto de prueba', fontsize=15)\nplt.title('Red neuronal multicapa: Predicciones sobre el conjunto de prueba', fontsize=20)\n\n\nText(0.5, 1.0, 'Red neuronal multicapa: Predicciones sobre el conjunto de prueba')\n\n\n\n\n\n\n\n\n\n\n\nCode\npredicciones_prueba = model_mlp.predict(w1.test)*train_std['Total']+train_mean['Total']\ntrain_val_predict = model_mlp.predict(train_plus_val)*train_std['Total']+train_mean['Total']\n\n\n\n\nCode\n#| scrolled: true\nplt.figure(figsize=(16,8))\nplt.title('Red neuronal multicapa para la serie de tiempo de exportaciones', fontsize=20)\nplt.xlabel('Fecha', fontsize=18)\nplt.ylabel('Total de exportaciones', fontsize=18)\nplt.plot(Exportaciones['Mes'], Exportaciones['Total'])\nplt.plot(Exportaciones['Mes'][12:196], train_val_predict[1:185])\nplt.plot(Exportaciones['Mes'][208:251], train_val_predict[186:])\nplt.plot(Exportaciones['Mes'][264:281], predicciones_prueba)\nplt.legend(['Serie original', 'Predicciones en entrenamiento + validación', 'Predicciones en prueba'], loc='lower right',\n          fontsize=15)\nplt.show()"
  },
  {
    "objectID": "red_exportaciones_con_12.html#errores-de-predicción-del-modelo",
    "href": "red_exportaciones_con_12.html#errores-de-predicción-del-modelo",
    "title": "Predicción 1 paso adelante usando 12 retardos",
    "section": "6 Errores de Predicción del Modelo",
    "text": "6 Errores de Predicción del Modelo\n\n\n6.1 Sobre el conjunto de entrenamiento\n\n\nCode\nlabels_train = np.concatenate([y for x, y in w1.train], axis=0)\nlabels_train.shape\n\n\n\n\nCode\nlista=list(w1.train.unbatch().map(lambda x, y: (x, y)))\n\n\n\n\nCode\nprediccion_intra_muestra=model_mlp.predict(w1.train, verbose=1)\nprediccion_intra_muestra=prediccion_intra_muestra.reshape(185,1,1)\n\n\n\n\nCode\neror_prediction_train=labels_train-prediccion_intra_muestra\n\n\n\n\nCode\nx_vals = train_df.index[1:]\n\n\n\n\nCode\nprint(eror_prediction_train.shape)\nprint(x_vals.shape)\n\n\n\n\nCode\neror_prediction_train=eror_prediction_train.reshape(eror_prediction_train.shape[0])\neror_prediction_train.shape\n\n\n\n\nCode\nfig = plt.figure(figsize=(15,8))\nplt.plot(eror_prediction_train)\nplt.ylabel('$Y-\\hat{Y}$', fontsize=14)\nplt.title('Error de predicción sobre los datos de entrenamiento', fontsize=16);\n\n\n\n\n\n\n\n\n\n\n\nCode\ngraficapacf=plot_pacf(eror_prediction_train,lags=50,method='ldbiased') ###Se puede usar también em method='ywmle'\ngraficaacf=plot_acf(eror_prediction_train,lags=50,adjusted='ldbiased')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.2 Sobre el conjunto de prueba\n\n\nCode\nlabels_test = np.concatenate([y for x, y in w1.test], axis=0)\nlabels_test.shape\n\n\n\n\nCode\nprediccion_conjunto_test=model_mlp.predict(w1.test, verbose=1)\nprediccion_conjunto_test=prediccion_conjunto_test.reshape(17,1,1)\neror_prediction_test=labels_test-prediccion_conjunto_test\neror_prediction_test=eror_prediction_test.reshape(eror_prediction_test.shape[0])\n\n\n\n\nCode\nfig1 = plt.figure(figsize=(15,8))\nplt.plot(eror_prediction_test)\nplt.ylabel('$Y-\\hat{Y}$', fontsize=14)\nplt.title('Error de predicción sobre los datos de prueba', fontsize=16)\n\n\nText(0.5, 1.0, 'Error de predicción sobre los datos de prueba')\n\n\n\n\n\n\n\n\n\n\n\nCode\ngraficapacf=plot_pacf(eror_prediction_test,lags=7,method='ldbiased') ###Se puede usar también em method='ywmle'\ngraficaacf=plot_acf(eror_prediction_test,lags=7,adjusted='ldbiased')"
  },
  {
    "objectID": "red_exportaciones_con_2.html",
    "href": "red_exportaciones_con_2.html",
    "title": "Predicción 1 paso adelante usando 2 retardos",
    "section": "",
    "text": "Code\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import CSVLogger, EarlyStopping\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport seaborn as sns\nimport time\nimport gc\nimport sys\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\nprint(f\"Tensorflow Version: {tf.__version__}\")\nprint(f\"Pandas Version: {pd.__version__}\")\nprint(f\"Numpy Version: {np.__version__}\")\nprint(f\"System Version: {sys.version}\")\n\nmpl.rcParams['figure.figsize'] = (17, 5)\nmpl.rcParams['axes.grid'] = False\nsns.set_style(\"whitegrid\")\n\nnotebookstart= time.time()\nCode\nimport IPython\nimport IPython.display"
  },
  {
    "objectID": "red_exportaciones_con_2.html#preparación-de-los-datos",
    "href": "red_exportaciones_con_2.html#preparación-de-los-datos",
    "title": "Predicción 1 paso adelante usando 2 retardos",
    "section": "1 Preparación de los datos",
    "text": "1 Preparación de los datos\n\n\nCode\n# Lectura de la serie\nExportaciones = pd.read_excel(\"C:/Users/dofca/Desktop/series/datos/Exportaciones.xlsx\",\n                   header = 0, usecols = ['Mes','Total']).iloc[96:].reset_index(drop = True).round()\nExportaciones['Total'] = Exportaciones['Total'].astype(int) \nExportaciones\n\n\n\n\n\n\n\n\n\nMes\nTotal\n\n\n\n\n0\n2000-01-01\n1011676\n\n\n1\n2000-02-01\n1054098\n\n\n2\n2000-03-01\n1053546\n\n\n3\n2000-04-01\n886359\n\n\n4\n2000-05-01\n1146258\n\n\n...\n...\n...\n\n\n277\n2023-02-01\n4202234\n\n\n278\n2023-03-01\n4431911\n\n\n279\n2023-04-01\n3739214\n\n\n280\n2023-05-01\n4497862\n\n\n281\n2023-06-01\n3985981\n\n\n\n\n282 rows × 2 columns\n\n\n\n\n\nCode\nfeatures_considered = ['Total'] # la variable a usar en la predicción es ella misma\nfeatures = Exportaciones[features_considered] # solo se usará la variable Total en la predicción\nfeatures.index = Exportaciones['Mes'] # variable que indica el tiempo (la serie es mensual)\nfeatures.head()\n\n\n\n\nCode\nfeatures.plot(subplots = True, ) # gráfico de la serie de tiempo\n\n\narray([&lt;Axes: xlabel='Mes'&gt;], dtype=object)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# partición del conjuntos de datos en entrenamiento, validación y prueba\ncolumn_indices = {name: i for i, name in enumerate(features.columns)} # índice = 0\n\nn = len(features) # longitud de la serie (282)\ntrain_df = features[0:int(n*0.7)] # 2000-01 hasta 2016-05\nval_df = features[int(n*0.7):int(n*0.9)] # 2016-06 hasta 2021-01\ntest_df = features[int(n*0.9):] # 2021-02 hasta 2023-06\n\nnum_features = features.shape[1]\n\n\n\n\nCode\nprint(\"longitud dataframe entrenamiento:\", train_df.shape)\nprint(\"longitud dataframe validación:\", val_df.shape)\nprint(\"longitud dataframe prueba:\", test_df.shape)\n\n\nlongitud dataframe entrenamiento: (197, 1)\nlongitud dataframe validación: (56, 1)\nlongitud dataframe prueba: (29, 1)\n\n\n\n\nCode\n# Normalización de las observaciones\ntrain_mean = train_df.mean()\ntrain_std = train_df.std()\n\ntrain_df = (train_df - train_mean) / train_std\nval_df = (val_df - train_mean) / train_std\ntest_df = (test_df - train_mean) / train_std\n\n\n\n\nCode\n# todo el dataframe normalizado por train_mean y train_std\ndf_std = (features - train_mean) / train_std\ndf_std = df_std.melt(var_name='', value_name='Datos normalizados')\ndf_std\n\n\n\n\nCode\nplt.figure(figsize=(12, 8))\nax = sns.violinplot(x = '', y = 'Datos normalizados', data = df_std)\n_ = ax.set_xticklabels(features.keys(), rotation=0)"
  },
  {
    "objectID": "red_exportaciones_con_2.html#definición-de-clases-y-funciones-para-el-problema-de-aprendizaje-automático",
    "href": "red_exportaciones_con_2.html#definición-de-clases-y-funciones-para-el-problema-de-aprendizaje-automático",
    "title": "Predicción 1 paso adelante usando 2 retardos",
    "section": "2 Definición de clases y funciones para el problema de aprendizaje automático",
    "text": "2 Definición de clases y funciones para el problema de aprendizaje automático\n\n\nCode\nclass WindowGenerator():\n  def __init__(self, input_width, label_width, shift,\n               train_df=train_df, val_df=val_df, test_df=test_df,\n               label_columns=None):\n    # Store the raw data.\n    self.train_df = train_df\n    self.val_df = val_df\n    self.test_df = test_df\n\n    # Work out the label column indices.\n    self.label_columns = label_columns\n    if label_columns is not None:\n      self.label_columns_indices = {name: i for i, name in\n                                    enumerate(label_columns)}\n    self.column_indices = {name: i for i, name in\n                           enumerate(train_df.columns)}\n\n    # Work out the window parameters.\n    self.input_width = input_width\n    self.label_width = label_width\n    self.shift = shift\n\n    self.total_window_size = input_width + shift\n\n    self.input_slice = slice(0, input_width)\n    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n\n    self.label_start = self.total_window_size - self.label_width\n    self.labels_slice = slice(self.label_start, None)\n    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n\n  def __repr__(self):\n    return '\\n'.join([\n        f'Total window size: {self.total_window_size}',\n        f'Input indices: {self.input_indices}',\n        f'Label indices: {self.label_indices}',\n        f'Label column name(s): {self.label_columns}'])\n\n\n\n\nCode\ndef split_window(self, features):\n  inputs = features[:, self.input_slice, :]\n  labels = features[:, self.labels_slice, :]\n  if self.label_columns is not None:\n    labels = tf.stack(\n        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n        axis=-1)\n\n  # Slicing doesn't preserve static shape information, so set the shapes\n  # manually. This way the `tf.data.Datasets` are easier to inspect.\n  inputs.set_shape([None, self.input_width, None])\n  labels.set_shape([None, self.label_width, None])\n\n  return inputs, labels\n\nWindowGenerator.split_window = split_window\n\n\n\n\nCode\ndef make_dataset(self, data):\n  data = np.array(data, dtype=np.float32)\n  ds = tf.keras.utils.timeseries_dataset_from_array(\n      data=data,\n      targets=None,\n      sequence_length=self.total_window_size,\n      sequence_stride=1,\n      shuffle=False,\n      batch_size=32,) \n\n  ds = ds.map(self.split_window)\n\n  return ds\n\nWindowGenerator.make_dataset = make_dataset\n\n\n\n\nCode\n@property\ndef train(self):\n  return self.make_dataset(self.train_df)\n\n@property\ndef val(self):\n  return self.make_dataset(self.val_df)\n\n@property\ndef test(self):\n  return self.make_dataset(self.test_df)\n\n@property\ndef example(self):\n  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n  result = getattr(self, '_example', None)\n  if result is None:\n    # No example batch was found, so get one from the `.train` dataset\n    result = next(iter(self.train))\n    # And cache it for next time\n    self._example = result\n  return result\n\nWindowGenerator.train = train\nWindowGenerator.val = val\nWindowGenerator.test = test\nWindowGenerator.example = example\n\n\n\n\nCode\ndef plot(self, model=None, plot_col='Total', max_subplots=3):\n  inputs, labels = self.example\n  plt.figure(figsize=(12, 8))\n  plot_col_index = self.column_indices[plot_col]\n  max_n = min(max_subplots, len(inputs))\n  for n in range(max_n):\n    plt.subplot(max_n, 1, n+1)\n    plt.ylabel(f'{plot_col} [normed]')\n    plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n             label='Inputs', marker='.', zorder=-10)\n\n    if self.label_columns:\n      label_col_index = self.label_columns_indices.get(plot_col, None)\n    else:\n      label_col_index = plot_col_index\n\n    if label_col_index is None:\n      continue\n\n    plt.scatter(self.label_indices, labels[n, :, label_col_index],\n                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n    if model is not None:\n      predictions = model(inputs)\n      plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n                  marker='X', edgecolors='k', label='Predictions',\n                  c='#ff7f0e', s=64)\n\n    if n == 0:\n      plt.legend()\n\n  plt.xlabel('Time [h]')\n\nWindowGenerator.plot = plot\n\n\n\n\nCode\n# Definimos número de épocas necesarias y funciones de pérdida\nMAX_EPOCHS = 20\n\ndef compile_and_fit(model, window, patience=2): #patiences como el número de épocas que espera antes de parar\n  # Para evitar sobreajuste\n  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                                    patience=patience,\n                                                    mode='min')\n\n  model.compile(loss=tf.losses.MeanSquaredError(),\n                optimizer=tf.optimizers.Adam(),\n                metrics=[tf.metrics.MeanAbsoluteError()])\n\n  history = model.fit(window.train, epochs=MAX_EPOCHS,\n                      validation_data=window.val,\n                      callbacks=[early_stopping])\n  return history"
  },
  {
    "objectID": "red_exportaciones_con_2.html#configuración-del-modelo",
    "href": "red_exportaciones_con_2.html#configuración-del-modelo",
    "title": "Predicción 1 paso adelante usando 2 retardos",
    "section": "3 Configuración del modelo",
    "text": "3 Configuración del modelo\n\nw1 = WindowGenerator(input_width=2, label_width=1, shift=1,\n                     label_columns=['Total'])\nw1\n\n\n\nCode\nfor batch in w1.train.take(1):\n    inputs_train,targets_train = batch\n    \nprint(\"Input shape:\", inputs_train.numpy().shape)\nprint(\"Target shape:\", targets_train.numpy().shape)\n\n\nInput shape: (32, 2, 1)\nTarget shape: (32, 1, 1)\n\n\n\n\nCode\nfor batch in w1.val.take(1):\n    inputs_val,targets_val = batch\n\nprint(\"Input shape:\", inputs_val.numpy().shape)\nprint(\"Target shape:\", targets_val.numpy().shape)\n\n\n\n\nCode\nfor batch in w1.test.take(1):\n    inputs_test,targets_test = batch\n\nprint(\"Input shape:\", inputs_val.numpy().shape)\nprint(\"Target shape:\", targets_val.numpy().shape)\n\n\n\n\nCode\nw1.train.element_spec\n\n\n\n\nCode\nw1.plot()\n\n\n\n\n\n\n\n\n\n\n\nCode\n## Ejemplo de los lotes en los datos de entrenamiento\ni=1\nfor batch in w1.train.take(1):\n    inputs, targets = batch\n    print(\"Covariable o input\",i,inputs)\n    print(\"Respuesta o etiqueta\",i,targets)\n    i=i+1\n\n\n\n\nCode\n## Ejemplo de los lotes en los datos de validación\ni=1\nfor batch in w1.val.take(1):\n    inputs, targets = batch\n    print(\"Covariable o input\",i,inputs)\n    print(\"Respuesta o etiqueta\",i,targets)\n    i=i+1\n\n\n\n\nCode\n## Ejemplo de los lotes en los datos de prueba\ni=1\nfor batch in w1.test.take(1):\n    inputs, targets = batch\n    print(\"Covariable o input\",i,inputs)\n    print(\"Respuesta o etiqueta\",i,targets)\n    i=i+1\n\n\n\n\nCode\ninput_dataset_train = w1.train.map(lambda x,y: x)\ntarget_dataset_train = w1.train.map(lambda x,y: y)\n\n\n\n\nCode\ninput_dataset_val = w1.val.map(lambda x,y: x)\ntarget_dataset_val = w1.val.map(lambda x,y: y)\n\n\n\n\nCode\ninput_dataset_test = w1.test.map(lambda x,y: x)\ntarget_dataset_test = w1.test.map(lambda x,y: y)"
  },
  {
    "objectID": "red_exportaciones_con_2.html#búsqueda-de-los-hiperparámetros-e-implementación-del-modelo",
    "href": "red_exportaciones_con_2.html#búsqueda-de-los-hiperparámetros-e-implementación-del-modelo",
    "title": "Predicción 1 paso adelante usando 2 retardos",
    "section": "4 búsqueda de los hiperparámetros e implementación del modelo",
    "text": "4 búsqueda de los hiperparámetros e implementación del modelo\n\n\nCode\nfrom tensorflow import keras\nimport keras_tuner as kt\nfrom tensorflow.keras import layers\nfrom keras.layers import Dense, Activation, Flatten\n\n\n\n\nCode\ndef build_model(hp):\n    model = keras.Sequential()\n    model.add(layers.Dense(units=hp.Int(\"num_units\", min_value=32, max_value=564, step=32),\n                activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"])))\n    # Tune the number of layers.\n    for i in range(hp.Int(\"num_layers\", 1, 5)):\n        model.add(\n            layers.Dense(\n                # Tune number of units separately.\n                units=hp.Int(f\"units_{i}\", min_value=32, max_value=564, step=32),\n                activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"]),\n            )\n        )\n    if hp.Boolean(\"dropout\"):\n        model.add(layers.Dropout(rate=0.25))\n    model.add(Flatten())\n    model.add(layers.Dense(1, activation=\"linear\"))\n    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n        loss=\"mean_squared_error\",\n        metrics=[\"mean_squared_error\"]\n    )\n    return model\n\n\nbuild_model(kt.HyperParameters())\n\n\n\n\nCode\ntuner_BayesianOptimization_mlp = kt.BayesianOptimization(\n    hypermodel=build_model,\n    objective=\"val_loss\",\n    max_trials=50,\n    seed=12325,\n    overwrite=True,\n    directory=\"dirsalida\",\n    project_name=\"helloworld\"\n)\n\n\n\n\nCode\n# tuner_BayesianOptimization_mlp.search_space_summary()\n\n\n\n\nCode\nstop_early=tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",patience=0)\n\n\n\n\nCode\ntuner_BayesianOptimization_mlp.search((w1.train), epochs=20, validation_data=(w1.val),callbacks=[stop_early])\n\n\n\n\nCode\n# Get the top 2 models.\nmodels_mlp = tuner_BayesianOptimization_mlp.get_best_models(num_models=2)\nbest_model_mlp = models_mlp[0]\n# Build the model.\n# Needed for `Sequential` without specified `input_shape`.\nbest_model_mlp.build(input_shape=(32,2,1))\nbest_model_mlp.summary()\n\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               (32, 2, 256)              512       \n                                                                 \n dense_1 (Dense)             (32, 2, 32)               8224      \n                                                                 \n dense_2 (Dense)             (32, 2, 224)              7392      \n                                                                 \n flatten (Flatten)           (32, 448)                 0         \n                                                                 \n dense_3 (Dense)             (32, 1)                   449       \n                                                                 \n=================================================================\nTotal params: 16577 (64.75 KB)\nTrainable params: 16577 (64.75 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\nCode\ntuner_BayesianOptimization_mlp.results_summary()\n\n\n\n\nCode\ntrain_plus_val=w1.train.concatenate(w1.val)###verificar que en efecto\ntrain_plus_val\n\n\n\n\nCode\n# Get the top 2 hyperparameters.\nbest_hps_mlp = tuner_BayesianOptimization_mlp.get_best_hyperparameters(5)\n# Build the model with the best hp.\ncallback=tf.keras.callbacks.EarlyStopping(monitor=\"loss\",patience=5)\nmodel_mlp = build_model(best_hps_mlp[0])\n# Fit with the entire dataset.\nmodel_mlp.fit(train_plus_val, epochs=20,callbacks=[callback])\n\n\nUna vez re-entrenado el modelo con el conjunto de hiperparámetros hallado anteriormente, se obtiene el siguiente MSE en en conjunto de entrenamiento + validación:\n\n\nCode\nmodel_mlp.evaluate(train_plus_val, verbose=0)\n\n\n[0.05178812891244888, 0.05178812891244888]\n\n\n\ny el MSE sobre el conjunto de prueba:\n\n\nCode\nmodel_mlp.evaluate(w1.test, verbose=0)\n\n\n[0.11763034015893936, 0.11763034015893936]"
  },
  {
    "objectID": "red_exportaciones_con_2.html#predicción-sobre-el-conjunto-de-prueba",
    "href": "red_exportaciones_con_2.html#predicción-sobre-el-conjunto-de-prueba",
    "title": "Predicción 1 paso adelante usando 2 retardos",
    "section": "5 Predicción sobre el conjunto de prueba",
    "text": "5 Predicción sobre el conjunto de prueba\n\n\nCode\nprediction_test=(model_mlp.predict(w1.test, verbose=1)*train_std['Total']+train_mean['Total'])\nprint(prediction_test.shape)\n\n\n\n\nCode\ni=1\nfor batch in target_dataset_test.take(10):\n    if i==1:\n        targets_test = batch.numpy()\n    elif i&gt;1:\n        targets_test_aux = batch.numpy()\n        targets_test=np.append(targets_test,targets_test_aux)\n    i=i+1\n    \nprint(targets_test.shape)\n\n\n\n\nCode\ntrue_series=targets_test*train_std['Total']+train_mean['Total']\ntrue_series=true_series.reshape((27,1,1))\nprint(true_series.shape)\n\n\nUna vez que se hacen las predicciones sobre el conjunto de prueba, se hace la comparación con los valores reales y se obtiene el siguiente RECM (en la escala original):\n\n\nCode\nerrors_squared=tf.keras.metrics.mean_squared_error(true_series, prediction_test).numpy()\nprint(\"RECM:\",errors_squared.mean()**0.5)\n\n\nRECM: 890501.7311875367\n\n\n\n\nCode\ntest_index=test_df.index[:27]\ntrue_series_final=true_series.reshape(27)\nprediction_test_final=prediction_test.reshape(27)\n\n\n\n\nCode\nplt.plot(true_series_final)\nplt.plot(prediction_test_final)\nplt.legend(['Respesta real','Predicción de la Respuesta'],loc='lower right', fontsize=15)\nplt.ylabel('Y y $\\hat{Y}$ en conjunto de prueba', fontsize=15)\nplt.title('Red neuronal recurrente: Predicciones sobre el conjunto de prueba', fontsize=20)\n\n\nText(0.5, 1.0, 'Red neuronal recurrente: Predicciones sobre el conjunto de prueba')\n\n\n\n\n\n\n\n\n\n\n\nCode\npredicciones_prueba = model_mlp.predict(w1.test)*train_std['Total']+train_mean['Total']\ntrain_val_predict = model_mlp.predict(train_plus_val)*train_std['Total']+train_mean['Total']\n\n\n\n\nCode\nplt.figure(figsize=(20,10))\nplt.title('Red neuronal multicapa para la serie de tiempo de exportaciones', fontsize=20)\nplt.xlabel('Fecha', fontsize=18)\nplt.ylabel('Total de exportaciones', fontsize=18)\nplt.plot(Exportaciones['Mes'], Exportaciones['Total'])\nplt.plot(Exportaciones['Mes'][2:196], train_val_predict[1:195])\nplt.plot(Exportaciones['Mes'][198:251], train_val_predict[196:])\nplt.plot(Exportaciones['Mes'][254:281], predicciones_prueba)\nplt.legend(['Serie original', 'Predicciones en entrenamiento + validación', 'Predicciones en prueba'], loc='lower right',\n          fontsize=15)\nplt.show()"
  },
  {
    "objectID": "red_exportaciones_con_2.html#errores-de-predicción-del-modelo",
    "href": "red_exportaciones_con_2.html#errores-de-predicción-del-modelo",
    "title": "Predicción 1 paso adelante usando 2 retardos",
    "section": "6 Errores de Predicción del Modelo",
    "text": "6 Errores de Predicción del Modelo\n\n\n6.1 Sobre el conjunto de entrenamiento\n\n\nCode\nlabels_train = np.concatenate([y for x, y in w1.train], axis=0)\nlabels_train.shape\n\n\n\n\nCode\nlista=list(w1.train.unbatch().map(lambda x, y: (x, y)))\n\n\n\n\nCode\nprediccion_intra_muestra=model_mlp.predict(w1.train, verbose=1)\nprediccion_intra_muestra=prediccion_intra_muestra.reshape(195,1,1)\n\n\n\n\nCode\neror_prediction_train=labels_train-prediccion_intra_muestra\n\n\n\n\nCode\nx_vals = train_df.index[1:]\n\n\n\n\nCode\nprint(eror_prediction_train.shape)\nprint(x_vals.shape)\n\n\n\n\nCode\neror_prediction_train=eror_prediction_train.reshape(eror_prediction_train.shape[0])\neror_prediction_train.shape\n\n\n\n\nCode\nfig = plt.figure(figsize=(15,8))\nplt.plot(eror_prediction_train)\nplt.ylabel('$Y-\\hat{Y}$', fontsize=14)\nplt.title('Error de predicción sobre los datos de entrenamiento', fontsize=16);\n\n\n\n\n\n\n\n\n\n\n\nCode\ngraficapacf=plot_pacf(eror_prediction_train,lags=50,method='ldbiased') ###Se puede usar también em method='ywmle'\ngraficaacf=plot_acf(eror_prediction_train,lags=50,adjusted='ldbiased')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.2 Sobre el conjunto de prueba\n\n\nCode\nlabels_test = np.concatenate([y for x, y in w1.test], axis=0)\nlabels_test.shape\n\n\n\n\nCode\nprediccion_conjunto_test=model_mlp.predict(w1.test, verbose=1)\nprediccion_conjunto_test=prediccion_conjunto_test.reshape(27,1,1)\neror_prediction_test=labels_test-prediccion_conjunto_test\neror_prediction_test=eror_prediction_test.reshape(eror_prediction_test.shape[0])\n\n\n\n\nCode\nfig1 = plt.figure(figsize=(15,8))\nplt.plot(eror_prediction_test)\nplt.ylabel('$Y-\\hat{Y}$', fontsize=14)\nplt.title('Error de predicción sobre los datos de prueba', fontsize=16)\n\n\nText(0.5, 1.0, 'Error de predicción sobre los datos de prueba')\n\n\n\n\n\n\n\n\n\n\n\nCode\ngraficapacf=plot_pacf(eror_prediction_test,lags=12,method='ldbiased') ###Se puede usar también em method='ywmle'\ngraficaacf=plot_acf(eror_prediction_test,lags=12,adjusted='ldbiased')"
  }
]