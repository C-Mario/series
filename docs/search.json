[
  {
    "objectID": "DBitcoin.html",
    "href": "DBitcoin.html",
    "title": "Descriptiva-josé",
    "section": "",
    "text": "#datos\nBTC_Daily &lt;- read.csv(\"datos/BTC-Daily.csv\")\nData &lt;- data.frame(BTC_Daily$date,BTC_Daily$close)\nData &lt;- data.frame(BTC_Daily$date,BTC_Daily$close)\ncolnames(Data) &lt;- c(\"FechaTiempo\", \"Valor\")\n# limpiando datos faltantes\nstr(Data)\n\n'data.frame':   2651 obs. of  2 variables:\n $ FechaTiempo: chr  \"2022-03-01 00:00:00\" \"2022-02-28 00:00:00\" \"2022-02-27 00:00:00\" \"2022-02-26 00:00:00\" ...\n $ Valor      : num  43185 43179 37713 39147 39232 ...\n\nData$FechaTiempo &lt;- strftime(Data$FechaTiempo, format=\"%Y-%m-%d\")\nstr(Data)\n\n'data.frame':   2651 obs. of  2 variables:\n $ FechaTiempo: chr  \"2022-03-01\" \"2022-02-28\" \"2022-02-27\" \"2022-02-26\" ...\n $ Valor      : num  43185 43179 37713 39147 39232 ...\n\nData$FechaTiempo &lt;- as.Date(Data$FechaTiempo)\n# procesamiento de los datos\n\nBitcoin &lt;- Data %&gt;%\n      filter(FechaTiempo &gt;= as.Date(\"2017-01-01\"),\n             FechaTiempo &lt;= as.Date(\"2021-12-31\"))\nstr(Bitcoin)\n\n'data.frame':   1826 obs. of  2 variables:\n $ FechaTiempo: Date, format: \"2021-12-31\" \"2021-12-30\" ...\n $ Valor      : num  46214 47151 46483 47543 50718 ...\n\n\n\n\n\n# objeto serie de tiempo\nData_xts &lt;- xts::xts(Bitcoin$Valor, order.by = Bitcoin$FechaTiempo)\nhead(Data_xts)\n\n              [,1]\n2017-01-01  998.80\n2017-01-02 1014.10\n2017-01-03 1036.99\n2017-01-04 1122.56\n2017-01-05  994.02\n2017-01-06  891.56\n\nTSstudio::ts_info(Data_xts)\n\n The Data_xts series is a xts object with 1 variable and 1826 observations\n Frequency: daily \n Start time: 2017-01-01 \n End time: 2021-12-31 \n\n#class(Data_xts)\n#frequency(Data_xts)\n#xts::periodicity(Data_xts)\n#xts::tclass(Data_xts)\n#plot(Data_xts)\n\nVista de los datos para inspección visual\n\nTSstudio::ts_plot(Data_xts,\n         title = \"Valor de cierre bitcoin en bolsa\",\n         Ytitle = \"Valor en dolares\",\n          Xtitle = \"Fecha\",\n          Xgrid = TRUE,\n       Ygrid = TRUE)\n\n\n\n\n\n\nVarianza marginal: Se notan periodos donde el rango de valores que puede tomar la variable se va fluctuando a medida que pasa el tiempo.\nComponente Estacional: No se evidencia un comportamiento cíclico en la serie.\nTendencia: Se muestra la serie no oscila sobre un valor fijo y tiene cambios abruptos de crecimiento y decrecimiento en algunos momentos.\n\n\n\n\nUsaremos la transformación de Box-Cox para estabilizar la varianza; primero miramos el lambda\n\n#Valor de lambda\nforecast::BoxCox.lambda(Data_xts, method =\"loglik\", lower = -1, upper = 3)\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\n[1] 0\n\nMASS::boxcox(lm(Data_xts ~ 1),seq(-1/2, 1/2, length = 50))\n\n\n\n\nVemos que se sugiere el valor \\(\\lambda = 0\\) lo cual dada de transformación de Box-Cox se usa la función logaritmo natural para la estabilización de la variabilidad así tenemos que:\n\n#trasnformación\nlData_xts &lt;- log(Data_xts)\n#plot(lData_xts)\n\n\nTSstudio::ts_plot(lData_xts,\n          title = \"Valor de Serie Trasnformada\",\n           Ytitle = \"Valor de la trasnformación\",\n          Xtitle = \"Fecha\",\n          Xgrid = TRUE,\n           Ygrid = TRUE)\n\n\n\n\n\nAhora miramos si es necesario aplicar otra transformación a la serie\n\n#Valor de lambda\n(forecast::BoxCox.lambda(lData_xts, method =\"loglik\", lower = -1, upper = 3))\n\n[1] 0.9\n\nMASS::boxcox(lm(lData_xts ~ 1),seq(-1, 2, length = 50))\n\n\n\n\nVemos que la sugerencia es \\(\\lambda = 0.9\\) lo cual es cercano a \\(1\\), además el IC de confianza captura al \\(1\\), por ende la transformación logarítmica parece haber estabilizado la varianza.\n\n#Gráfico de ellas juntas\npar(mfrow=c(2,1))\nplot(Data_xts, main = \"Series original\")\nplot(lData_xts, main = \"Series transformada\")\n\n\n\n\nSe puede ver cómo la transformación aplicada logra estabilizar la varianza en gran medida.\n\n\n\nTrabajaremos con la serie a la cuál se le realizo la transformación para estabilizar la varianza, realizaremos el gráfico de los valores de la función de auto-correlación\n\n#ACf\nacf(lData_xts, 180, main = \"Serie Bitcoin Trasnformada\")\n\n\n\n\nNotamos que los valores van teniendo un decaimiento leve lo cual nos da un indicio más claro de que existe tendencia en la serie, analizaremos el gráfico de retardos de la serie trasnformada para ver si podemos tener indicios de una relación no-lineal o lineal en la serie.\n\n#serie transformada\npar(mar = c(3,2,3,2))\nastsa::lag1.plot(lData_xts, 16,corr=T)\n\n\n\n\nVemos que se nota un fuerte relación linea hasta para el retraso número 16, por lo tanto con lo mostrado por el acf y el gráfico de retardos tenemos indicios fuertes de tendencia en la serie así usaremos los métodos: lineal determinista, Descomposición de promedios móviles y descomposición STL para estimar dicha componente.\n\n\najustaremos el modelo eliminaremos la tendencia y analizaremos los resultados\n\n#pasar a ts para Graficarlo\nldata_ts &lt;- TSstudio::xts_to_ts(lData_xts,frequency = 365,\n                                start = as.Date(\"2017-01-01\"))\n#modelo lineal\nsummary(fit &lt;- lm(ldata_ts~time(ldata_ts), na.action=NULL))\n\n\nCall:\nlm(formula = ldata_ts ~ time(ldata_ts), na.action = NULL)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.06366 -0.49265  0.01902  0.36341  1.68667 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.230e+03  1.780e+01  -69.10   &lt;2e-16 ***\ntime(ldata_ts)  6.135e-01  8.813e-03   69.61   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5439 on 1824 degrees of freedom\nMultiple R-squared:  0.7265,    Adjusted R-squared:  0.7264 \nF-statistic:  4846 on 1 and 1824 DF,  p-value: &lt; 2.2e-16\n\n# Gráfico\nplot(ldata_ts, ylab= \"Valor en escala logarítmica\")\nabline(fit,col = \"red\")# Se añade la recta ajusta\n\n\n\n\nahora eliminaremos la tendencia de la serie\n\n###Eliminamos la tendencia con la predicción la recta\nElimTenldata_ts &lt;- ldata_ts - predict(fit)\nplot(ElimTenldata_ts, main=\"Serie Sin tendencia\", \n     ylab= \"Valor en escala logarítmica\")\n\n\n\n\nObservamos que en la serie obtenida después de eliminar la tendencia lineal parece tener un comportamiento de alta variabilidad similar una caminata aleatoria.\n\nacf(ElimTenldata_ts,lag.max =length(ElimTenldata_ts), \n    main=\"Serie Sin tendencia\")\n\n\n\n# Series trasnfromada sin tendencia Retardos\npar(mar = c(3,2,3,2))\nastsa::lag1.plot(ElimTenldata_ts, 16,corr=F)\n\n\n\n\nNotamos que en la gráfica del acf se sigue teniendo un decaimiento lento de los valores de la función de auto-correlación para los primeros rezagos, además en el gráfico de retardos se sigue manteniendo una alta relación lineal entre el valor actual y sus regazos. Por ende todo esto nos da los argumentos necesarios para descartar la estimación linealcómo una buena estimación de la tendencia.\n\n\n\n\n# descomposición de promedios moviles\ndescom_ldata &lt;- decompose(ldata_ts)\nplot(descom_ldata)\n\n\n\n\nPodemos observar que usando un filtro de promedio móvil la tendencia estimada no se aproxima mucho a una lineal, cómo se puede apreciar en el siguiente gráfico; además la componente estacional no parece ser estimada de buena manera ya que no se ve un patrón de comportamiento claramente, además el residual presenta un comportamiento no estacionario aparentemente.\n\n# Gráfico\nplot(ldata_ts, ylab= \"Valor en escala logarítmica\")\nabline(fit,col = \"red\")# Se añade la recta ajusta\npoints(time(ldata_ts), descom_ldata$trend, col =\"green\", cex=0.3)\n\n\n\n\nEliminaremos la tendencia del promedio móvil centrado y de la frecuencia\n\n###Eliminamos la tendencia con la predicción promedio movil\nElimTenldata_ts &lt;- ldata_ts - descom_ldata$trend\nplot(ElimTenldata_ts, main=\"Serie Sin tendencia\", \n     ylab= \"Valor en escala logarítmica\")\n\n\n\n\nPodemos ver que la serie cómo en el caso lineal parece mostrar un comportamiento de caminata aleatoria.\n\nacf(ElimTenldata_ts[183:1644],lag.max = 730, \n    main=\"Serie Sin tendencia\")\n\n\n\n# Series trasnfromada sin tendencia Retardos\npar(mar = c(3,2,3,2))\nastsa::lag1.plot(ElimTenldata_ts[183:1644], 16,corr=F)\n\n\n\n\nNotamos que tanto en el acf cómo en la gráfica de retardos se ve un comportamiento similar al anterior lo cuál nos hace descartar la descompsición de promedios moviles para la estimación de la tendencia.\nFiltro promedio móvil con solo retrasos\nIntentaremos ajustar un promedio móvil que tenga en cuenta solo los retrasos y sea de los periodos de un año, seis meses, tres meses y mes.\n\n#gráfico\nfilter_1=stats::filter(ldata_ts, filter = rep(1/365, 365), sides = 1)\nfilter_2=stats::filter(ldata_ts, filter = rep(1/182, 182), sides = 1)\nfilter_3=stats::filter(ldata_ts, filter = rep(1/90, 90), sides = 1)\nfilter_4=stats::filter(ldata_ts, filter = rep(1/30, 30), sides = 1)\nplot(ldata_ts, ylab= \"Valor en escala logarítmica\")\npoints(time(ldata_ts), filter_1, col =\"green\", cex=0.33)\npoints(time(ldata_ts), filter_2, col =\"blue\", cex=0.33)\npoints(time(ldata_ts), filter_3, col =\"red\", cex=0.35)\npoints(time(ldata_ts), filter_4, col =\"cyan\", cex=0.31)\n\n\n\n#legend(locator(1), c(\"365 días\",\"182 días\",\"90 días\",\"30 días\"), col=c(\"green\",\"blue\",\"red\",\"cyan\"),lty=c(1,1,1,1),lwd=c(2,2,2,2))\n\nNotamos que para 3 meses y 6 meses los filtros de promedios móviles muestra una mejor estimación, por ende tomaremos para 3 meses cómo estimación de la tendencia de la serie\n\n###Eliminamos la tendencia con la predicción promedio movil\nElimTenldata_ts &lt;- ldata_ts - filter_3\nplot(ElimTenldata_ts, main=\"Serie Sin tendencia\", \n     ylab= \"Valor en escala logarítmica\")\n\n\n\n#\nacf(ElimTenldata_ts[90:1826],lag.max =1095, \n    main=\"Serie Sin tendencia\")\n\n\n\n# Series trasnfromada sin tendencia Retardos\npar(mar = c(3,2,3,2))\nastsa::lag1.plot(ElimTenldata_ts[90:1826], 16,corr=F)\n\n\n\n\nEl comportamiento de los gráficos es similar a los anteriores con eso tenemos indicios de que la estimación de la tendencia de manera determinista potencialmente no es buena idea.\n\n\n\nUsando la descomposición STL obtenemos la estimación de la tendencia\n\nindice_ldata &lt;- sort(Bitcoin$FechaTiempo)\n#  as.Date(as.yearmon(tk_index(ldata_ts)))\n## Otra forma de extraer el indice estimetk::tk_index(lAirPass)\nlogdata &lt;- as.matrix(ldata_ts)\ndf_ldata &lt;- data.frame(Fecha=indice_ldata,logdata=as.matrix(ldata_ts))\nstr(df_ldata)\n\n'data.frame':   1826 obs. of  2 variables:\n $ Fecha   : Date, format: \"2017-01-01\" \"2017-01-02\" ...\n $ Series.1: num  6.91 6.92 6.94 7.02 6.9 ...\n\ncolnames(df_ldata) &lt;- c(\"Fecha\", \"logdata\")\nstr(df_ldata)\n\n'data.frame':   1826 obs. of  2 variables:\n $ Fecha  : Date, format: \"2017-01-01\" \"2017-01-02\" ...\n $ logdata: num  6.91 6.92 6.94 7.02 6.9 ...\n\ntibble_ldata &lt;- tibble(df_ldata)\n####Primera aproximación del ajuste STL\ntibble_ldata%&gt;%timetk::plot_time_series(Fecha, logdata, \n                   .interactive = TRUE,\n                   .plotly_slider = TRUE)\n\n\n\n\n\n\n#####Ajuste STL\n#Note que obtenemos un objeto adicional en tibble_logpasajeros con Logpasa_ajus con parámetros que se pueden mover.\nlogdata_ajus &lt;- smooth_vec(logdata,span = 0.75, degree = 2)\ntibble_ldata%&gt;%dplyr::mutate(logdata_ajus)\n\n# A tibble: 1,826 × 3\n   Fecha      logdata logdata_ajus\n   &lt;date&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n 1 2017-01-01    6.91         6.90\n 2 2017-01-02    6.92         6.91\n 3 2017-01-03    6.94         6.91\n 4 2017-01-04    7.02         6.92\n 5 2017-01-05    6.90         6.92\n 6 2017-01-06    6.79         6.93\n 7 2017-01-07    6.80         6.94\n 8 2017-01-08    6.81         6.94\n 9 2017-01-09    6.80         6.95\n10 2017-01-10    6.81         6.96\n# ℹ 1,816 more rows\n\n###Ajuste STL moviendo los parámetros\ntibble_ldata%&gt;%mutate(logdata_ajus=smooth_vec(logdata,span = 0.75, degree = 2))%&gt;%\n  ggplot(aes(Fecha, logdata)) +\n    geom_line() +\n    geom_line(aes(y = logdata_ajus), color = \"red\")\n\n\n\n\nSe puede evidenciar que la Estimación de la tendencia via STL parece mejorar aspectos que la descomposición movil intentada con información de un año no se tenia.\n\n###Eliminamos la tendencia con la predicción la STL\nElimTenldata_xts &lt;- lData_xts - logdata_ajus\nplot(ElimTenldata_xts, main=\"Serie Sin tendencia\", \n     ylab= \"Valor en escala logarítmica\")\n\n\n\nacf(ElimTenldata_xts,lag.max =1094, \n    main=\"Serie Sin tendencia\")\n\n\n\n# Series trasnfromada sin tendencia Retardos\npar(mar = c(3,2,3,2))\nastsa::lag1.plot(ElimTenldata_xts, 16,corr=F)\n\n\n\n\nVemos que a diferencia de los promedio móviles si tenemos una estimación para todos los valores de la serie, además notamos que la acf y el gráfico de retardos tiene un comportamiento similar a los métodos anteriores.\n\n\n\n\n\n###Diferenciando con base en el objeto ts\ndldata&lt;-diff(ldata_ts)\n#plot(dldata)\n#abline(h=0, col = \"red\")\n#acf(dldata,lag.max =90, main=\"Serie Diferenciada\")\n\n\n###Diferenciando con base en el objeto xts\ndldata_xts&lt;-diff(lData_xts)\ndldata_xts &lt;- dldata_xts[-1]\nplot(dldata_xts, main = \"Serie diferenciada\")\n\n\n\n\nVemos que la serie al ser diferenciada muestra un comportamiento estacionario pues los valores oscilan sobre un valor fijo, además de un valor que muestra un comportamiento extremo pues varia demasiado con respecto a los demás.\n\n# función de autocorrelación\nacf(dldata_xts,lag.max =30, main=\"Serie Diferenciada\")\n\n\n\n# Series trasnfromada sin tendencia Retardos\npar(mar = c(3,2,3,2))\nastsa::lag1.plot(dldata_xts, 16,corr=T)\n\n\n\n\nEl gráfico de acf muestra que la tendencia parce a ver desparecido, además no parece destacar ningún valor para algún retraso de manera clara. Para el gráfico de retardos vemos claramente que ya no hay una relación lineal ni no lineal del valor actual con sus retardos.\nTomando en cuenta todo lo anterior trabajaremos con la serie aplicada la transformación de Box-Cox sugerida y diferenciada, además se tiene sospecha de que la serie presenta comportamiento de una camina aleatoria con tendencia no determinista.\n\n\n\n\n\n\n#Serie diferenciada\nTSstudio::ts_heatmap(dldata_xts, title = \"Mapa de calor - Cierre Bitcoin en bolsa dias año\")\n\n\n\n\n\nPara la serie diferenciada no se evidencia ninguno tipo de patrón lo cual nos da indicios de que no se tiene un componente estacional.\n\n\n\n\n# periodograma\nPeriodograma &lt;- spectrum(as.numeric(dldata_xts),log=\"no\",span=c(5,5))\n\n\n\n#\nubicacionDif &lt;- which.max(Periodograma$spec)\nsprintf(\"El valor de la frecuencia donde se máximiza el periodograma para la serie es: %s\",Periodograma$freq[ubicacionDif])\n\n[1] \"El valor de la frecuencia donde se máximiza el periodograma para la serie es: 0.4016\"\n\n#\nsprintf(\"El periodo correspondiente es aproximadamente: %s\",1/Periodograma$freq[ubicacionDif])\n\n[1] \"El periodo correspondiente es aproximadamente: 2.49003984063745\"\n\n\nPara la serie diferenciada el periodograma no es claro, a pesar del suviazamiento usado la curva sigue mostrando varios picos en su recorrido el máximo lo encontramos de tal manera que \\(\\omega = \\frac{251}{625}=0.416\\) lo cual se corresponde con un \\(s \\approx 2.5\\) .\n\n# intentando sacar el segundo más alto\nn_dld &lt;- length(Periodograma$spec)\nvalor_seg &lt;- sort(Periodograma$spec,partial=n_dld-1)[n_dld-1]\nubica_segundo &lt;- which(Periodograma$spec==valor_seg)\n\nsprintf(\"El valor de la frecuencia donde se alcanza el segundo máximo para el periodograma para REC es: %s\",Periodograma$freq[ubica_segundo])\n\n[1] \"El valor de la frecuencia donde se alcanza el segundo máximo para el periodograma para REC es: 0.401066666666667\"\n\n#\nsprintf(\"El periodo correspondiente es aproximadamente: %s\",1/Periodograma$freq[ubica_segundo])\n\n[1] \"El periodo correspondiente es aproximadamente: 2.49335106382979\"\n\n\nvemos que el segundo valor es bastante parecido al segundo.\n\n# valor de frecuencia \ntail(sort(Periodograma$spec))\n\n[1] 0.004895118 0.004914000 0.005095353 0.005470611 0.005752230 0.005966273\n\n\nCómo se puede observar los primeros seis valores son bastante cercanos entre ellos, por lo tanto sus valores de periodo serán similiares.\nCon esto descartamos la estimación de una componente estacional pues no tenemos evidencia clara de su existencia."
  },
  {
    "objectID": "DBitcoin.html#serie-diferenciada",
    "href": "DBitcoin.html#serie-diferenciada",
    "title": "Descriptiva-josé",
    "section": "",
    "text": "###Diferenciando con base en el objeto ts\ndldata&lt;-diff(ldata_ts)\n#plot(dldata)\n#abline(h=0, col = \"red\")\n#acf(dldata,lag.max =90, main=\"Serie Diferenciada\")\n\n\n###Diferenciando con base en el objeto xts\ndldata_xts&lt;-diff(lData_xts)\ndldata_xts &lt;- dldata_xts[-1]\nplot(dldata_xts, main = \"Serie diferenciada\")\n\n\n\n\nVemos que la serie al ser diferenciada muestra un comportamiento estacionario pues los valores oscilan sobre un valor fijo, además de un valor que muestra un comportamiento extremo pues varia demasiado con respecto a los demás.\n\n# función de autocorrelación\nacf(dldata_xts,lag.max =30, main=\"Serie Diferenciada\")\n\n\n\n# Series trasnfromada sin tendencia Retardos\npar(mar = c(3,2,3,2))\nastsa::lag1.plot(dldata_xts, 16,corr=T)\n\n\n\n\nEl gráfico de acf muestra que la tendencia parce a ver desparecido, además no parece destacar ningún valor para algún retraso de manera clara. Para el gráfico de retardos vemos claramente que ya no hay una relación lineal ni no lineal del valor actual con sus retardos.\nTomando en cuenta todo lo anterior trabajaremos con la serie aplicada la transformación de Box-Cox sugerida y diferenciada, además se tiene sospecha de que la serie presenta comportamiento de una camina aleatoria con tendencia no determinista."
  },
  {
    "objectID": "DBitcoin.html#análisis-de-estacionalidad",
    "href": "DBitcoin.html#análisis-de-estacionalidad",
    "title": "Descriptiva-josé",
    "section": "",
    "text": "#Serie diferenciada\nTSstudio::ts_heatmap(dldata_xts, title = \"Mapa de calor - Cierre Bitcoin en bolsa dias año\")\n\n\n\n\n\nPara la serie diferenciada no se evidencia ninguno tipo de patrón lo cual nos da indicios de que no se tiene un componente estacional.\n\n\n\n\n# periodograma\nPeriodograma &lt;- spectrum(as.numeric(dldata_xts),log=\"no\",span=c(5,5))\n\n\n\n#\nubicacionDif &lt;- which.max(Periodograma$spec)\nsprintf(\"El valor de la frecuencia donde se máximiza el periodograma para la serie es: %s\",Periodograma$freq[ubicacionDif])\n\n[1] \"El valor de la frecuencia donde se máximiza el periodograma para la serie es: 0.4016\"\n\n#\nsprintf(\"El periodo correspondiente es aproximadamente: %s\",1/Periodograma$freq[ubicacionDif])\n\n[1] \"El periodo correspondiente es aproximadamente: 2.49003984063745\"\n\n\nPara la serie diferenciada el periodograma no es claro, a pesar del suviazamiento usado la curva sigue mostrando varios picos en su recorrido el máximo lo encontramos de tal manera que \\(\\omega = \\frac{251}{625}=0.416\\) lo cual se corresponde con un \\(s \\approx 2.5\\) .\n\n# intentando sacar el segundo más alto\nn_dld &lt;- length(Periodograma$spec)\nvalor_seg &lt;- sort(Periodograma$spec,partial=n_dld-1)[n_dld-1]\nubica_segundo &lt;- which(Periodograma$spec==valor_seg)\n\nsprintf(\"El valor de la frecuencia donde se alcanza el segundo máximo para el periodograma para REC es: %s\",Periodograma$freq[ubica_segundo])\n\n[1] \"El valor de la frecuencia donde se alcanza el segundo máximo para el periodograma para REC es: 0.401066666666667\"\n\n#\nsprintf(\"El periodo correspondiente es aproximadamente: %s\",1/Periodograma$freq[ubica_segundo])\n\n[1] \"El periodo correspondiente es aproximadamente: 2.49335106382979\"\n\n\nvemos que el segundo valor es bastante parecido al segundo.\n\n# valor de frecuencia \ntail(sort(Periodograma$spec))\n\n[1] 0.004895118 0.004914000 0.005095353 0.005470611 0.005752230 0.005966273\n\n\nCómo se puede observar los primeros seis valores son bastante cercanos entre ellos, por lo tanto sus valores de periodo serán similiares.\nCon esto descartamos la estimación de una componente estacional pues no tenemos evidencia clara de su existencia."
  },
  {
    "objectID": "Descriptiva.html#serie-valor-de-cierre-en-bolsa-diario-del-bitcoin-2017-2021",
    "href": "Descriptiva.html#serie-valor-de-cierre-en-bolsa-diario-del-bitcoin-2017-2021",
    "title": "Descriptiva",
    "section": "2 Serie valor de cierre en bolsa diario del bitcoin (2017-2021)",
    "text": "2 Serie valor de cierre en bolsa diario del bitcoin (2017-2021)"
  },
  {
    "objectID": "descriptivoMario.html",
    "href": "descriptivoMario.html",
    "title": "Análisis descriptivo de las series de tiempo",
    "section": "",
    "text": "Esta serie de tiempo mide el total de las exportaciones colombianas mensualmente desde Enero del año 2000 hasta Junio del 2023. Los valores del total de exportaciones se miden en miles de dólares (FOB, Free onboard). Esta serie de tiempo incluye las exportaciones tradicionales (Café, Carbón, Petróleo y sus derivados, Ferroníquel) y las no tradicionales en conjunto.\n\n\nEl gráfico de la serie de tiempo del total de exportaciones de colombia se muestra a continuación:\n\n\n\n\n\n\nA simple vista, se puede evidenciar que:\n\nLa serie presenta tendencia, pues los valores de la serie no oscilan al rededor de un valor fijo. El dinero total proveniente de las exportaciones es cambiante a lo largo del tiempo observado.\nLa serie presenta varianza marginal no constante, pues en ciertos periodos de tiempo los cambios mes a mes son pequeños, mientras que en otros periodos de tiempo esos cambios son significativamente mucho más grandes.\nNo se ve claramente la existencia de una componente estacional.\n\n\n\n\nDado que visualmente se observó que la serie presenta varianza marginal no constante, se usará la transformación de Box-Cox para obtener la serie estabilizada.\n\n(lambda &lt;- forecast::BoxCox.lambda(Exportaciones_ts, method =\"loglik\", lower = -2, upper = 3))\n\n[1] 0.2\n\n\nDe esta manera se sugiere un valor de \\(\\lambda =\\) 0.2. El gráfico obtenido de la serie estabilizada es el siguiente:\n\n\n\n\n\n\n\n\n\nNo se ve un cambio significativo respecto a la serie original. Además, la librería MASS sugiere un valor diferente de \\(\\lambda\\), como se muestra a continuación:\n\nlambda_loglik &lt;- MASS::boxcox(lm(Exportaciones_ts ~ 1),seq(-2, 3, length = 100))\n\n\n\n\n\n\n\n\n\nmaximo &lt;- which.max(lambda_loglik$y)\n(lambda_loglik &lt;- lambda_loglik$x[maximo])\n\n[1] 0.6262626\n\n\nSe procede a realizar la transformación de la serie usando \\(\\lambda =\\) 0.6262626, obteniendo el siguiente gráfico para la serie estabilizada:\n\n\n\n\n\n\n\n\n\nAunque visualmente no se ven cambios importantes en cuanto a la estabilización de la varianza, al aplicar nuevamente la transformación de Box-Cox a la serie estabilizada, se obtiene el siguiente resultado:\n\nlambda_loglik &lt;- MASS::boxcox(lm(serie_estabilizada062 ~ 1),seq(-2, 3, length = 100))  \n\n\n\n\n\n\n\n\nEl intervalo de confianza para \\(\\lambda\\) incluye al 1, indicando que ya no es necesario aplicar nuevamente una transformación.\n\nmaximo &lt;- which.max(lambda_loglik$y)\n(lambda_loglik &lt;- lambda_loglik$x[maximo])\n\n[1] 1.030303\n\n\nExactamente, se sugiere un valor de \\(\\lambda =\\) 1.030303 \\(\\approx 1\\). Se trabajará desde ahora con la serie estabilizada.\n\nserie &lt;- serie_estabilizada062\n\n\n\n\n\n\n\n\n     Total    Fecha\n1 9202.399 Jan 2000\n2 9442.243 Feb 2000\n3 9439.146 Mar 2000\n4 8470.853 Apr 2000\n5 9951.207 May 2000\n6 9993.015 Jun 2000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA partir de la serie sin tendencia, después de haberla estimado por el método STL, se hace el gráfico de subseries (por meses) para ver si se evidencia la existencia de algún tipo de estacionalidad.\nEl gráfico de subseries muestra diferencias en las medias de varios meses. Especialmente los meses de enero y febrero, presentan una un valor medio más bajo que el resto de las meses en el total de las exportaciones.\nAquí no se ve muy claro un patrón en las subseries anuales, pero se ve que en la mayoría de estas, los meses de enero y febrero tienen generalmente un valor menor que en el resto de los meses, aunque no siempre es así. En general no se ve de forma claro algún patrón que se repita a lo largo del tiempo.\nLos gráficos de dispersión de los retardos con el valor actual se presentan a continuación:\nNo se ve que exista una relación lineal con ninguno de los retardos, indicando posiblemente la no existencia de una componente estacional en la serie."
  },
  {
    "objectID": "descriptivoMario.html#exportaciones",
    "href": "descriptivoMario.html#exportaciones",
    "title": "Análisis descriptivo de las series de tiempo",
    "section": "",
    "text": "Esta serie de tiempo mide el total de las exportaciones colombianas mensualmente desde Enero del año 2000 hasta Junio del 2023. Los valores del total de exportaciones se miden en miles de dólares (FOB, Free onboard). Esta serie de tiempo incluye las exportaciones tradicionales (Café, Carbón, Petróleo y sus derivados, Ferroníquel) y las no tradicionales en conjunto.\n\n\nEl gráfico de la serie de tiempo del total de exportaciones de colombia se muestra a continuación:\n\n\n\n\n\n\nA simple vista, se puede evidenciar que:\n\nLa serie presenta tendencia, pues los valores de la serie no oscilan al rededor de un valor fijo. El dinero total proveniente de las exportaciones es cambiante a lo largo del tiempo observado.\nLa serie presenta varianza marginal no constante, pues en ciertos periodos de tiempo los cambios mes a mes son pequeños, mientras que en otros periodos de tiempo esos cambios son significativamente mucho más grandes.\nNo se ve claramente la existencia de una componente estacional.\n\n\n\n\nDado que visualmente se observó que la serie presenta varianza marginal no constante, se usará la transformación de Box-Cox para obtener la serie estabilizada.\n\n(lambda &lt;- forecast::BoxCox.lambda(Exportaciones_ts, method =\"loglik\", lower = -2, upper = 3))\n\n[1] 0.2\n\n\nDe esta manera se sugiere un valor de \\(\\lambda =\\) 0.2. El gráfico obtenido de la serie estabilizada es el siguiente:\n\n\n\n\n\n\n\n\n\nNo se ve un cambio significativo respecto a la serie original. Además, la librería MASS sugiere un valor diferente de \\(\\lambda\\), como se muestra a continuación:\n\nlambda_loglik &lt;- MASS::boxcox(lm(Exportaciones_ts ~ 1),seq(-2, 3, length = 100))\n\n\n\n\n\n\n\n\n\nmaximo &lt;- which.max(lambda_loglik$y)\n(lambda_loglik &lt;- lambda_loglik$x[maximo])\n\n[1] 0.6262626\n\n\nSe procede a realizar la transformación de la serie usando \\(\\lambda =\\) 0.6262626, obteniendo el siguiente gráfico para la serie estabilizada:\n\n\n\n\n\n\n\n\n\nAunque visualmente no se ven cambios importantes en cuanto a la estabilización de la varianza, al aplicar nuevamente la transformación de Box-Cox a la serie estabilizada, se obtiene el siguiente resultado:\n\nlambda_loglik &lt;- MASS::boxcox(lm(serie_estabilizada062 ~ 1),seq(-2, 3, length = 100))  \n\n\n\n\n\n\n\n\nEl intervalo de confianza para \\(\\lambda\\) incluye al 1, indicando que ya no es necesario aplicar nuevamente una transformación.\n\nmaximo &lt;- which.max(lambda_loglik$y)\n(lambda_loglik &lt;- lambda_loglik$x[maximo])\n\n[1] 1.030303\n\n\nExactamente, se sugiere un valor de \\(\\lambda =\\) 1.030303 \\(\\approx 1\\). Se trabajará desde ahora con la serie estabilizada.\n\nserie &lt;- serie_estabilizada062\n\n\n\n\n\n\n\n\n     Total    Fecha\n1 9202.399 Jan 2000\n2 9442.243 Feb 2000\n3 9439.146 Mar 2000\n4 8470.853 Apr 2000\n5 9951.207 May 2000\n6 9993.015 Jun 2000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA partir de la serie sin tendencia, después de haberla estimado por el método STL, se hace el gráfico de subseries (por meses) para ver si se evidencia la existencia de algún tipo de estacionalidad.\nEl gráfico de subseries muestra diferencias en las medias de varios meses. Especialmente los meses de enero y febrero, presentan una un valor medio más bajo que el resto de las meses en el total de las exportaciones.\nAquí no se ve muy claro un patrón en las subseries anuales, pero se ve que en la mayoría de estas, los meses de enero y febrero tienen generalmente un valor menor que en el resto de los meses, aunque no siempre es así. En general no se ve de forma claro algún patrón que se repita a lo largo del tiempo.\nLos gráficos de dispersión de los retardos con el valor actual se presentan a continuación:\nNo se ve que exista una relación lineal con ninguno de los retardos, indicando posiblemente la no existencia de una componente estacional en la serie."
  },
  {
    "objectID": "descriptivoMario.html#bitcoin",
    "href": "descriptivoMario.html#bitcoin",
    "title": "Análisis descriptivo de las series de tiempo",
    "section": "2 Bitcoin",
    "text": "2 Bitcoin"
  },
  {
    "objectID": "DExportaciones.html",
    "href": "DExportaciones.html",
    "title": "Descriptiva-Serie-Exportaciones",
    "section": "",
    "text": "library(TSstudio)\nlibrary(zoo)\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\nlibrary(xts)\nlibrary(readxl)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::first()  masks xts::first()\n✖ dplyr::lag()    masks stats::lag()\n✖ dplyr::last()   masks xts::last()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(feasts)\n\nLoading required package: fabletools\n\nlibrary(fable)\nlibrary(timetk)\nlibrary(tsibble)\n\n\nAttaching package: 'tsibble'\n\nThe following object is masked from 'package:lubridate':\n\n    interval\n\nThe following object is masked from 'package:zoo':\n\n    index\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, union\n\nlibrary(lubridate)\nlibrary(dygraphs)\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\n\n1 Exportaciones mensuales de Colombia\n\n#Gráfico dinámico con dygraphs\ndygraph(exportaciones_ts,main=\"Serie de Exportaciones Mensuales en Colombia\", ylab=\"Valor total de las exportaciones\")%&gt;% dyRangeSelector()\n\n\n\n\n\n\n#Obtener información del objeto de tipo ts\nTSstudio::ts_info(exportaciones_ts)\n\n The exportaciones_ts series is a ts object with 1 variable and 282 observations\n Frequency: 12 \n Start time: 2000 1 \n End time: 2023 6 \n\n\nLa serie de tiempo de exportaciones de Colombia presnetada en el gráfico es una serie de tiempo mensual con frecuencia 12 con valores desde enero del 2000 hasta junio del 2023, sin ninguna observación faltante, o sea que cuenta con 282 observaciones. Del gráfico podemos suponer las siguientes características de la serie:\n\nVarianza marginal: La serie pareciera tener varianza marginal no constante, pues su rango de valores resulta ser pequeño en ciertos periodos de tiempo y grande en otros. Por ejemplo, de noviembre de 2008 a febrero de 2011 hay un rango grande, de marzo de 2011 a octubre de 2014 hay un rango pequeño y de noviembre de 2014 a febrero de 2016 hay de nuevo un rango grande.\nTendencia: A simple vista la serie presenta una tendencia determinística lineal creciente en los primeros años de observación. Sin embargo, desde el año 2011 la serie no sigue el mismo patrón y su tendencia empieza a variar por periodos de tiempo, por lo que se puede pensar en una tendencia de tipo estocástico.\nComponente Estacional: Haciendo una observación detallada de la serie acortando los años con ayuda del gráfico dinámico, se puede ver que la serie presenta picos en los meses de mayo y noviembre. Por los que se cree que hay existencia de estacionalidad de periodo 6 meses.\n\n\n1.0.1 Estabilización de la Varianza\nA continuación se evaluará si es necesario y conveniente hacer una tranformación de Box-Cox para estabilizar la varianza marginal de la serie.\nVeremos primero qué valores del parámetro lambda de la transformación de Box-Cox nos maximiza la log-verosimilitud. Si estos valores están lejos de 1 se sugiere hacer una transformación de Box-Cox.\n\nMASS::boxcox(lm(exportaciones_ts ~ 1),seq(-1, 2, length = 500))\n\n\n\n\nA patir de la gráfica anterior se verifica que los valores de lambda que maximizan la log-verosimilitud estan alrededor de 0.6, con un intervalo que no incluye al 1. Se toma la decisión de transformar la serie con lambda igual a \\(0.6\\) y se observam los resultados en comparación con la serie original.\n\nlexportaciones_ts &lt;- (1/0.6)*((exportaciones_ts^(0.6))-1)\n\n\npar(mar = c(1,1,1,1))\npar(mfrow=c(1,2))\ndygraph(exportaciones_ts,main=\"Serie de Exportaciones Mensuales en Colombia\", ylab=\"Valor total de las exportaciones\")%&gt;% dyRangeSelector()\n\n\n\n\ndygraph(lexportaciones_ts,main=\"Serie transformada con lambda=0.6\", ylab=\"Valor\")%&gt;% dyRangeSelector()\n\n\n\n\n\nSe nota un ligero cambio en la varianza marginal de la serie, en particular una disminución general de la varianza marginal. Se verifican los valores del parámetro lambda de la transforación de Box-Cox que maximizan la log-verosimilitud de la serie transformada.\n\nMASS::boxcox(lm(lexportaciones_ts ~ 1),seq(-1, 2, length = 500))\n\n\n\n\nEl valor de lambda que maximiza la log-verosimilitud sobre la serie transformada es ahora muy cercano a 1, por lo que se considera una buena transformación. Sin embargo, dado que el cambio no es muy notorio se toma la recomendación de ser flexibles con el valor de lambda y se realiza una transformación que es más notoria y que cumple con incluir al uno entre los valores de lambda que maximizan la verosimilitud.\nEl valor usado será lambda = 0.45, obteniendo los resultados siguientes\n\nlexportaciones_ts &lt;- (1/0.45)*((exportaciones_ts^(0.45))-1)\n\n\npar(mar = c(1,1,1,1))\npar(mfrow=c(2,1))\ndygraph(exportaciones_ts,main=\"Serie de Exportaciones Mensuales en Colombia\", ylab=\"Valor total de las exportaciones\")%&gt;% dyRangeSelector()\n\n\n\n\ndygraph(lexportaciones_ts,main=\"Serie transformada con lambda=0.45\", ylab=\"Valor\")%&gt;% dyRangeSelector()\n\n\n\n\n\n\nMASS::boxcox(lm(lexportaciones_ts ~ 1),seq(-1, 5, length = 500))\n\n\n\n\nSe decide continuar con esta última serie obtenida.\n\n#Otras opciones de transformación \nlibrary(VGAM)\n\nLoading required package: stats4\n\n\nLoading required package: splines\n\nprueba&lt;-VGAM::yeo.johnson(exportaciones_ts, lambda = 0.4)\n\n\ndygraph(prueba,main=\"Serie transformada con yeo.jhonson\", ylab=\"Valor\")%&gt;% dyRangeSelector()\n\n\n\n\n\n\n\n1.0.2 Análisis de Tendencia\nInicialmente revisaremos la función de autocorrelación, pues su forma nos dará indicios de si exisste tendencia o no.\n\nacf(lexportaciones_ts, 36, main = \"ACF de la serie estabilizada\")\n\n\n\n\nLa serie presenta alta autocorrelación de los rezagos con un decaimiento leve a medida que el rezago es mayor, estoo es un indicio de que la serie presenta tendencia. A continuación se realizaran ajustes determinísticos de la misma.\n\n1.0.2.1 Ajuste determinístico lineal\n\nsummary(fit &lt;- lm(lexportaciones_ts~time(lexportaciones_ts), na.action=NULL))\n\n\nCall:\nlm(formula = lexportaciones_ts ~ time(lexportaciones_ts), na.action = NULL)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-633.53 -195.39  -61.69  170.42  666.72 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             -81496.270   4705.423  -17.32   &lt;2e-16 ***\ntime(lexportaciones_ts)     41.382      2.339   17.69   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 266.5 on 280 degrees of freedom\nMultiple R-squared:  0.5278,    Adjusted R-squared:  0.5261 \nF-statistic:   313 on 1 and 280 DF,  p-value: &lt; 0.00000000000000022\n\n\n\nplot(lexportaciones_ts, main= \"Tendencia lineal ajustada\")\nabline(fit,col = \"red\")# Se añade la recta ajusta\n\n\n\n\n\n\n1.0.2.2 Serie sin tendencia\n\nnoTendlexportaciones_ts &lt;- lexportaciones_ts - predict(fit)\nplot(noTendlexportaciones_ts, main=\"Serie Sin tendencia\", \n     ylab= \"Valor\")\n\n\n\n\n\nacf(noTendlexportaciones_ts,lag.max =length(noTendlexportaciones_ts), \n    main=\"ACF Serie Sin tendencia\")\n\n\n\n\n\n\n1.0.2.3 Ajuste por medio de filtro de promedios móviles\n\ndescom_lexportaciones &lt;- decompose(lexportaciones_ts)\nplot(descom_lexportaciones)\n\n\n\n\n\nplot(lexportaciones_ts, ylab= \"Valor en escala logarítmica\")\nabline(fit,col = \"red\")# Se añade la recta ajusta\npoints(time(lexportaciones_ts), descom_lexportaciones$trend, col =\"green\", cex=0.4)\n\n\n\n\nLa línea de tendencia ajustada po medio del filtro de promedios móviles es muy distinta a la línea de tendencia lineal ajustada. Veamos como queda la serie al eliminar la tendencia usando este ajuste.\n\nnoTendlexportaciones_ts2 &lt;- lexportaciones_ts - descom_lexportaciones$trend\nplot(noTendlexportaciones_ts2, main=\"Serie Sin tendencia\", \n     ylab= \"Valor\")\n\n\n\n\nSe observa que esta estimación de la tendencia se ajusta mejor, pues al eliminar la tendencia la serie oscila constantemente alrededor de cero.\nSin embargo, a pesar que se estabilizó la varianza previamente, se sigue notando algo de varianza marginal no constante y de forma más notoria. Al inicio varinza baja y al final varianza alta.\n\nacf(noTendlexportaciones_ts2[7:276],lag.max =length(noTendlexportaciones_ts2), \n    main=\"ACF serie Sin tendencia\")\n\n\n\n\n\nacf(noTendlexportaciones_ts2[7:276],lag.max =48, \n    main=\"ACF serie Sin tendencia\")\n\n\n\n\n\n\n\n\n2 Descomposición STL\n\n####Primera aproximación del ajuste STL\nlexportaciones_tbl%&gt;%timetk::plot_time_series(Fecha, lexportaciones, \n                   .interactive = TRUE,\n                   .plotly_slider = TRUE)\n\n\n\n\n\n\n#####Ajuste STL\n\nlexportaciones_ajus &lt;- smooth_vec(lexportaciones,span = 0.75, degree = 2)\nlexportaciones_tbl%&gt;%dplyr::mutate(lexportaciones_ajus)\n\n# A tibble: 282 × 3\n   Fecha      lexportaciones lexportaciones_ajus\n   &lt;date&gt;              &lt;dbl&gt;               &lt;dbl&gt;\n 1 2000-01-01          1117.               1021.\n 2 2000-02-01          1138.               1026.\n 3 2000-03-01          1138.               1031.\n 4 2000-04-01          1053.               1036.\n 5 2000-05-01          1182.               1041.\n 6 2000-06-01          1186.               1046.\n 7 2000-07-01          1162.               1051.\n 8 2000-08-01          1226.               1056.\n 9 2000-09-01          1162.               1061.\n10 2000-10-01          1102.               1067.\n# ℹ 272 more rows\n\n\n\n###Ajuste STL moviendo los parámetros\nlexportaciones_tbl%&gt;%mutate(lexportaciones_ajus=smooth_vec(lexportaciones,span = 0.75, degree = 2))%&gt;%\n  ggplot(aes(Fecha, lexportaciones)) +\n    geom_line() +\n    geom_line(aes(y = lexportaciones_ajus), color = \"red\")\n\n\n\n\nSe observa que el ajuste es mucho más suavizado.\n\n###Eliminamos la tendencia con la predicción la recta xts\nnoTendlexportaciones_ts3 &lt;- lexportaciones_ts - lexportaciones_ajus\nplot(noTendlexportaciones_ts3, main=\"Serie Sin tendencia\", \n     ylab= \"Valor en escala logarítmica\") \n\n\n\n\n\nacf(noTendlexportaciones_ts3,lag.max =length(noTendlexportaciones_ts3), \n    main=\"ACF serie Sin tendencia\")\n\n\n\n\n\n2.0.0.1 Serie Diferenciada\n\n###Diferenciando con base en el objeto ts\ndlexportaciones&lt;-diff(lexportaciones_ts)\nplot(dlexportaciones)\nabline(h=0, col = \"red\")\n\n\n\n\nAl observar la serie diferenciada se hace más evidente la presencia de varianza marginal no constante en la serie, lo que quiere decir que las tansformación no la ha corregido lo suficiente.\n\nacf(dlexportaciones,lag.max =length(dlexportaciones), main=\"ACF de la Serie Diferenciada\")\n\n\n\n\n\nacf(dlexportaciones,lag.max =100, main=\"ACF de la Serie Diferenciada\")\n\n\n\n\nVarios rezagos superan el umbral de autocorrelación, en especial los que se encuentran cerca a las unidades. Por lo que se cree que hay presencia de una componente estacional de periodo 12.\nDadas las anteriores estimaciones de la tendencia y analizando cada una de las series con dichas tendencias estimadas eliminadas, se opta por continuar el estudio de la serie sin la tendencia dada por el filtro de promedios móviles y con la serie con la tendencia eliminada por medio de la diferenciación de la serie.\n\n\n2.0.1 Análisis de Estacionalidad\nA continuación se busca determinar si la serie presenta una componente estacional con la ayuda de diversos métodos. Se aplican los métodos sobre las series que fueron elegidas en la sección anterior.\n\n2.0.1.1 Correlación de las observaciones con sus retardos\nSe observa que tan correlacionadas estan las observaciones con sus retardos desde el 1 hasta el 12. Primero los resultados con la serie obtenida al estimar la tendencia con filtro de promedios móviles.\n\npar(mar = c(3,2,3,2))\nastsa::lag1.plot(noTendlexportaciones_ts2[7:276], 12,corr=T)\n\n\n\n\nLas correlaciones negativas que más destacan son las de los rezagos 4, 5 y 6. Y las positivas que mas destacan son las de los rezagos 1 y 12.\nAhora veamos de la serie diferenciada:\n\npar(mar = c(3,2,3,2))\nastsa::lag1.plot(dlexportaciones, 12,corr=T)\n\n\n\n\nLas correlaciones negativas que destacan son las de los rezagos 1 y 6, y la positiva es la del rezago 12. Coinciden en ser correlaciones negativas las del rezago 6 para ambos casos y en ser correlaciones positivas las del rezago 12 en ambos casos. Por lo que se sospecha de un ciclo estacional anual.\n\n\n2.0.1.2 Función de autocorrelación parcial\n\n#pacf\npacf(noTendlexportaciones_ts2[7:276], 100, main = \"PACF Serie sin tendencia\")\n\n\n\n\n\n#pacf\npacf(dlexportaciones, 100, main = \"PACF Serie Diferenciada\")\n\n\n\n\n\n\n2.0.1.3 AMI\nSobre la serie ajustada por medio del filtro de promedios móviles\n\ntseriesChaos::mutual(noTendlexportaciones_ts2[7:276], partitions = 16, lag.max = 50, plot=TRUE)\n\n\n\n\nAhora veamos sobre la serie diferenciada:\n\ntseriesChaos::mutual(dlexportaciones, partitions = 16, lag.max = 50, plot=TRUE)\n\n\n\n\n\n\n2.0.1.4 Mapas de calor\nCon la serie sin tendencia:\n\nprueba&lt;-xts(noTendlexportaciones_ts2[7:276],indice[7:276])\n\nTSstudio::ts_heatmap(prueba, title = \"Mapa de calor - Exportaciones sin tendencia\")\n\n\n\n\n\n\nTSstudio::ts_heatmap(dlexportaciones, title = \"Mapa de calor - Exportaciones sin tendencia\")\n\n\n\n\n\n\n\n2.0.1.5 Perdiodograma\n\n# periodograma\nPeriodograma &lt;- spectrum(as.numeric(noTendlexportaciones_ts2[7:276]),log=\"no\",span=c(5,5))\n\n\n\n\n\nubicacionElim&lt;- which.max(Periodograma$spec)\nsprintf(\"El valor de la frecuencia donde se máximiza el periodograma para la serie es: %s\",Periodograma$freq[ubicacionElim])\n\n[1] \"El valor de la frecuencia donde se máximiza el periodograma para la serie es: 0.0851851851851852\"\n\n\n\nsprintf(\"El periodo correspondiente es aproximadamente: %s\",1/Periodograma$freq[ubicacionElim])\n\n[1] \"El periodo correspondiente es aproximadamente: 11.7391304347826\"\n\n\n\n# periodograma\nPeriodograma &lt;- spectrum(as.numeric(dlexportaciones),log=\"no\",span=c(5,5))\n\n\n\n\n\nubicacionElim&lt;- which.max(Periodograma$spec)\nsprintf(\"El valor de la frecuencia donde se máximiza el periodograma para la serie es: %s\",Periodograma$freq[ubicacionElim])\n\n[1] \"El valor de la frecuencia donde se máximiza el periodograma para la serie es: 0.416666666666667\"\n\n\n\nsprintf(\"El periodo correspondiente es aproximadamente: %s\",1/Periodograma$freq[ubicacionElim])\n\n[1] \"El periodo correspondiente es aproximadamente: 2.4\"\n\n\n\n\n\n\n\n\n\n\n Volver arriba"
  },
  {
    "objectID": "ejemplo.html",
    "href": "ejemplo.html",
    "title": "Descriptiva-josé",
    "section": "",
    "text": "Code\n#datos\nBTC_Daily &lt;- read.csv(\"datos/BTC-Daily.csv\")\nData &lt;- data.frame(BTC_Daily$date,BTC_Daily$close)\nData &lt;- data.frame(BTC_Daily$date,BTC_Daily$close)\ncolnames(Data) &lt;- c(\"FechaTiempo\", \"Valor\")\n# limpiando datos faltantes\nstr(Data)\n\n\n'data.frame':   2651 obs. of  2 variables:\n $ FechaTiempo: chr  \"2022-03-01 00:00:00\" \"2022-02-28 00:00:00\" \"2022-02-27 00:00:00\" \"2022-02-26 00:00:00\" ...\n $ Valor      : num  43185 43179 37713 39147 39232 ...\n\n\nCode\nData$FechaTiempo &lt;- strftime(Data$FechaTiempo, format=\"%Y-%m-%d\")\nstr(Data)\n\n\n'data.frame':   2651 obs. of  2 variables:\n $ FechaTiempo: chr  \"2022-03-01\" \"2022-02-28\" \"2022-02-27\" \"2022-02-26\" ...\n $ Valor      : num  43185 43179 37713 39147 39232 ...\n\n\nCode\nData$FechaTiempo &lt;- as.Date(Data$FechaTiempo)\n# procesamiento de los datos\n\nBitcoin &lt;- Data %&gt;%\n      filter(FechaTiempo &gt;= as.Date(\"2017-01-01\"),\n             FechaTiempo &lt;= as.Date(\"2021-12-31\"))\nstr(Bitcoin)\n\n\n'data.frame':   1826 obs. of  2 variables:\n $ FechaTiempo: Date, format: \"2021-12-31\" \"2021-12-30\" ...\n $ Valor      : num  46214 47151 46483 47543 50718 ...\n\n\n\n\n\n\nCode\n# objeto serie de tiempo\nData_xts &lt;- xts::xts(Bitcoin$Valor, order.by = Bitcoin$FechaTiempo)\nhead(Data_xts)\n\n\n              [,1]\n2017-01-01  998.80\n2017-01-02 1014.10\n2017-01-03 1036.99\n2017-01-04 1122.56\n2017-01-05  994.02\n2017-01-06  891.56\n\n\nCode\nTSstudio::ts_info(Data_xts)\n\n\n The Data_xts series is a xts object with 1 variable and 1826 observations\n Frequency: daily \n Start time: 2017-01-01 \n End time: 2021-12-31 \n\n\nCode\n#class(Data_xts)\n#frequency(Data_xts)\n#xts::periodicity(Data_xts)\n#xts::tclass(Data_xts)\n#plot(Data_xts)\n\n\nVista de los datos para inspección visual\n\n\nCode\nTSstudio::ts_plot(Data_xts,\n         title = \"Valor de cierre bitcoin en bolsa\",\n         Ytitle = \"Valor en dolares\",\n          Xtitle = \"Fecha\",\n          Xgrid = TRUE,\n       Ygrid = TRUE)\n\n\n\n\n\n\n\nVarianza marginal: Se notan periodos donde el rango de valores que puede tomar la variable se va fluctuando a medida que pasa el tiempo.\nComponente Estacional: No se evidencia un comportamiento cíclico en la serie.\nTendencia: Se muestra la serie no oscila sobre un valor fijo y tiene cambios abruptos de crecimiento y decrecimiento en algunos momentos.\n\n\n\n\nUsaremos la transformación de Box-Cox para estabilizar la varianza; primero miramos el lambda\n\n\nCode\n#Valor de lambda\nforecast::BoxCox.lambda(Data_xts, method =\"loglik\", lower = -1, upper = 3)\n\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\n[1] 0\n\n\nCode\nMASS::boxcox(lm(Data_xts ~ 1),seq(-1/2, 1/2, length = 50))\n\n\n\n\n\nVemos que se sugiere el valor \\(\\lambda = 0\\) lo cual dada de transformación de Box-Cox se usa la función logaritmo natural para la estabilización de la variabilidad así tenemos que:\n\n\nCode\n#trasnformación\nlData_xts &lt;- log(Data_xts)\n#plot(lData_xts)\n\n\n\n\nCode\nTSstudio::ts_plot(lData_xts,\n          title = \"Valor de Serie Trasnformada\",\n           Ytitle = \"Valor de la trasnformación\",\n          Xtitle = \"Fecha\",\n          Xgrid = TRUE,\n           Ygrid = TRUE)\n\n\n\n\n\n\nAhora miramos si es necesario aplicar otra transformación a la serie\n\n\nCode\n#Valor de lambda\n(forecast::BoxCox.lambda(lData_xts, method =\"loglik\", lower = -1, upper = 3))\n\n\n[1] 0.9\n\n\nCode\nMASS::boxcox(lm(lData_xts ~ 1),seq(-1, 2, length = 50))\n\n\n\n\n\nVemos que la sugerencia es \\(\\lambda = 0.9\\) lo cual es cercano a \\(1\\), además el IC de confianza captura al \\(1\\), por ende la transformación logarítmica parece haber estabilizado la varianza.\n\n\nCode\n#Gráfico de ellas juntas\npar(mfrow=c(2,1))\nplot(Data_xts, main = \"Series original\")\nplot(lData_xts, main = \"Series transformada\")\n\n\n\n\n\nSe puede ver cómo la transformación aplicada logra estabilizar la varianza en gran medida.\n\n\n\nTrabajaremos con la serie a la cuál se le realizo la transformación para estabilizar la varianza, realizaremos el gráfico de los valores de la función de auto-correlación\n\n\nCode\n#ACf\nacf(lData_xts, 180, main = \"Serie Bitcoin Trasnformada\")\n\n\n\n\n\nNotamos que los valores van teniendo un decaimiento leve lo cual nos da un indicio más claro de que existe tendencia en la serie, analizaremos el gráfico de retardos de la serie trasnformada para ver si podemos tener indicios de una relación no-lineal o lineal en la serie.\n\n\nCode\n#serie transformada\npar(mar = c(3,2,3,2))\nastsa::lag1.plot(lData_xts, 16,corr=T)\n\n\n\n\n\nVemos que se nota un fuerte relación linea hasta para el retraso número 16, por lo tanto con lo mostrado por el acf y el gráfico de retardos tenemos indicios fuertes de tendencia en la serie así usaremos los métodos: lineal determinista, Descomposición de promedios móviles y descomposición STL para estimar dicha componente.\n\n\najustaremos el modelo eliminaremos la tendencia y analizaremos los resultados\n\n\nCode\n#pasar a ts para Graficarlo\nldata_ts &lt;- TSstudio::xts_to_ts(lData_xts,frequency = 365,\n                                start = as.Date(\"2017-01-01\"))\n#modelo lineal\nsummary(fit &lt;- lm(ldata_ts~time(ldata_ts), na.action=NULL))\n\n\n\nCall:\nlm(formula = ldata_ts ~ time(ldata_ts), na.action = NULL)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.06366 -0.49265  0.01902  0.36341  1.68667 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.230e+03  1.780e+01  -69.10   &lt;2e-16 ***\ntime(ldata_ts)  6.135e-01  8.813e-03   69.61   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5439 on 1824 degrees of freedom\nMultiple R-squared:  0.7265,    Adjusted R-squared:  0.7264 \nF-statistic:  4846 on 1 and 1824 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n# Gráfico\nplot(ldata_ts, ylab= \"Valor en escala logarítmica\")\nabline(fit,col = \"red\")# Se añade la recta ajusta\n\n\n\n\n\nahora eliminaremos la tendencia de la serie\n\n\nCode\n###Eliminamos la tendencia con la predicción la recta\nElimTenldata_ts &lt;- ldata_ts - predict(fit)\nplot(ElimTenldata_ts, main=\"Serie Sin tendencia\", \n     ylab= \"Valor en escala logarítmica\")\n\n\n\n\n\nObservamos que en la serie obtenida después de eliminar la tendencia lineal parece tener un comportamiento de alta variabilidad similar una caminata aleatoria.\n\n\nCode\nacf(ElimTenldata_ts,lag.max =length(ElimTenldata_ts), \n    main=\"Serie Sin tendencia\")\n\n\n\n\n\nCode\n# Series trasnfromada sin tendencia Retardos\npar(mar = c(3,2,3,2))\nastsa::lag1.plot(ElimTenldata_ts, 16,corr=F)\n\n\n\n\n\nNotamos que en la gráfica del acf se sigue teniendo un decaimiento lento de los valores de la función de auto-correlación para los primeros rezagos, además en el gráfico de retardos se sigue manteniendo una alta relación lineal entre el valor actual y sus regazos. Por ende todo esto nos da los argumentos necesarios para descartar la estimación linealcómo una buena estimación de la tendencia.\n\n\n\n\n\nCode\n# descomposición de promedios moviles\ndescom_ldata &lt;- decompose(ldata_ts)\nplot(descom_ldata)\n\n\n\n\n\nPodemos observar que usando un filtro de promedio móvil la tendencia estimada no se aproxima mucho a una lineal, cómo se puede apreciar en el siguiente gráfico; además la componente estacional no parece ser estimada de buena manera ya que no se ve un patrón de comportamiento claramente, además el residual presenta un comportamiento no estacionario aparentemente.\n\n\nCode\n# Gráfico\nplot(ldata_ts, ylab= \"Valor en escala logarítmica\")\nabline(fit,col = \"red\")# Se añade la recta ajusta\npoints(time(ldata_ts), descom_ldata$trend, col =\"green\", cex=0.3)\n\n\n\n\n\nEliminaremos la tendencia del promedio móvil centrado y de la frecuencia\n\n\nCode\n###Eliminamos la tendencia con la predicción promedio movil\nElimTenldata_ts &lt;- ldata_ts - descom_ldata$trend\nplot(ElimTenldata_ts, main=\"Serie Sin tendencia\", \n     ylab= \"Valor en escala logarítmica\")\n\n\n\n\n\nPodemos ver que la serie cómo en el caso lineal parece mostrar un comportamiento de caminata aleatoria.\n\n\nCode\nacf(ElimTenldata_ts[183:1644],lag.max = 730, \n    main=\"Serie Sin tendencia\")\n\n\n\n\n\nCode\n# Series trasnfromada sin tendencia Retardos\npar(mar = c(3,2,3,2))\nastsa::lag1.plot(ElimTenldata_ts[183:1644], 16,corr=F)\n\n\n\n\n\nNotamos que tanto en el acf cómo en la gráfica de retardos se ve un comportamiento similar al anterior lo cuál nos hace descartar la descompsición de promedios moviles para la estimación de la tendencia.\nFiltro promedio móvil con solo retrasos\nIntentaremos ajustar un promedio móvil que tenga en cuenta solo los retrasos y sea de los periodos de un año, seis meses, tres meses y mes.\n\n\nCode\n#gráfico\nfilter_1=stats::filter(ldata_ts, filter = rep(1/365, 365), sides = 1)\nfilter_2=stats::filter(ldata_ts, filter = rep(1/182, 182), sides = 1)\nfilter_3=stats::filter(ldata_ts, filter = rep(1/90, 90), sides = 1)\nfilter_4=stats::filter(ldata_ts, filter = rep(1/30, 30), sides = 1)\nplot(ldata_ts, ylab= \"Valor en escala logarítmica\")\npoints(time(ldata_ts), filter_1, col =\"green\", cex=0.33)\npoints(time(ldata_ts), filter_2, col =\"blue\", cex=0.33)\npoints(time(ldata_ts), filter_3, col =\"red\", cex=0.35)\npoints(time(ldata_ts), filter_4, col =\"cyan\", cex=0.31)\n\n\n\n\n\nCode\n#legend(locator(1), c(\"365 días\",\"182 días\",\"90 días\",\"30 días\"), col=c(\"green\",\"blue\",\"red\",\"cyan\"),lty=c(1,1,1,1),lwd=c(2,2,2,2))\n\n\nNotamos que para 3 meses y 6 meses los filtros de promedios móviles muestra una mejor estimación, por ende tomaremos para 3 meses cómo estimación de la tendencia de la serie\n\n\nCode\n###Eliminamos la tendencia con la predicción promedio movil\nElimTenldata_ts &lt;- ldata_ts - filter_3\nplot(ElimTenldata_ts, main=\"Serie Sin tendencia\", \n     ylab= \"Valor en escala logarítmica\")\n\n\n\n\n\nCode\n#\nacf(ElimTenldata_ts[90:1826],lag.max =1095, \n    main=\"Serie Sin tendencia\")\n\n\n\n\n\nCode\n# Series trasnfromada sin tendencia Retardos\npar(mar = c(3,2,3,2))\nastsa::lag1.plot(ElimTenldata_ts[90:1826], 16,corr=F)\n\n\n\n\n\nEl comportamiento de los gráficos es similar a los anteriores con eso tenemos indicios de que la estimación de la tendencia de manera determinista potencialmente no es buena idea.\n\n\n\nUsando la descomposición STL obtenemos la estimación de la tendencia\n\n\nCode\nindice_ldata &lt;- sort(Bitcoin$FechaTiempo)\n#  as.Date(as.yearmon(tk_index(ldata_ts)))\n## Otra forma de extraer el indice estimetk::tk_index(lAirPass)\nlogdata &lt;- as.matrix(ldata_ts)\ndf_ldata &lt;- data.frame(Fecha=indice_ldata,logdata=as.matrix(ldata_ts))\nstr(df_ldata)\n\n\n'data.frame':   1826 obs. of  2 variables:\n $ Fecha   : Date, format: \"2017-01-01\" \"2017-01-02\" ...\n $ Series.1: num  6.91 6.92 6.94 7.02 6.9 ...\n\n\nCode\ncolnames(df_ldata) &lt;- c(\"Fecha\", \"logdata\")\nstr(df_ldata)\n\n\n'data.frame':   1826 obs. of  2 variables:\n $ Fecha  : Date, format: \"2017-01-01\" \"2017-01-02\" ...\n $ logdata: num  6.91 6.92 6.94 7.02 6.9 ...\n\n\nCode\ntibble_ldata &lt;- tibble(df_ldata)\n####Primera aproximación del ajuste STL\ntibble_ldata%&gt;%timetk::plot_time_series(Fecha, logdata, \n                   .interactive = TRUE,\n                   .plotly_slider = TRUE)\n\n\n\n\n\n\n\n\nCode\n#####Ajuste STL\n#Note que obtenemos un objeto adicional en tibble_logpasajeros con Logpasa_ajus con parámetros que se pueden mover.\nlogdata_ajus &lt;- smooth_vec(logdata,span = 0.75, degree = 2)\ntibble_ldata%&gt;%dplyr::mutate(logdata_ajus)\n\n\n# A tibble: 1,826 × 3\n   Fecha      logdata logdata_ajus\n   &lt;date&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n 1 2017-01-01    6.91         6.90\n 2 2017-01-02    6.92         6.91\n 3 2017-01-03    6.94         6.91\n 4 2017-01-04    7.02         6.92\n 5 2017-01-05    6.90         6.92\n 6 2017-01-06    6.79         6.93\n 7 2017-01-07    6.80         6.94\n 8 2017-01-08    6.81         6.94\n 9 2017-01-09    6.80         6.95\n10 2017-01-10    6.81         6.96\n# ℹ 1,816 more rows\n\n\nCode\n###Ajuste STL moviendo los parámetros\ntibble_ldata%&gt;%mutate(logdata_ajus=smooth_vec(logdata,span = 0.75, degree = 2))%&gt;%\n  ggplot(aes(Fecha, logdata)) +\n    geom_line() +\n    geom_line(aes(y = logdata_ajus), color = \"red\")\n\n\n\n\n\nSe puede evidenciar que la Estimación de la tendencia via STL parece mejorar aspectos que la descomposición movil intentada con información de un año no se tenia.\n\n\nCode\n###Eliminamos la tendencia con la predicción la STL\nElimTenldata_xts &lt;- lData_xts - logdata_ajus\nplot(ElimTenldata_xts, main=\"Serie Sin tendencia\", \n     ylab= \"Valor en escala logarítmica\")\n\n\n\n\n\nCode\nacf(ElimTenldata_xts,lag.max =1094, \n    main=\"Serie Sin tendencia\")\n\n\n\n\n\nCode\n# Series trasnfromada sin tendencia Retardos\npar(mar = c(3,2,3,2))\nastsa::lag1.plot(ElimTenldata_xts, 16,corr=F)\n\n\n\n\n\nVemos que a diferencia de los promedio móviles si tenemos una estimación para todos los valores de la serie, además notamos que la acf y el gráfico de retardos tiene un comportamiento similar a los métodos anteriores.\n\n\n\n\n\n\nCode\n###Diferenciando con base en el objeto ts\ndldata&lt;-diff(ldata_ts)\n#plot(dldata)\n#abline(h=0, col = \"red\")\n#acf(dldata,lag.max =90, main=\"Serie Diferenciada\")\n\n\n\n\nCode\n###Diferenciando con base en el objeto xts\ndldata_xts&lt;-diff(lData_xts)\ndldata_xts &lt;- dldata_xts[-1]\nplot(dldata_xts, main = \"Serie diferenciada\")\n\n\n\n\n\nVemos que la serie al ser diferenciada muestra un comportamiento estacionario pues los valores oscilan sobre un valor fijo, además de un valor que muestra un comportamiento extremo pues varia demasiado con respecto a los demás.\n\n\nCode\n# función de autocorrelación\nacf(dldata_xts,lag.max =30, main=\"Serie Diferenciada\")\n\n\n\n\n\nCode\n# Series trasnfromada sin tendencia Retardos\npar(mar = c(3,2,3,2))\nastsa::lag1.plot(dldata_xts, 16,corr=T)\n\n\n\n\n\nEl gráfico de acf muestra que la tendencia parce a ver desparecido, además no parece destacar ningún valor para algún retraso de manera clara. Para el gráfico de retardos vemos claramente que ya no hay una relación lineal ni no lineal del valor actual con sus retardos.\nTomando en cuenta todo lo anterior trabajaremos con la serie aplicada la transformación de Box-Cox sugerida y diferenciada, además se tiene sospecha de que la serie presenta comportamiento de una camina aleatoria con tendencia no determinista.\n\n\n\n\n\n\n\nCode\n#Serie diferenciada\nTSstudio::ts_heatmap(dldata_xts, title = \"Mapa de calor - Cierre Bitcoin en bolsa dias año\")\n\n\n\n\n\n\nPara la serie diferenciada no se evidencia ninguno tipo de patrón lo cual nos da indicios de que no se tiene un componente estacional.\n\n\n\n\n\nCode\n# periodograma\nPeriodograma &lt;- spectrum(as.numeric(dldata_xts),log=\"no\",span=c(5,5))\n\n\n\n\n\nCode\n#\nubicacionDif &lt;- which.max(Periodograma$spec)\nsprintf(\"El valor de la frecuencia donde se máximiza el periodograma para la serie es: %s\",Periodograma$freq[ubicacionDif])\n\n\n[1] \"El valor de la frecuencia donde se máximiza el periodograma para la serie es: 0.4016\"\n\n\nCode\n#\nsprintf(\"El periodo correspondiente es aproximadamente: %s\",1/Periodograma$freq[ubicacionDif])\n\n\n[1] \"El periodo correspondiente es aproximadamente: 2.49003984063745\"\n\n\nPara la serie diferenciada el periodograma no es claro, a pesar del suviazamiento usado la curva sigue mostrando varios picos en su recorrido el máximo lo encontramos de tal manera que \\(\\omega = \\frac{251}{625}=0.416\\) lo cual se corresponde con un \\(s \\approx 2.5\\) .\n\n\nCode\n# intentando sacar el segundo más alto\nn_dld &lt;- length(Periodograma$spec)\nvalor_seg &lt;- sort(Periodograma$spec,partial=n_dld-1)[n_dld-1]\nubica_segundo &lt;- which(Periodograma$spec==valor_seg)\n\nsprintf(\"El valor de la frecuencia donde se alcanza el segundo máximo para el periodograma para REC es: %s\",Periodograma$freq[ubica_segundo])\n\n\n[1] \"El valor de la frecuencia donde se alcanza el segundo máximo para el periodograma para REC es: 0.401066666666667\"\n\n\nCode\n#\nsprintf(\"El periodo correspondiente es aproximadamente: %s\",1/Periodograma$freq[ubica_segundo])\n\n\n[1] \"El periodo correspondiente es aproximadamente: 2.49335106382979\"\n\n\nvemos que el segundo valor es bastante parecido al segundo.\n\n\nCode\n# valor de frecuencia \ntail(sort(Periodograma$spec))\n\n\n[1] 0.004895118 0.004914000 0.005095353 0.005470611 0.005752230 0.005966273\n\n\nCómo se puede observar los primeros seis valores son bastante cercanos entre ellos, por lo tanto sus valores de periodo serán similiares.\nCon esto descartamos la estimación de una componente estacional pues no tenemos evidencia clara de su existencia."
  },
  {
    "objectID": "ejemplo.html#serie-diferenciada",
    "href": "ejemplo.html#serie-diferenciada",
    "title": "Descriptiva-josé",
    "section": "",
    "text": "Code\n###Diferenciando con base en el objeto ts\ndldata&lt;-diff(ldata_ts)\n#plot(dldata)\n#abline(h=0, col = \"red\")\n#acf(dldata,lag.max =90, main=\"Serie Diferenciada\")\n\n\n\n\nCode\n###Diferenciando con base en el objeto xts\ndldata_xts&lt;-diff(lData_xts)\ndldata_xts &lt;- dldata_xts[-1]\nplot(dldata_xts, main = \"Serie diferenciada\")\n\n\n\n\n\nVemos que la serie al ser diferenciada muestra un comportamiento estacionario pues los valores oscilan sobre un valor fijo, además de un valor que muestra un comportamiento extremo pues varia demasiado con respecto a los demás.\n\n\nCode\n# función de autocorrelación\nacf(dldata_xts,lag.max =30, main=\"Serie Diferenciada\")\n\n\n\n\n\nCode\n# Series trasnfromada sin tendencia Retardos\npar(mar = c(3,2,3,2))\nastsa::lag1.plot(dldata_xts, 16,corr=T)\n\n\n\n\n\nEl gráfico de acf muestra que la tendencia parce a ver desparecido, además no parece destacar ningún valor para algún retraso de manera clara. Para el gráfico de retardos vemos claramente que ya no hay una relación lineal ni no lineal del valor actual con sus retardos.\nTomando en cuenta todo lo anterior trabajaremos con la serie aplicada la transformación de Box-Cox sugerida y diferenciada, además se tiene sospecha de que la serie presenta comportamiento de una camina aleatoria con tendencia no determinista."
  },
  {
    "objectID": "ejemplo.html#análisis-de-estacionalidad",
    "href": "ejemplo.html#análisis-de-estacionalidad",
    "title": "Descriptiva-josé",
    "section": "",
    "text": "Code\n#Serie diferenciada\nTSstudio::ts_heatmap(dldata_xts, title = \"Mapa de calor - Cierre Bitcoin en bolsa dias año\")\n\n\n\n\n\n\nPara la serie diferenciada no se evidencia ninguno tipo de patrón lo cual nos da indicios de que no se tiene un componente estacional.\n\n\n\n\n\nCode\n# periodograma\nPeriodograma &lt;- spectrum(as.numeric(dldata_xts),log=\"no\",span=c(5,5))\n\n\n\n\n\nCode\n#\nubicacionDif &lt;- which.max(Periodograma$spec)\nsprintf(\"El valor de la frecuencia donde se máximiza el periodograma para la serie es: %s\",Periodograma$freq[ubicacionDif])\n\n\n[1] \"El valor de la frecuencia donde se máximiza el periodograma para la serie es: 0.4016\"\n\n\nCode\n#\nsprintf(\"El periodo correspondiente es aproximadamente: %s\",1/Periodograma$freq[ubicacionDif])\n\n\n[1] \"El periodo correspondiente es aproximadamente: 2.49003984063745\"\n\n\nPara la serie diferenciada el periodograma no es claro, a pesar del suviazamiento usado la curva sigue mostrando varios picos en su recorrido el máximo lo encontramos de tal manera que \\(\\omega = \\frac{251}{625}=0.416\\) lo cual se corresponde con un \\(s \\approx 2.5\\) .\n\n\nCode\n# intentando sacar el segundo más alto\nn_dld &lt;- length(Periodograma$spec)\nvalor_seg &lt;- sort(Periodograma$spec,partial=n_dld-1)[n_dld-1]\nubica_segundo &lt;- which(Periodograma$spec==valor_seg)\n\nsprintf(\"El valor de la frecuencia donde se alcanza el segundo máximo para el periodograma para REC es: %s\",Periodograma$freq[ubica_segundo])\n\n\n[1] \"El valor de la frecuencia donde se alcanza el segundo máximo para el periodograma para REC es: 0.401066666666667\"\n\n\nCode\n#\nsprintf(\"El periodo correspondiente es aproximadamente: %s\",1/Periodograma$freq[ubica_segundo])\n\n\n[1] \"El periodo correspondiente es aproximadamente: 2.49335106382979\"\n\n\nvemos que el segundo valor es bastante parecido al segundo.\n\n\nCode\n# valor de frecuencia \ntail(sort(Periodograma$spec))\n\n\n[1] 0.004895118 0.004914000 0.005095353 0.005470611 0.005752230 0.005966273\n\n\nCómo se puede observar los primeros seis valores son bastante cercanos entre ellos, por lo tanto sus valores de periodo serán similiares.\nCon esto descartamos la estimación de una componente estacional pues no tenemos evidencia clara de su existencia."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Series de tiempo univariadas",
    "section": "",
    "text": "Página web del curso 🤪.\n\n\n\n\n\n Volver arriba"
  },
  {
    "objectID": "redes.html",
    "href": "redes.html",
    "title": "Redes neuronales",
    "section": "",
    "text": "Code\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import CSVLogger, EarlyStopping\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport seaborn as sns\nimport time\nimport gc\nimport sys\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\nprint(f\"Tensorflow Version: {tf.__version__}\")\nprint(f\"Pandas Version: {pd.__version__}\")\nprint(f\"Numpy Version: {np.__version__}\")\nprint(f\"System Version: {sys.version}\")\n\nmpl.rcParams['figure.figsize'] = (17, 5)\nmpl.rcParams['axes.grid'] = False\nsns.set_style(\"whitegrid\")\n\nnotebookstart= time.time()\n\n\n\n\nCode\nimport IPython\nimport IPython.display\n\n\n\n\nTotal de Exportaciones Colombia desde enero del año 2000 hasta junio del año 2023.\n\n\nCode\n# Lectura de la serie\nExportaciones = pd.read_excel(\"datos/Exportaciones.xlsx\",\n                   header = 0, usecols = ['Mes','Total']).iloc[96:].reset_index(drop = True).round()\nExportaciones['Total'] = Exportaciones['Total'].astype(int) \nExportaciones\n\n\n\n\n\n\n\n\n\nMes\nTotal\n\n\n\n\n0\n2000-01-01\n1011676\n\n\n1\n2000-02-01\n1054098\n\n\n2\n2000-03-01\n1053546\n\n\n3\n2000-04-01\n886359\n\n\n4\n2000-05-01\n1146258\n\n\n...\n...\n...\n\n\n277\n2023-02-01\n4202234\n\n\n278\n2023-03-01\n4431911\n\n\n279\n2023-04-01\n3739214\n\n\n280\n2023-05-01\n4497862\n\n\n281\n2023-06-01\n3985981\n\n\n\n\n282 rows × 2 columns\n\n\n\n\n\n\n\nSe quiere predecir la variable “Total de Exportaciones” (Total) en base a sus retardos.  A continuación se presenta el gráfico de la serie de tiempo:\n\n\nCode\nfeatures_considered = ['Total'] # la variable a usar en la predicción es ella misma\nfeatures = Exportaciones[features_considered] # solo se usará la variable Total en la predicción\nfeatures.index = Exportaciones['Mes'] # variable que indica el tiempo (la serie es mensual)\nfeatures.head()\n\n\n\n\nCode\nfeatures.plot(subplots = True, ) # gráfico de la serie de tiempo\n\n\narray([&lt;Axes: xlabel='Mes'&gt;], dtype=object)\n\n\n\n\n\n\n\n\n\nEl conjunto de datos original se divide en tres partes, de la siguiente manera:\n\nDatos de entrenamiento (70%) para un total de 197 observaciones. Desde 2000-01 hasta 2016-05.\nDatos de validación (20%) para un total de 56 observaciones. Desde 2016-06 hasta 2021-01.\nDatos de entrenamiento (10%) para un total de 29 observaciones. Desde 2021-02 hasta 2023-06.\n\n\n\nCode\n# partición del conjuntos de datos en entrenamiento, validación y prueba\ncolumn_indices = {name: i for i, name in enumerate(features.columns)} # índice = 0\n\nn = len(features) # longitud de la serie (282)\ntrain_df = features[0:int(n*0.7)] # 2000-01 hasta 2016-05\nval_df = features[int(n*0.7):int(n*0.9)] # 2016-06 hasta 2021-01\ntest_df = features[int(n*0.9):] # 2021-02 hasta 2023-06\n\nnum_features = features.shape[1]\n\n\n\n\nCode\nprint(\"longitud dataframe entrenamiento:\", train_df.shape)\nprint(\"longitud dataframe validación:\", val_df.shape)\nprint(\"longitud dataframe prueba:\", test_df.shape)\n\n\n\n\nCode\n# Normalización de las observaciones\ntrain_mean = train_df.mean()\ntrain_std = train_df.std()\n\ntrain_df = (train_df - train_mean) / train_std\nval_df = (val_df - train_mean) / train_std\ntest_df = (test_df - train_mean) / train_std\n\n\n\n\nCode\n# todo el dataframe normalizado por train_mean y train_std\ndf_std = (features - train_mean) / train_std\ndf_std = df_std.melt(var_name='', value_name='Datos normalizados')\ndf_std\n\n\n\n\nCode\n# plt.subplot(1, 2, 1)\n# sns.violinplot(y=Exportaciones[\"Total\"])\n\nplt.figure(figsize=(12, 8))\nax = sns.violinplot(x = '', y = 'Datos normalizados', data = df_std)\n_ = ax.set_xticklabels(features.keys(), rotation=0)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe definen las clases y funciones necesarias para resolver el problema de redes neuronales por medio de tensores.\n\nSe usará un tamaño de lote de 32.\nEl número máximo de épocas será de 20.\n\n\n\nCode\nclass WindowGenerator():\n  def __init__(self, input_width, label_width, shift,\n               train_df=train_df, val_df=val_df, test_df=test_df,\n               label_columns=None):\n    # Store the raw data.\n    self.train_df = train_df\n    self.val_df = val_df\n    self.test_df = test_df\n\n    # Work out the label column indices.\n    self.label_columns = label_columns\n    if label_columns is not None:\n      self.label_columns_indices = {name: i for i, name in\n                                    enumerate(label_columns)}\n    self.column_indices = {name: i for i, name in\n                           enumerate(train_df.columns)}\n\n    # Work out the window parameters.\n    self.input_width = input_width\n    self.label_width = label_width\n    self.shift = shift\n\n    self.total_window_size = input_width + shift\n\n    self.input_slice = slice(0, input_width)\n    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n\n    self.label_start = self.total_window_size - self.label_width\n    self.labels_slice = slice(self.label_start, None)\n    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n\n  def __repr__(self):\n    return '\\n'.join([\n        f'Total window size: {self.total_window_size}',\n        f'Input indices: {self.input_indices}',\n        f'Label indices: {self.label_indices}',\n        f'Label column name(s): {self.label_columns}'])\n\n\n\n\nCode\ndef split_window(self, features):\n  inputs = features[:, self.input_slice, :]\n  labels = features[:, self.labels_slice, :]\n  if self.label_columns is not None:\n    labels = tf.stack(\n        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n        axis=-1)\n\n  # Slicing doesn't preserve static shape information, so set the shapes\n  # manually. This way the `tf.data.Datasets` are easier to inspect.\n  inputs.set_shape([None, self.input_width, None])\n  labels.set_shape([None, self.label_width, None])\n\n  return inputs, labels\n\nWindowGenerator.split_window = split_window\n\n\n\n\nCode\ndef make_dataset(self, data):\n  data = np.array(data, dtype=np.float32)\n  ds = tf.keras.utils.timeseries_dataset_from_array(\n      data=data,\n      targets=None,\n      sequence_length=self.total_window_size,\n      sequence_stride=1,\n      shuffle=False,\n      batch_size=32,) \n\n  ds = ds.map(self.split_window)\n\n  return ds\n\nWindowGenerator.make_dataset = make_dataset\n\n\n\n\nCode\n@property\ndef train(self):\n  return self.make_dataset(self.train_df)\n\n@property\ndef val(self):\n  return self.make_dataset(self.val_df)\n\n@property\ndef test(self):\n  return self.make_dataset(self.test_df)\n\n@property\ndef example(self):\n  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n  result = getattr(self, '_example', None)\n  if result is None:\n    # No example batch was found, so get one from the `.train` dataset\n    result = next(iter(self.train))\n    # And cache it for next time\n    self._example = result\n  return result\n\nWindowGenerator.train = train\nWindowGenerator.val = val\nWindowGenerator.test = test\nWindowGenerator.example = example\n\n\n\n\nCode\ndef plot(self, model=None, plot_col='Total', max_subplots=3):\n  inputs, labels = self.example\n  plt.figure(figsize=(12, 8))\n  plot_col_index = self.column_indices[plot_col]\n  max_n = min(max_subplots, len(inputs))\n  for n in range(max_n):\n    plt.subplot(max_n, 1, n+1)\n    plt.ylabel(f'{plot_col} [normed]')\n    plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n             label='Inputs', marker='.', zorder=-10)\n\n    if self.label_columns:\n      label_col_index = self.label_columns_indices.get(plot_col, None)\n    else:\n      label_col_index = plot_col_index\n\n    if label_col_index is None:\n      continue\n\n    plt.scatter(self.label_indices, labels[n, :, label_col_index],\n                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n    if model is not None:\n      predictions = model(inputs)\n      plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n                  marker='X', edgecolors='k', label='Predictions',\n                  c='#ff7f0e', s=64)\n\n    if n == 0:\n      plt.legend()\n\n  plt.xlabel('Time [h]')\n\nWindowGenerator.plot = plot\n\n\n\n\nCode\n# Definimos número de épocas necesarias y funciones de pérdida\nMAX_EPOCHS = 20\n\ndef compile_and_fit(model, window, patience=2): #patiences como el número de épocas que espera antes de parar\n  # Para evitar sobreajuste\n  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                                    patience=patience,\n                                                    mode='min')\n\n  model.compile(loss=tf.losses.MeanSquaredError(),\n                optimizer=tf.optimizers.Adam(),\n                metrics=[tf.metrics.MeanAbsoluteError()])\n\n  history = model.fit(window.train, epochs=MAX_EPOCHS,\n                      validation_data=window.val,\n                      callbacks=[early_stopping])\n  return history\n\n\n\n\n\nEl interés en este caso es predecir el valor del total de las exportaciones 1 paso adelante (siguiente mes) con base en la información disponible de los últimos 12 meses del total de las exportaciones.\n\nw1 = WindowGenerator(input_width=12, label_width=1, shift=1,\n                     label_columns=['Total'])\nw1\n\nA continuación se muestra la configuración del modelo. Se usan las 12 observaciones anteriores para predecir la siguiente.\n\nw1.plot()\n\n\n\n\n\n\n\n\n\n\n\nLa búsqueda de los hiperparámetros que minimicen el error cuadrático medio sobre el conjunto de validación se hará con el método BayesianOptimization Tuner. El método consiste en escoger aleatoriamente algunas (pocas) combinaciones de los parámetros, luego basado en el rendimiento de esos hiperparámetros, escoge los siguientes mejores hiperparámetros. Es decir que toma en cuenta el histórico de los hiperparámetros que ya han sido probados en el modelo.\n\n\nCode\ndef build_model(hp):\n    model = keras.Sequential()\n    model.add(layers.Dense(units=hp.Int(\"num_units\", min_value=32, max_value=564, step=32),\n                activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"])))\n    # Tune the number of layers.\n    for i in range(hp.Int(\"num_layers\", 1, 5)):\n        model.add(\n            layers.Dense(\n                # Tune number of units separately.\n                units=hp.Int(f\"units_{i}\", min_value=32, max_value=564, step=32),\n                activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"]),\n            )\n        )\n    if hp.Boolean(\"dropout\"):\n        model.add(layers.Dropout(rate=0.25))\n    model.add(layers.Dense(1, activation=\"linear\"))\n    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n        loss=\"mean_squared_error\",\n        metrics=[\"mean_squared_error\"]\n    )\n    return model\n\n\nbuild_model(kt.HyperParameters())\n\n\n\n\nCode\ntuner_BayesianOptimization_mlp = kt.BayesianOptimization(\n    hypermodel=build_model,\n    objective=\"val_loss\",\n    max_trials=10,\n    num_initial_points=None,\n    alpha=0.0001,\n    beta=2.6,\n    seed=1234,\n    hyperparameters=None,\n    tune_new_entries=True,\n    allow_new_entries=True,\n    max_retries_per_trial=0,\n    max_consecutive_failed_trials=5,\n    overwrite=True,\n    directory=\"dirsalida\",\n    project_name=\"helloworld\"\n)\n\n\n\n\nCode\nstop_early=tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",patience=0)\n\n\n\n\nCode\ntuner_BayesianOptimization_mlp.search((w1.train), epochs=20, validation_data=(w1.val),callbacks=[stop_early])\n\n\n\n\nCode\n# Get the top 2 models.\nmodels_mlp = tuner_BayesianOptimization_mlp.get_best_models(num_models=2)\nbest_model_mlp = models_mlp[0]\n# Build the model.\n# Needed for `Sequential` without specified `input_shape`.\nbest_model_mlp.build(input_shape=(32, 12, 1))\nbest_model_mlp.summary()\n\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               (32, 12, 480)             960       \n                                                                 \n dense_1 (Dense)             (32, 12, 96)              46176     \n                                                                 \n dense_2 (Dense)             (32, 12, 288)             27936     \n                                                                 \n dropout (Dropout)           (32, 12, 288)             0         \n                                                                 \n dense_3 (Dense)             (32, 12, 1)               289       \n                                                                 \n=================================================================\nTotal params: 75361 (294.38 KB)\nTrainable params: 75361 (294.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\n\n\n# Get the top 2 hyperparameters.\nbest_hps_mlp = tuner_BayesianOptimization_mlp.get_best_hyperparameters(5)\n# Build the model with the best hp.\ncallback=tf.keras.callbacks.EarlyStopping(monitor=\"loss\",patience=5)\nmodel_mlp = build_model(best_hps_mlp[0])\n# Fit with the entire dataset.\nprint(model_mlp.fit(train_plus_val, epochs=20,callbacks=[callback]))\n\nEpoch 1/20\n1/8 [==&gt;...........................] - ETA: 5s - loss: 1.4122 - mean_squared_error: 1.4122\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8/8 [==============================] - 1s 4ms/step - loss: 0.7863 - mean_squared_error: 0.7863\nEpoch 2/20\n1/8 [==&gt;...........................] - ETA: 0s - loss: 0.5162 - mean_squared_error: 0.5162\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8/8 [==============================] - 0s 7ms/step - loss: 0.3506 - mean_squared_error: 0.3506\nEpoch 3/20\n1/8 [==&gt;...........................] - ETA: 0s - loss: 0.0233 - mean_squared_error: 0.0233\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8/8 [==============================] - 0s 6ms/step - loss: 0.1684 - mean_squared_error: 0.1684\nEpoch 4/20\n1/8 [==&gt;...........................] - ETA: 0s - loss: 0.0290 - mean_squared_error: 0.0290\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8/8 [==============================] - 0s 5ms/step - loss: 0.1420 - mean_squared_error: 0.1420\nEpoch 5/20\n1/8 [==&gt;...........................] - ETA: 0s - loss: 0.0712 - mean_squared_error: 0.0712\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8/8 [==============================] - 0s 4ms/step - loss: 0.1642 - mean_squared_error: 0.1642\nEpoch 6/20\n1/8 [==&gt;...........................] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8/8 [==============================] - 0s 4ms/step - loss: 0.1443 - mean_squared_error: 0.1443\nEpoch 7/20\n1/8 [==&gt;...........................] - ETA: 0s - loss: 0.1885 - mean_squared_error: 0.1885\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8/8 [==============================] - 0s 4ms/step - loss: 0.1640 - mean_squared_error: 0.1640\nEpoch 8/20\n1/8 [==&gt;...........................] - ETA: 0s - loss: 0.0703 - mean_squared_error: 0.0703\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8/8 [==============================] - 0s 7ms/step - loss: 0.1592 - mean_squared_error: 0.1592\nEpoch 9/20\n1/8 [==&gt;...........................] - ETA: 0s - loss: 0.1406 - mean_squared_error: 0.1406\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8/8 [==============================] - 0s 4ms/step - loss: 0.1597 - mean_squared_error: 0.1597\n&lt;keras.src.callbacks.History object at 0x0000024D8D4B66D0&gt;\n\n\n\n\n\n\n\nmodel_mlp.evaluate(w1.test, verbose=0)\n\n[0.5116912126541138, 0.5116912126541138]\n\n\n\ninput_predict_mlp=tf.constant(np.array(test_df[-17:]),dtype=tf.float32, shape=(17,1,1))\nprediction_test=model_mlp.predict(input_predict_mlp)*train_std['Total']+train_mean['Total']\n\n\n# Verdaderos valores vs predicciones\nprint(pd.DataFrame({'Verdaderos valores':true_series.reshape(17), 'Predicciones':prediction_test.reshape(17)}))\n\n    Verdaderos valores  Predicciones\n0            4209198.0    3971073.50\n1            4780210.0    4430670.00\n2            5460531.0    4976678.50\n3            4662521.0    4335952.00\n4            5497617.0    5006430.50\n5            5913682.0    5339993.00\n6            4388737.0    4115597.00\n7            4778520.0    4429310.00\n8            4213182.0    3974280.50\n9            4562248.0    4255250.00\n10           4642084.0    4319503.50\n11           3696188.0    3557442.75\n12           4202234.0    3965467.50\n13           4431911.0    4150350.75\n14           3739214.0    3592181.50\n15           4497862.0    4203431.00\n16           3985981.0    3791390.25\n\n\nLa raíz del error cuadrático medio de las predicciones respecto a los verdaderos valores en el conjunto de prueba es igual a:\n\nerrors_squared=tf.keras.metrics.mean_squared_error(true_series, prediction_test).numpy()\nprint(\"RECM:\",errors_squared.mean()**0.5)\n\nRECM: 329234.1145871734\n\n\n\n\n\n\nplt.plot(true_series_final)\nplt.plot(prediction_test_final)\nplt.legend(['Respesta real','Predicción de la Respuesta'],loc=\"best\", fontsize=15)\nplt.ylabel('Y y $\\hat{Y}$ en conjunto de prueba', fontsize=15)\n\nText(0, 0.5, 'Y y $\\\\hat{Y}$ en conjunto de prueba')"
  },
  {
    "objectID": "redes.html#exportaciones",
    "href": "redes.html#exportaciones",
    "title": "Redes neuronales",
    "section": "",
    "text": "Code\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import CSVLogger, EarlyStopping\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport seaborn as sns\nimport time\nimport gc\nimport sys\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\nprint(f\"Tensorflow Version: {tf.__version__}\")\nprint(f\"Pandas Version: {pd.__version__}\")\nprint(f\"Numpy Version: {np.__version__}\")\nprint(f\"System Version: {sys.version}\")\n\nmpl.rcParams['figure.figsize'] = (17, 5)\nmpl.rcParams['axes.grid'] = False\nsns.set_style(\"whitegrid\")\n\nnotebookstart= time.time()\n\n\n\n\nCode\nimport IPython\nimport IPython.display\n\n\n\n\nTotal de Exportaciones Colombia desde enero del año 2000 hasta junio del año 2023.\n\n\nCode\n# Lectura de la serie\nExportaciones = pd.read_excel(\"datos/Exportaciones.xlsx\",\n                   header = 0, usecols = ['Mes','Total']).iloc[96:].reset_index(drop = True).round()\nExportaciones['Total'] = Exportaciones['Total'].astype(int) \nExportaciones\n\n\n\n\n\n\n\n\n\nMes\nTotal\n\n\n\n\n0\n2000-01-01\n1011676\n\n\n1\n2000-02-01\n1054098\n\n\n2\n2000-03-01\n1053546\n\n\n3\n2000-04-01\n886359\n\n\n4\n2000-05-01\n1146258\n\n\n...\n...\n...\n\n\n277\n2023-02-01\n4202234\n\n\n278\n2023-03-01\n4431911\n\n\n279\n2023-04-01\n3739214\n\n\n280\n2023-05-01\n4497862\n\n\n281\n2023-06-01\n3985981\n\n\n\n\n282 rows × 2 columns\n\n\n\n\n\n\n\nSe quiere predecir la variable “Total de Exportaciones” (Total) en base a sus retardos.  A continuación se presenta el gráfico de la serie de tiempo:\n\n\nCode\nfeatures_considered = ['Total'] # la variable a usar en la predicción es ella misma\nfeatures = Exportaciones[features_considered] # solo se usará la variable Total en la predicción\nfeatures.index = Exportaciones['Mes'] # variable que indica el tiempo (la serie es mensual)\nfeatures.head()\n\n\n\n\nCode\nfeatures.plot(subplots = True, ) # gráfico de la serie de tiempo\n\n\narray([&lt;Axes: xlabel='Mes'&gt;], dtype=object)\n\n\n\n\n\n\n\n\n\nEl conjunto de datos original se divide en tres partes, de la siguiente manera:\n\nDatos de entrenamiento (70%) para un total de 197 observaciones. Desde 2000-01 hasta 2016-05.\nDatos de validación (20%) para un total de 56 observaciones. Desde 2016-06 hasta 2021-01.\nDatos de entrenamiento (10%) para un total de 29 observaciones. Desde 2021-02 hasta 2023-06.\n\n\n\nCode\n# partición del conjuntos de datos en entrenamiento, validación y prueba\ncolumn_indices = {name: i for i, name in enumerate(features.columns)} # índice = 0\n\nn = len(features) # longitud de la serie (282)\ntrain_df = features[0:int(n*0.7)] # 2000-01 hasta 2016-05\nval_df = features[int(n*0.7):int(n*0.9)] # 2016-06 hasta 2021-01\ntest_df = features[int(n*0.9):] # 2021-02 hasta 2023-06\n\nnum_features = features.shape[1]\n\n\n\n\nCode\nprint(\"longitud dataframe entrenamiento:\", train_df.shape)\nprint(\"longitud dataframe validación:\", val_df.shape)\nprint(\"longitud dataframe prueba:\", test_df.shape)\n\n\n\n\nCode\n# Normalización de las observaciones\ntrain_mean = train_df.mean()\ntrain_std = train_df.std()\n\ntrain_df = (train_df - train_mean) / train_std\nval_df = (val_df - train_mean) / train_std\ntest_df = (test_df - train_mean) / train_std\n\n\n\n\nCode\n# todo el dataframe normalizado por train_mean y train_std\ndf_std = (features - train_mean) / train_std\ndf_std = df_std.melt(var_name='', value_name='Datos normalizados')\ndf_std\n\n\n\n\nCode\n# plt.subplot(1, 2, 1)\n# sns.violinplot(y=Exportaciones[\"Total\"])\n\nplt.figure(figsize=(12, 8))\nax = sns.violinplot(x = '', y = 'Datos normalizados', data = df_std)\n_ = ax.set_xticklabels(features.keys(), rotation=0)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe definen las clases y funciones necesarias para resolver el problema de redes neuronales por medio de tensores.\n\nSe usará un tamaño de lote de 32.\nEl número máximo de épocas será de 20.\n\n\n\nCode\nclass WindowGenerator():\n  def __init__(self, input_width, label_width, shift,\n               train_df=train_df, val_df=val_df, test_df=test_df,\n               label_columns=None):\n    # Store the raw data.\n    self.train_df = train_df\n    self.val_df = val_df\n    self.test_df = test_df\n\n    # Work out the label column indices.\n    self.label_columns = label_columns\n    if label_columns is not None:\n      self.label_columns_indices = {name: i for i, name in\n                                    enumerate(label_columns)}\n    self.column_indices = {name: i for i, name in\n                           enumerate(train_df.columns)}\n\n    # Work out the window parameters.\n    self.input_width = input_width\n    self.label_width = label_width\n    self.shift = shift\n\n    self.total_window_size = input_width + shift\n\n    self.input_slice = slice(0, input_width)\n    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n\n    self.label_start = self.total_window_size - self.label_width\n    self.labels_slice = slice(self.label_start, None)\n    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n\n  def __repr__(self):\n    return '\\n'.join([\n        f'Total window size: {self.total_window_size}',\n        f'Input indices: {self.input_indices}',\n        f'Label indices: {self.label_indices}',\n        f'Label column name(s): {self.label_columns}'])\n\n\n\n\nCode\ndef split_window(self, features):\n  inputs = features[:, self.input_slice, :]\n  labels = features[:, self.labels_slice, :]\n  if self.label_columns is not None:\n    labels = tf.stack(\n        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n        axis=-1)\n\n  # Slicing doesn't preserve static shape information, so set the shapes\n  # manually. This way the `tf.data.Datasets` are easier to inspect.\n  inputs.set_shape([None, self.input_width, None])\n  labels.set_shape([None, self.label_width, None])\n\n  return inputs, labels\n\nWindowGenerator.split_window = split_window\n\n\n\n\nCode\ndef make_dataset(self, data):\n  data = np.array(data, dtype=np.float32)\n  ds = tf.keras.utils.timeseries_dataset_from_array(\n      data=data,\n      targets=None,\n      sequence_length=self.total_window_size,\n      sequence_stride=1,\n      shuffle=False,\n      batch_size=32,) \n\n  ds = ds.map(self.split_window)\n\n  return ds\n\nWindowGenerator.make_dataset = make_dataset\n\n\n\n\nCode\n@property\ndef train(self):\n  return self.make_dataset(self.train_df)\n\n@property\ndef val(self):\n  return self.make_dataset(self.val_df)\n\n@property\ndef test(self):\n  return self.make_dataset(self.test_df)\n\n@property\ndef example(self):\n  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n  result = getattr(self, '_example', None)\n  if result is None:\n    # No example batch was found, so get one from the `.train` dataset\n    result = next(iter(self.train))\n    # And cache it for next time\n    self._example = result\n  return result\n\nWindowGenerator.train = train\nWindowGenerator.val = val\nWindowGenerator.test = test\nWindowGenerator.example = example\n\n\n\n\nCode\ndef plot(self, model=None, plot_col='Total', max_subplots=3):\n  inputs, labels = self.example\n  plt.figure(figsize=(12, 8))\n  plot_col_index = self.column_indices[plot_col]\n  max_n = min(max_subplots, len(inputs))\n  for n in range(max_n):\n    plt.subplot(max_n, 1, n+1)\n    plt.ylabel(f'{plot_col} [normed]')\n    plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n             label='Inputs', marker='.', zorder=-10)\n\n    if self.label_columns:\n      label_col_index = self.label_columns_indices.get(plot_col, None)\n    else:\n      label_col_index = plot_col_index\n\n    if label_col_index is None:\n      continue\n\n    plt.scatter(self.label_indices, labels[n, :, label_col_index],\n                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n    if model is not None:\n      predictions = model(inputs)\n      plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n                  marker='X', edgecolors='k', label='Predictions',\n                  c='#ff7f0e', s=64)\n\n    if n == 0:\n      plt.legend()\n\n  plt.xlabel('Time [h]')\n\nWindowGenerator.plot = plot\n\n\n\n\nCode\n# Definimos número de épocas necesarias y funciones de pérdida\nMAX_EPOCHS = 20\n\ndef compile_and_fit(model, window, patience=2): #patiences como el número de épocas que espera antes de parar\n  # Para evitar sobreajuste\n  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                                    patience=patience,\n                                                    mode='min')\n\n  model.compile(loss=tf.losses.MeanSquaredError(),\n                optimizer=tf.optimizers.Adam(),\n                metrics=[tf.metrics.MeanAbsoluteError()])\n\n  history = model.fit(window.train, epochs=MAX_EPOCHS,\n                      validation_data=window.val,\n                      callbacks=[early_stopping])\n  return history\n\n\n\n\n\nEl interés en este caso es predecir el valor del total de las exportaciones 1 paso adelante (siguiente mes) con base en la información disponible de los últimos 12 meses del total de las exportaciones.\n\nw1 = WindowGenerator(input_width=12, label_width=1, shift=1,\n                     label_columns=['Total'])\nw1\n\nA continuación se muestra la configuración del modelo. Se usan las 12 observaciones anteriores para predecir la siguiente.\n\nw1.plot()\n\n\n\n\n\n\n\n\n\n\n\nLa búsqueda de los hiperparámetros que minimicen el error cuadrático medio sobre el conjunto de validación se hará con el método BayesianOptimization Tuner. El método consiste en escoger aleatoriamente algunas (pocas) combinaciones de los parámetros, luego basado en el rendimiento de esos hiperparámetros, escoge los siguientes mejores hiperparámetros. Es decir que toma en cuenta el histórico de los hiperparámetros que ya han sido probados en el modelo.\n\n\nCode\ndef build_model(hp):\n    model = keras.Sequential()\n    model.add(layers.Dense(units=hp.Int(\"num_units\", min_value=32, max_value=564, step=32),\n                activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"])))\n    # Tune the number of layers.\n    for i in range(hp.Int(\"num_layers\", 1, 5)):\n        model.add(\n            layers.Dense(\n                # Tune number of units separately.\n                units=hp.Int(f\"units_{i}\", min_value=32, max_value=564, step=32),\n                activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"]),\n            )\n        )\n    if hp.Boolean(\"dropout\"):\n        model.add(layers.Dropout(rate=0.25))\n    model.add(layers.Dense(1, activation=\"linear\"))\n    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n        loss=\"mean_squared_error\",\n        metrics=[\"mean_squared_error\"]\n    )\n    return model\n\n\nbuild_model(kt.HyperParameters())\n\n\n\n\nCode\ntuner_BayesianOptimization_mlp = kt.BayesianOptimization(\n    hypermodel=build_model,\n    objective=\"val_loss\",\n    max_trials=10,\n    num_initial_points=None,\n    alpha=0.0001,\n    beta=2.6,\n    seed=1234,\n    hyperparameters=None,\n    tune_new_entries=True,\n    allow_new_entries=True,\n    max_retries_per_trial=0,\n    max_consecutive_failed_trials=5,\n    overwrite=True,\n    directory=\"dirsalida\",\n    project_name=\"helloworld\"\n)\n\n\n\n\nCode\nstop_early=tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",patience=0)\n\n\n\n\nCode\ntuner_BayesianOptimization_mlp.search((w1.train), epochs=20, validation_data=(w1.val),callbacks=[stop_early])\n\n\n\n\nCode\n# Get the top 2 models.\nmodels_mlp = tuner_BayesianOptimization_mlp.get_best_models(num_models=2)\nbest_model_mlp = models_mlp[0]\n# Build the model.\n# Needed for `Sequential` without specified `input_shape`.\nbest_model_mlp.build(input_shape=(32, 12, 1))\nbest_model_mlp.summary()\n\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               (32, 12, 480)             960       \n                                                                 \n dense_1 (Dense)             (32, 12, 96)              46176     \n                                                                 \n dense_2 (Dense)             (32, 12, 288)             27936     \n                                                                 \n dropout (Dropout)           (32, 12, 288)             0         \n                                                                 \n dense_3 (Dense)             (32, 12, 1)               289       \n                                                                 \n=================================================================\nTotal params: 75361 (294.38 KB)\nTrainable params: 75361 (294.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\n\n\n# Get the top 2 hyperparameters.\nbest_hps_mlp = tuner_BayesianOptimization_mlp.get_best_hyperparameters(5)\n# Build the model with the best hp.\ncallback=tf.keras.callbacks.EarlyStopping(monitor=\"loss\",patience=5)\nmodel_mlp = build_model(best_hps_mlp[0])\n# Fit with the entire dataset.\nprint(model_mlp.fit(train_plus_val, epochs=20,callbacks=[callback]))\n\nEpoch 1/20\n1/8 [==&gt;...........................] - ETA: 5s - loss: 1.4122 - mean_squared_error: 1.4122\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8/8 [==============================] - 1s 4ms/step - loss: 0.7863 - mean_squared_error: 0.7863\nEpoch 2/20\n1/8 [==&gt;...........................] - ETA: 0s - loss: 0.5162 - mean_squared_error: 0.5162\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8/8 [==============================] - 0s 7ms/step - loss: 0.3506 - mean_squared_error: 0.3506\nEpoch 3/20\n1/8 [==&gt;...........................] - ETA: 0s - loss: 0.0233 - mean_squared_error: 0.0233\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8/8 [==============================] - 0s 6ms/step - loss: 0.1684 - mean_squared_error: 0.1684\nEpoch 4/20\n1/8 [==&gt;...........................] - ETA: 0s - loss: 0.0290 - mean_squared_error: 0.0290\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8/8 [==============================] - 0s 5ms/step - loss: 0.1420 - mean_squared_error: 0.1420\nEpoch 5/20\n1/8 [==&gt;...........................] - ETA: 0s - loss: 0.0712 - mean_squared_error: 0.0712\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8/8 [==============================] - 0s 4ms/step - loss: 0.1642 - mean_squared_error: 0.1642\nEpoch 6/20\n1/8 [==&gt;...........................] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8/8 [==============================] - 0s 4ms/step - loss: 0.1443 - mean_squared_error: 0.1443\nEpoch 7/20\n1/8 [==&gt;...........................] - ETA: 0s - loss: 0.1885 - mean_squared_error: 0.1885\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8/8 [==============================] - 0s 4ms/step - loss: 0.1640 - mean_squared_error: 0.1640\nEpoch 8/20\n1/8 [==&gt;...........................] - ETA: 0s - loss: 0.0703 - mean_squared_error: 0.0703\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8/8 [==============================] - 0s 7ms/step - loss: 0.1592 - mean_squared_error: 0.1592\nEpoch 9/20\n1/8 [==&gt;...........................] - ETA: 0s - loss: 0.1406 - mean_squared_error: 0.1406\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8/8 [==============================] - 0s 4ms/step - loss: 0.1597 - mean_squared_error: 0.1597\n&lt;keras.src.callbacks.History object at 0x0000024D8D4B66D0&gt;\n\n\n\n\n\n\n\nmodel_mlp.evaluate(w1.test, verbose=0)\n\n[0.5116912126541138, 0.5116912126541138]\n\n\n\ninput_predict_mlp=tf.constant(np.array(test_df[-17:]),dtype=tf.float32, shape=(17,1,1))\nprediction_test=model_mlp.predict(input_predict_mlp)*train_std['Total']+train_mean['Total']\n\n\n# Verdaderos valores vs predicciones\nprint(pd.DataFrame({'Verdaderos valores':true_series.reshape(17), 'Predicciones':prediction_test.reshape(17)}))\n\n    Verdaderos valores  Predicciones\n0            4209198.0    3971073.50\n1            4780210.0    4430670.00\n2            5460531.0    4976678.50\n3            4662521.0    4335952.00\n4            5497617.0    5006430.50\n5            5913682.0    5339993.00\n6            4388737.0    4115597.00\n7            4778520.0    4429310.00\n8            4213182.0    3974280.50\n9            4562248.0    4255250.00\n10           4642084.0    4319503.50\n11           3696188.0    3557442.75\n12           4202234.0    3965467.50\n13           4431911.0    4150350.75\n14           3739214.0    3592181.50\n15           4497862.0    4203431.00\n16           3985981.0    3791390.25\n\n\nLa raíz del error cuadrático medio de las predicciones respecto a los verdaderos valores en el conjunto de prueba es igual a:\n\nerrors_squared=tf.keras.metrics.mean_squared_error(true_series, prediction_test).numpy()\nprint(\"RECM:\",errors_squared.mean()**0.5)\n\nRECM: 329234.1145871734\n\n\n\n\n\n\nplt.plot(true_series_final)\nplt.plot(prediction_test_final)\nplt.legend(['Respesta real','Predicción de la Respuesta'],loc=\"best\", fontsize=15)\nplt.ylabel('Y y $\\hat{Y}$ en conjunto de prueba', fontsize=15)\n\nText(0, 0.5, 'Y y $\\\\hat{Y}$ en conjunto de prueba')"
  },
  {
    "objectID": "REDNEURONAL_SERIES.html",
    "href": "REDNEURONAL_SERIES.html",
    "title": "Redes neuronales para la serie de tiempo de Exportaciones",
    "section": "",
    "text": "import tensorflow as tf\nfrom tensorflow.keras.callbacks import CSVLogger, EarlyStopping\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport seaborn as sns\nimport time\nimport gc\nimport sys\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\nprint(f\"Tensorflow Version: {tf.__version__}\")\nprint(f\"Pandas Version: {pd.__version__}\")\nprint(f\"Numpy Version: {np.__version__}\")\nprint(f\"System Version: {sys.version}\")\n\nmpl.rcParams['figure.figsize'] = (17, 5)\nmpl.rcParams['axes.grid'] = False\nsns.set_style(\"whitegrid\")\n\nnotebookstart= time.time()\n\nTensorflow Version: 2.14.0\nPandas Version: 1.5.3\nNumpy Version: 1.23.5\nSystem Version: 3.10.9 | packaged by Anaconda, Inc. | (main, Mar  1 2023, 18:18:15) [MSC v.1916 64 bit (AMD64)]\nimport IPython\nimport IPython.display"
  },
  {
    "objectID": "REDNEURONAL_SERIES.html#datos-serie-de-tiempo-del-total-de-exportaciones-de-colombia-2000---2023",
    "href": "REDNEURONAL_SERIES.html#datos-serie-de-tiempo-del-total-de-exportaciones-de-colombia-2000---2023",
    "title": "Redes neuronales para la serie de tiempo de Exportaciones",
    "section": "0.1 Datos: Serie de tiempo del total de exportaciones de Colombia 2000 - 2023",
    "text": "0.1 Datos: Serie de tiempo del total de exportaciones de Colombia 2000 - 2023\n\n# Lectura de la serie\nExportaciones = pd.read_excel(\"C:/Users/dofca/Desktop/series/datos/Exportaciones.xlsx\",\n                   header = 0, usecols = ['Mes','Total']).iloc[96:].reset_index(drop = True).round()\nExportaciones['Total'] = Exportaciones['Total'].astype(int) \nExportaciones\n\n\n\n\n\n\n\n\nMes\nTotal\n\n\n\n\n0\n2000-01-01\n1011676\n\n\n1\n2000-02-01\n1054098\n\n\n2\n2000-03-01\n1053546\n\n\n3\n2000-04-01\n886359\n\n\n4\n2000-05-01\n1146258\n\n\n...\n...\n...\n\n\n277\n2023-02-01\n4202234\n\n\n278\n2023-03-01\n4431911\n\n\n279\n2023-04-01\n3739214\n\n\n280\n2023-05-01\n4497862\n\n\n281\n2023-06-01\n3985981\n\n\n\n\n282 rows × 2 columns"
  },
  {
    "objectID": "REDNEURONAL_SERIES.html#predicción-univariada",
    "href": "REDNEURONAL_SERIES.html#predicción-univariada",
    "title": "Redes neuronales para la serie de tiempo de Exportaciones",
    "section": "0.2 Predicción Univariada",
    "text": "0.2 Predicción Univariada\nSe desea predecir la variable total de exportaciones (Total) usando los retardos de ella misma.\n\nfeatures_considered = ['Total'] # la variable a usar en la predicción es ella misma\n\n\nfeatures = Exportaciones[features_considered] # solo se usará la variable Total en la predicción\nfeatures.index = Exportaciones['Mes'] # variable que indica el tiempo (la serie es mensual)\nfeatures.head()\n\n\n\n\n\n\n\n\nTotal\n\n\nMes\n\n\n\n\n\n2000-01-01\n1011676\n\n\n2000-02-01\n1054098\n\n\n2000-03-01\n1053546\n\n\n2000-04-01\n886359\n\n\n2000-05-01\n1146258\n\n\n\n\n\n\n\n\nfeatures.plot(subplots = True) # gráfico de la serie de tiempo\n\narray([&lt;Axes: xlabel='Mes'&gt;], dtype=object)"
  },
  {
    "objectID": "REDNEURONAL_SERIES.html#separación-entrenamiento-validación-prueba-y-normalización.",
    "href": "REDNEURONAL_SERIES.html#separación-entrenamiento-validación-prueba-y-normalización.",
    "title": "Redes neuronales para la serie de tiempo de Exportaciones",
    "section": "0.3 Separación (Entrenamiento-Validación-Prueba) y Normalización.",
    "text": "0.3 Separación (Entrenamiento-Validación-Prueba) y Normalización.\n\n# partición del conjuntos de datos en entrenamiento, validación y prueba\ncolumn_indices = {name: i for i, name in enumerate(features.columns)} # índice = 0\n\nn = len(features) # longitud de la serie (282)\ntrain_df = features[0:int(n*0.7)] # 2000-01 hasta 2016-05\nval_df = features[int(n*0.7):int(n*0.9)] # 2016-06 hasta 2021-01\ntest_df = features[int(n*0.9):] # 2021-02 hasta 2023-06\n\nnum_features = features.shape[1]\n\n\nprint(\"longitud dataframe entrenamiento:\", train_df.shape)\nprint(\"longitud dataframe validación:\", val_df.shape)\nprint(\"longitud dataframe prueba:\", test_df.shape)\n\nlongitud dataframe entrenamiento: (197, 1)\nlongitud dataframe validación: (56, 1)\nlongitud dataframe prueba: (29, 1)\n\n\n\n# Normalización de las observaciones\n# StandardScalar de sklearn\ntrain_mean = train_df.mean()\ntrain_std = train_df.std()\n\ntrain_df = (train_df - train_mean) / train_std\nval_df = (val_df - train_mean) / train_std\ntest_df = (test_df - train_mean) / train_std\n\n\n# todo el dataframe normalizado por train_mean y train_std\ndf_std = (features - train_mean) / train_std\ndf_std = df_std.melt(var_name='Column', value_name='Normalized')\ndf_std\n\n\n\n\n\n\n\n\nColumn\nNormalized\n\n\n\n\n0\nTotal\n-1.172383\n\n\n1\nTotal\n-1.142863\n\n\n2\nTotal\n-1.143247\n\n\n3\nTotal\n-1.259587\n\n\n4\nTotal\n-1.078732\n\n\n...\n...\n...\n\n\n277\nTotal\n1.047816\n\n\n278\nTotal\n1.207640\n\n\n279\nTotal\n0.725616\n\n\n280\nTotal\n1.253533\n\n\n281\nTotal\n0.897333\n\n\n\n\n282 rows × 2 columns\n\n\n\n\nplt.figure(figsize=(9, 5))\nax = sns.violinplot(x = 'Column', y = 'Normalized', data = df_std)\n_ = ax.set_xticklabels(features.keys(), rotation=90)\n\n\n\n\n\nclass WindowGenerator():\n  def __init__(self, input_width, label_width, shift,\n               train_df=train_df, val_df=val_df, test_df=test_df,\n               label_columns=None):\n    # Store the raw data.\n    self.train_df = train_df\n    self.val_df = val_df\n    self.test_df = test_df\n\n    # Work out the label column indices.\n    self.label_columns = label_columns\n    if label_columns is not None:\n      self.label_columns_indices = {name: i for i, name in\n                                    enumerate(label_columns)}\n    self.column_indices = {name: i for i, name in\n                           enumerate(train_df.columns)}\n\n    # Work out the window parameters.\n    self.input_width = input_width\n    self.label_width = label_width\n    self.shift = shift\n\n    self.total_window_size = input_width + shift\n\n    self.input_slice = slice(0, input_width)\n    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n\n    self.label_start = self.total_window_size - self.label_width\n    self.labels_slice = slice(self.label_start, None)\n    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n\n  def __repr__(self):\n    return '\\n'.join([\n        f'Total window size: {self.total_window_size}',\n        f'Input indices: {self.input_indices}',\n        f'Label indices: {self.label_indices}',\n        f'Label column name(s): {self.label_columns}'])"
  },
  {
    "objectID": "REDNEURONAL_SERIES.html#split",
    "href": "REDNEURONAL_SERIES.html#split",
    "title": "Redes neuronales para la serie de tiempo de Exportaciones",
    "section": "0.4 Split",
    "text": "0.4 Split\nDada una lista de entradas consecutivas, el método split_window las convertirá en una ventana de entradas y una ventana de etiquetas.\n\ndef split_window(self, features):\n  inputs = features[:, self.input_slice, :]\n  labels = features[:, self.labels_slice, :]\n  if self.label_columns is not None:\n    labels = tf.stack(\n        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n        axis=-1)\n\n  # Slicing doesn't preserve static shape information, so set the shapes\n  # manually. This way the `tf.data.Datasets` are easier to inspect.\n  inputs.set_shape([None, self.input_width, None])\n  labels.set_shape([None, self.label_width, None])\n\n  return inputs, labels\n\nWindowGenerator.split_window = split_window"
  },
  {
    "objectID": "REDNEURONAL_SERIES.html#transforma-nuestros-objetos-a-tipo-tensorflow",
    "href": "REDNEURONAL_SERIES.html#transforma-nuestros-objetos-a-tipo-tensorflow",
    "title": "Redes neuronales para la serie de tiempo de Exportaciones",
    "section": "0.5 Transforma nuestros objetos a tipo tensorflow",
    "text": "0.5 Transforma nuestros objetos a tipo tensorflow\nTamaño del lote batch size = 32\n\ndef make_dataset(self, data):\n  data = np.array(data, dtype=np.float32)\n  ds = tf.keras.utils.timeseries_dataset_from_array(\n      data=data,\n      targets=None,\n      sequence_length=self.total_window_size,\n      sequence_stride=1,\n      shuffle=False,\n      batch_size=32,) \n\n  ds = ds.map(self.split_window)\n\n  return ds\n\nWindowGenerator.make_dataset = make_dataset\n\n\n@property\ndef train(self):\n  return self.make_dataset(self.train_df)\n\n@property\ndef val(self):\n  return self.make_dataset(self.val_df)\n\n@property\ndef test(self):\n  return self.make_dataset(self.test_df)\n\n@property\ndef example(self):\n  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n  result = getattr(self, '_example', None)\n  if result is None:\n    # No example batch was found, so get one from the `.train` dataset\n    result = next(iter(self.train))\n    # And cache it for next time\n    self._example = result\n  return result\n\nWindowGenerator.train = train\nWindowGenerator.val = val\nWindowGenerator.test = test\nWindowGenerator.example = example"
  },
  {
    "objectID": "REDNEURONAL_SERIES.html#modelo-base-naive",
    "href": "REDNEURONAL_SERIES.html#modelo-base-naive",
    "title": "Redes neuronales para la serie de tiempo de Exportaciones",
    "section": "3.1 1. Modelo base (Naive)",
    "text": "3.1 1. Modelo base (Naive)\n\n# configuración del modelo base\nsingle_step_window = WindowGenerator(\n    input_width=1, label_width=1, shift=1,\n    label_columns=['Total'])\nsingle_step_window\n\n\n# Ejemplo de cómo se itera sobre los lotes en los tres datasets\nfor example_inputs, example_labels in single_step_window.train.take(1):\n    print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n    print(f'Labels shape (batch, time, features): {example_labels.shape}')\n\n\n# modelo base. Retorna como predicción del siguiente paso, la predicción actual.\nclass Baseline(tf.keras.Model):\n  def __init__(self, label_index=None):\n    super().__init__()\n    self.label_index = label_index\n\n  def call(self, inputs):\n    if self.label_index is None:\n      return inputs\n    result = inputs[:, :, self.label_index]\n    return result[:, :, tf.newaxis]\n\nSe entrena el modelo usando el conjunto de datos single_step_window.\n\nbaseline = Baseline(label_index=column_indices['Total'])\n\nbaseline.compile(loss=tf.losses.MeanSquaredError(),\n                 metrics=[tf.metrics.MeanAbsoluteError()])\n\nval_performance = {}\nperformance = {}\nval_performance['Baseline'] = baseline.evaluate(single_step_window.val)\nperformance['Baseline'] = baseline.evaluate(single_step_window.test, verbose=0)\n\nSe crea una ventana amplia para visualizar el comportamiento del modelo:\n\nwide_window = WindowGenerator(\n    input_width=12, label_width=12, shift=1,\n    label_columns=['Total'])\nwide_window\n\n\nprint('Input shape:', wide_window.example[0].shape)\nprint('Output shape:', baseline(wide_window.example[0]).shape)\n\n\nwide_window.plot(baseline)\n\n\nval_performance = {}\nperformance = {}\nval_performance['Baseline'] = baseline.evaluate(wide_window.val)\nperformance['Baseline'] = baseline.evaluate(wide_window.test, verbose=0)"
  },
  {
    "objectID": "REDNEURONAL_SERIES.html#modelo-lineal",
    "href": "REDNEURONAL_SERIES.html#modelo-lineal",
    "title": "Redes neuronales para la serie de tiempo de Exportaciones",
    "section": "3.2 2. Modelo lineal",
    "text": "3.2 2. Modelo lineal\n\nlinear = tf.keras.Sequential([\n    tf.keras.layers.Dense(units=1)\n])\nprint('Input shape:', single_step_window.example[0].shape)\nprint('Output shape:', linear(single_step_window.example[0]).shape)\n\n\nhistory = compile_and_fit(linear, single_step_window)\nval_performance['Linear'] = linear.evaluate(single_step_window.val)\nperformance['Linear'] = linear.evaluate(single_step_window.test, verbose=0)\n\n\nwide_window.plot(linear)"
  },
  {
    "objectID": "REDNEURONAL_SERIES.html#modelo-perceptrón-multicapa-mlp-con-dos-capas-ocultas-y-usando-función-de-activación-relu",
    "href": "REDNEURONAL_SERIES.html#modelo-perceptrón-multicapa-mlp-con-dos-capas-ocultas-y-usando-función-de-activación-relu",
    "title": "Redes neuronales para la serie de tiempo de Exportaciones",
    "section": "3.3 3. Modelo perceptrón multicapa (MLP) con dos capas ocultas y usando función de activación RELU",
    "text": "3.3 3. Modelo perceptrón multicapa (MLP) con dos capas ocultas y usando función de activación RELU\n\nsingle_step_window\n\n\ndense = tf.keras.Sequential([\n    tf.keras.layers.Dense(units=64, activation='relu'),\n    tf.keras.layers.Dense(units=64, activation='relu'),\n    tf.keras.layers.Dense(units=1)\n])\n\n\nhistory = compile_and_fit(dense, single_step_window)\nval_performance['Dense'] = dense.evaluate(single_step_window.val)\nperformance['Dense'] = dense.evaluate(single_step_window.test, verbose=0)\n\n\ndense.evaluate(single_step_window.test,verbose=1)\n\n\npd.DataFrame(history.history).plot(figsize=(8,5))\n\n\nsingle_step_window.plot(dense)"
  },
  {
    "objectID": "REDNEURONAL_SERIES.html#modelo-perceptrón-multicapa-mlp-con-múltiples-retardos-incluídos",
    "href": "REDNEURONAL_SERIES.html#modelo-perceptrón-multicapa-mlp-con-múltiples-retardos-incluídos",
    "title": "Redes neuronales para la serie de tiempo de Exportaciones",
    "section": "3.4 4. Modelo perceptrón multicapa (MLP) con múltiples retardos incluídos",
    "text": "3.4 4. Modelo perceptrón multicapa (MLP) con múltiples retardos incluídos\nSe usarán 6 retardos para predecir el valor de la serie un paso adelante.\n\nCONV_WIDTH = 6\nconv_window = WindowGenerator(\n    input_width=CONV_WIDTH,\n    label_width=1,\n    shift=1,\n    label_columns=['Total'])\nconv_window\n\n\nconv_window.plot()\nplt.title(\"A partir de los 6 meses pasados, predecir el siguiente el valor para el siguiente mes\")\nplt.tight_layout()\n\n\nmulti_step_dense = tf.keras.Sequential([\n    # Shape: (time, features) =&gt; (time*features)\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(units=32, activation='relu'),\n    tf.keras.layers.Dense(units=32, activation='relu'),\n    tf.keras.layers.Dense(units=1),\n    # Add back the time dimension.\n    # Shape: (outputs) =&gt; (1, outputs)\n    tf.keras.layers.Reshape([1, -1]),\n])\nprint('Input shape:', conv_window.example[0].shape)\nprint('Output shape:', multi_step_dense(conv_window.example[0]).shape)\n\nEjemplo de cómo se itera sobre los lotes:\n\nconv_window.example[0]\n\n\nhistory = compile_and_fit(multi_step_dense, conv_window)\nval_performance['Multi step dense'] = multi_step_dense.evaluate(conv_window.val)\nperformance['Multi step dense'] = multi_step_dense.evaluate(conv_window.test, verbose=0)\n\n\nconv_window.plot(multi_step_dense)"
  },
  {
    "objectID": "REDNEURONAL_SERIES.html#modelo-perceptrón-multicapa-mlp-con-múltiples-retardos-incluídos-a-través-de-una-convolución",
    "href": "REDNEURONAL_SERIES.html#modelo-perceptrón-multicapa-mlp-con-múltiples-retardos-incluídos-a-través-de-una-convolución",
    "title": "Redes neuronales para la serie de tiempo de Exportaciones",
    "section": "3.5 5. Modelo perceptrón multicapa (MLP) con múltiples retardos incluídos a través de una convolución",
    "text": "3.5 5. Modelo perceptrón multicapa (MLP) con múltiples retardos incluídos a través de una convolución\n\nconv_model = tf.keras.Sequential([\n    tf.keras.layers.Conv1D(filters=32,\n                           kernel_size=(CONV_WIDTH,),\n                           activation='relu'),\n    tf.keras.layers.Dense(units=32, activation='relu'),\n    tf.keras.layers.Dense(units=1),\n])\n\n\nprint(\"Conv model on `conv_window`\")\nprint('Input shape:', conv_window.example[0].shape)\nprint('Output shape:', conv_model(conv_window.example[0]).shape)\n\n\nhistory = compile_and_fit(conv_model, conv_window)\nIPython.display.clear_output()\nval_performance['Conv'] = conv_model.evaluate(conv_window.val)\nperformance['Conv'] = conv_model.evaluate(conv_window.test, verbose=0)\n\n\nprint(\"Wide window\")\nprint('Input shape:', wide_window.example[0].shape)\nprint('Labels shape:', wide_window.example[1].shape)\nprint('Output shape:', conv_model(wide_window.example[0]).shape)\n\n\nLABEL_WIDTH = 12\nINPUT_WIDTH = LABEL_WIDTH + (CONV_WIDTH - 1)\nwide_conv_window = WindowGenerator(\n    input_width=INPUT_WIDTH,\n    label_width=LABEL_WIDTH,\n    shift=1,\n    label_columns=['Total'])\nwide_conv_window\n\n\nprint(\"Wide conv window\")\nprint('Input shape:', wide_conv_window.example[0].shape)\nprint('Labels shape:', wide_conv_window.example[1].shape)\nprint('Output shape:', conv_model(wide_conv_window.example[0]).shape)\n\n\nwide_conv_window.plot(conv_model)"
  },
  {
    "objectID": "REDNEURONAL_SERIES.html#red-neuronal-recurrente-lstm-con-múltiples-retardos-incluídos",
    "href": "REDNEURONAL_SERIES.html#red-neuronal-recurrente-lstm-con-múltiples-retardos-incluídos",
    "title": "Redes neuronales para la serie de tiempo de Exportaciones",
    "section": "3.6 Red neuronal recurrente (LSTM) con múltiples retardos incluídos",
    "text": "3.6 Red neuronal recurrente (LSTM) con múltiples retardos incluídos\n\nlstm_model = tf.keras.models.Sequential([\n    # Shape [batch, time, features] =&gt; [batch, time, lstm_units]\n    tf.keras.layers.LSTM(32, return_sequences=True),\n    # Shape =&gt; [batch, time, features]\n    tf.keras.layers.Dense(units=1)\n])\n\n\nprint('Input shape:', wide_window.example[0].shape)\nprint('Output shape:', lstm_model(wide_window.example[0]).shape)\n\n\nhistory = compile_and_fit(lstm_model, wide_window)\nIPython.display.clear_output()\nval_performance['LSTM'] = lstm_model.evaluate(wide_window.val)\nperformance['LSTM'] = lstm_model.evaluate(wide_window.test, verbose=0)\n\n\nwide_window.plot(lstm_model)"
  },
  {
    "objectID": "REDNEURONAL_SERIES.html#evaluación-de-los-modelos",
    "href": "REDNEURONAL_SERIES.html#evaluación-de-los-modelos",
    "title": "Redes neuronales para la serie de tiempo de Exportaciones",
    "section": "3.7 Evaluación de los modelos",
    "text": "3.7 Evaluación de los modelos\n\nx = np.arange(len(performance))\nwidth = 0.3\nmetric_name = 'mean_absolute_error'\nmetric_index = lstm_model.metrics_names.index('mean_absolute_error')\nval_mae = [v[metric_index] for v in val_performance.values()]\ntest_mae = [v[metric_index] for v in performance.values()]\nplt.ylabel('mean_absolute_error [T (degC), normalized]')\nplt.bar(x - 0.17, val_mae, width, label='Validation')\nplt.bar(x + 0.17, test_mae, width, label='Test')\nplt.xticks(ticks=x, labels=performance.keys(),\n           rotation=45)\n_ = plt.legend() \n\n\nfor name, value in performance.items():\n  print(f'{name:12s}: {value[1]:0.4f}')\n\n\ndense.evaluate(single_step_window.test,verbose=1)\n\n\nconv_model.evaluate(conv_window.test,verbose=1)\n\n\nunpaso_con6meses = WindowGenerator(\n    input_width=6, label_width=1, shift=1,\n    label_columns=['Total'])# La variable a predecir es T Total\nunpaso_con6meses\n\n\nfor example_inputs, example_labels in unpaso_con6meses.train.take(1):\n  print(f'Forma(dimensión) de las entradas (batch, time, features): {example_inputs.shape}')\n  print(f'Forma(dimensión) de las etiquetas, salidas o respuestas (batch, time, features): {example_labels.shape}')\n\n\nunpaso_con6meses.plot()"
  },
  {
    "objectID": "REDNEURONAL_SERIES.html#modelo-base",
    "href": "REDNEURONAL_SERIES.html#modelo-base",
    "title": "Redes neuronales para la serie de tiempo de Exportaciones",
    "section": "6.1 Modelo base",
    "text": "6.1 Modelo base\n\nclass Baseline(tf.keras.Model):\n  def __init__(self, label_index=None):\n    super().__init__()\n    self.label_index = label_index\n\n  def call(self, inputs):\n    if self.label_index is None:\n      return inputs\n    result = inputs[:, :, self.label_index]\n    return result[:, :, tf.newaxis]\n\n\n6.1.1 se entrena el modelo usando el conjunto de datos (unpaso_con6meses)\n\nbaseline = Baseline(label_index=column_indices['Total'])\n\nbaseline.compile(loss=tf.losses.MeanSquaredError(),\n                 metrics=[tf.metrics.MeanAbsoluteError()])\n\nval_performance = {}\nperformance = {}\nval_performance['Baseline'] = baseline.evaluate(unpaso_con6meses.val)\nperformance['Baseline'] = baseline.evaluate(unpaso_con6meses.test, verbose=0)"
  }
]